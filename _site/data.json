[{"layout":"default","title":"Cluster version","content":"# Cluster version\n\n<img alt=\"Victoria Metrics\" src=\"logo.png\">\n\nVictoriaMetrics is a fast, cost-effective and scalable time series database. It can be used as a long-term remote storage for Prometheus.\n\nIt is recommended using [single-node version](https://github.com/VictoriaMetrics/VictoriaMetrics) instead of cluster version\nfor ingestion rates lower than a million of data points per second.\nSingle-node version [scales perfectly](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae)\nwith the number of CPU cores, RAM and available storage space.\nSingle-node version is easier to configure and operate comparing to cluster version, so think twice before sticking to cluster version.\n\nJoin [our Slack](http://slack.victoriametrics.com/) or [contact us](mailto:info@victoriametrics.com) with consulting and support questions.\n\n\n## Prominent features\n\n- Supports all the features of [single-node version](https://github.com/VictoriaMetrics/VictoriaMetrics).\n- Performance and capacity scales horizontally. See [these docs for details](#cluster-resizing-and-scalability).\n- Supports multiple independent namespaces for time series data (aka multi-tenancy). See [these docs for details](#multitenancy).\n- Supports replication. See [these docs for details](#replication-and-data-safety).\n\n\n## Architecture overview\n\nVictoriaMetrics cluster consists of the following services:\n\n- `vmstorage` - stores the data\n- `vminsert` - proxies the ingested data to `vmstorage` shards using consistent hashing\n- `vmselect` - performs incoming queries using the data from `vmstorage`\n\nEach service may scale independently and may run on the most suitable hardware.\n`vmstorage` nodes don't know about each other, don't communicate with each other and don't share any data.\nThis is [shared nothing architecture](https://en.wikipedia.org/wiki/Shared-nothing_architecture).\nIt increases cluster availability, simplifies cluster maintenance and cluster scaling.\n\n<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vTvk2raU9kFgZ84oF-OKolrGwHaePhHRsZEcfQ1I_EC5AB_XPWwB392XshxPramLJ8E4bqptTnFn5LL/pub?w=1104&amp;h=746\">\n\n\n## Multitenancy\n\nVictoriaMetrics cluster supports multiple isolated tenants (aka namespaces).\nTenants are identified by `accountID` or `accountID:projectID`, which are put inside request urls.\nSee [these docs](#url-format) for details. Some facts about tenants in VictoriaMetrics:\n\n* Each `accountID` and `projectID` is identified by an arbitrary 32-bit integer in the range `[0 .. 2^32)`.\nIf `projectID` is missing, then it is automatically assigned to `0`. It is expected that other information about tenants\nsuch as auth tokens, tenant names, limits, accounting, etc. is stored in a separate relational database. This database must be managed\nby a separate service sitting in front of VictoriaMetrics cluster such as [vmauth](https://victoriametrics.github.io/vmauth.html).\n[Contact us](mailto:info@victoriametrics.com) if you need help with creating such a service.\n\n* Tenants are automatically created when the first data point is written into the given tenant.\n\n* Data for all the tenants is evenly spread among available `vmstorage` nodes. This guarantees even load among `vmstorage` nodes\nwhen different tenants have different amounts of data and different query load.\n\n* VictoriaMetrics doesn't support querying multiple tenants in a single request.\n\n\n## Binaries\n\nCompiled binaries for cluster version are available in the `assets` section of [releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases).\nSee archives containing `cluster` word.\n\nDocker images for cluster version are available here:\n\n- `vminsert` - https://hub.docker.com/r/victoriametrics/vminsert/tags\n- `vmselect` - https://hub.docker.com/r/victoriametrics/vmselect/tags\n- `vmstorage` - https://hub.docker.com/r/victoriametrics/vmstorage/tags\n\n\n## Building from sources\n\nSource code for cluster version is available at [cluster branch](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster).\n\n\n### Production builds\n\nThere is no need in installing Go on a host system since binaries are built\ninside [the official docker container for Go](https://hub.docker.com/_/golang).\nThis makes reproducible builds.\nSo [install docker](https://docs.docker.com/install/) and run the following command:\n\n```\nmake vminsert-prod vmselect-prod vmstorage-prod\n```\n\nProduction binaries are built into statically linked binaries. They are put into `bin` folder with `-prod` suffixes:\n```\n$ make vminsert-prod vmselect-prod vmstorage-prod\n$ ls -1 bin\nvminsert-prod\nvmselect-prod\nvmstorage-prod\n```\n\n### Development Builds\n\n1. [Install go](https://golang.org/doc/install). The minimum supported version is Go 1.13.\n2. Run `make` from the repository root. It should build `vmstorage`, `vmselect`\n and `vminsert` binaries and put them into the `bin` folder.\n\n\n### Building docker images\n\nRun `make package`. It will build the following docker images locally:\n\n* `victoriametrics/vminsert:<PKG_TAG>`\n* `victoriametrics/vmselect:<PKG_TAG>`\n* `victoriametrics/vmstorage:<PKG_TAG>`\n\n`<PKG_TAG>` is auto-generated image tag, which depends on source code in the repository.\nThe `<PKG_TAG>` may be manually set via `PKG_TAG=foobar make package`.\n\nBy default images are built on top of [alpine](https://hub.docker.com/_/scratch) image in order to improve debuggability.\nIt is possible to build an image on top of any other base image by setting it via `<ROOT_IMAGE>` environment variable.\nFor example, the following command builds images on top of [scratch](https://hub.docker.com/_/scratch) image:\n\n```bash\nROOT_IMAGE=scratch make package\n```\n\n## Operation\n\n## Cluster setup\n\nA minimal cluster must contain the following nodes:\n\n* a single `vmstorage` node with `-retentionPeriod` and `-storageDataPath` flags\n* a single `vminsert` node with `-storageNode=<vmstorage_host>:8400`\n* a single `vmselect` node with `-storageNode=<vmstorage_host>:8401`\n\nIt is recommended to run at least two nodes for each service\nfor high availability purposes.\n\nAn http load balancer such as `nginx` must be put in front of `vminsert` and `vmselect` nodes:\n- requests starting with `/insert` must be routed to port `8480` on `vminsert` nodes.\n- requests starting with `/select` must be routed to port `8481` on `vmselect` nodes.\n\nPorts may be altered by setting `-httpListenAddr` on the corresponding nodes.\n\nIt is recommended setting up [monitoring](#monitoring) for the cluster.\n\n### Environment variables\n\nEach flag values can be set thru environment variables by following these rules:\n\n- The `-envflag.enable` flag must be set\n- Each `.` in flag names must be substituted by `_` (for example `-insert.maxQueueDuration <duration>` will translate to `insert_maxQueueDuration=<duration>`)\n- For repeating flags, an alternative syntax can be used by joining the different values into one using `,` as separator (for example `-storageNode <nodeA> -storageNode <nodeB>` will translate to `storageNode=<nodeA>,<nodeB>`)\n- It is possible setting prefix for environment vars with `-envflag.prefix`. For instance, if `-envflag.prefix=VM_`, then env vars must be prepended with `VM_`\n\n\n## Monitoring\n\nAll the cluster components expose various metrics in Prometheus-compatible format at `/metrics` page on the TCP port set in `-httpListenAddr` command-line flag.\nBy default the following TCP ports are used:\n- `vminsert` - 8480\n- `vmselect` - 8481\n- `vmstorage` - 8482\n\nIt is recommended setting up [vmagent](https://victoriametrics.github.io/vmagent.html)\nor Prometheus to scrape `/metrics` pages from all the cluster components, so they can be monitored and analyzed\nwith [the official Grafana dashboard for VictoriaMetrics cluster](https://grafana.com/grafana/dashboards/11176)\nor [an alternative dashboard for VictoriaMetrics cluster](https://grafana.com/grafana/dashboards/11831).\n\n\n## URL format\n\n* URLs for data ingestion: `http://<vminsert>:8480/insert/<accountID>/<suffix>`, where:\n - `<accountID>` is an arbitrary 32-bit integer identifying namespace for data ingestion (aka tenant). It is possible to set it as `accountID:projectID`,\n where `projectID` is also arbitrary 32-bit integer. If `projectID` isn't set, then it equals to `0`.\n - `<suffix>` may have the following values:\n - `prometheus` and `prometheus/api/v1/write` - for inserting data with [Prometheus remote write API](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write)\n - `influx/write` and `influx/api/v2/write` - for inserting data with [Influx line protocol](https://docs.influxdata.com/influxdb/v1.7/write_protocols/line_protocol_tutorial/).\n - `opentsdb/api/put` - for accepting [OpenTSDB HTTP /api/put requests](http://opentsdb.net/docs/build/html/api_http/put.html).\n This handler is disabled by default. It is exposed on a distinct TCP address set via `-opentsdbHTTPListenAddr` command-line flag.\n See [these docs](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#sending-opentsdb-data-via-http-apiput-requests) for details.\n - `prometheus/api/v1/import` - for importing data obtained via `api/v1/export` on `vmselect` (see below).\n - `prometheus/api/v1/import/native` - for importing data obtained via `api/v1/export/native` on `vmselect` (see below).\n - `prometheus/api/v1/import/csv` - for importing arbitrary CSV data. See [these docs](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-import-csv-data) for details.\n - `prometheus/api/v1/import/prometheus` - for importing data in [Prometheus text exposition format](https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format) and in [OpenMetrics format](https://github.com/OpenObservability/OpenMetrics/blob/master/specification/OpenMetrics.md). See [these docs](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-import-data-in-prometheus-exposition-format) for details.\n\n* URLs for [Prometheus querying API](https://prometheus.io/docs/prometheus/latest/querying/api/): `http://<vmselect>:8481/select/<accountID>/prometheus/<suffix>`, where:\n - `<accountID>` is an arbitrary number identifying data namespace for the query (aka tenant)\n - `<suffix>` may have the following values:\n - `api/v1/query` - performs [PromQL instant query](https://prometheus.io/docs/prometheus/latest/querying/api/#instant-queries).\n - `api/v1/query_range` - performs [PromQL range query](https://prometheus.io/docs/prometheus/latest/querying/api/#range-queries).\n - `api/v1/series` - performs [series query](https://prometheus.io/docs/prometheus/latest/querying/api/#finding-series-by-label-matchers).\n - `api/v1/labels` - returns a [list of label names](https://prometheus.io/docs/prometheus/latest/querying/api/#getting-label-names).\n - `api/v1/label/<label_name>/values` - returns values for the given `<label_name>` according [to API](https://prometheus.io/docs/prometheus/latest/querying/api/#querying-label-values).\n - `federate` - returns [federated metrics](https://prometheus.io/docs/prometheus/latest/federation/).\n - `api/v1/export` - exports raw data in JSON line format. See [this article](https://medium.com/@valyala/analyzing-prometheus-data-with-external-tools-5f3e5e147639) for details.\n - `api/v1/export/native` - exports raw data in native binary format. It may be imported into another VictoriaMetrics via `api/v1/import/native` (see above).\n - `api/v1/export/csv` - exports data in CSV. It may be imported into another VictoriaMetrics via `api/v1/import/csv` (see above).\n - `api/v1/status/tsdb` - for time series stats. See [these docs](https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-stats) for details.\n VictoriaMetrics accepts optional `topN=N` and `date=YYYY-MM-DD` query args for this handler, where `N` is the number of top entries to return in the response\n and `YYYY-MM-DD` is the date for collecting the stats. By default the stats is collected for the current day.\n - `api/v1/status/active_queries` - for currently executed active queries. Note that every `vmselect` maintains an independent list of active queries,\n which is returned in the response.\n - `api/v1/status/top_queries` - for listing the most frequently executed queries and queries taking the most duration.\n\n* URLs for [Graphite Metrics API](https://graphite-api.readthedocs.io/en/latest/api.html#the-metrics-api): `http://<vmselect>:8481/select/<accountID>/graphite/<suffix>`, where:\n - `<accountID>` is an arbitrary number identifying data namespace for query (aka tenant)\n - `<suffix>` may have the following values:\n - `render` - implements Graphite Render API. See [these docs](https://graphite.readthedocs.io/en/stable/render_api.html). This functionality is available in [Enterprise package](https://victoriametrics.com/enterprise.html).\n - `metrics/find` - searches Graphite metrics. See [these docs](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-find).\n - `metrics/expand` - expands Graphite metrics. See [these docs](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-expand).\n - `metrics/index.json` - returns all the metric names. See [these docs](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-index-json).\n - `tags/tagSeries` - registers time series. See [these docs](https://graphite.readthedocs.io/en/stable/tags.html#adding-series-to-the-tagdb).\n - `tags/tagMultiSeries` - register multiple time series. See [these docs](https://graphite.readthedocs.io/en/stable/tags.html#adding-series-to-the-tagdb).\n - `tags` - returns tag names. See [these docs](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags).\n - `tags/<tag_name>` - returns tag values for the given `<tag_name>`. See [these docs](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags).\n - `tags/findSeries` - returns series matching the given `expr`. See [these docs](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags).\n - `tags/autoComplete/tags` - returns tags matching the given `tagPrefix` and/or `expr`. See [these docs](https://graphite.readthedocs.io/en/stable/tags.html#auto-complete-support).\n - `tags/autoComplete/values` - returns tag values matching the given `valuePrefix` and/or `expr`. See [these docs](https://graphite.readthedocs.io/en/stable/tags.html#auto-complete-support).\n - `tags/delSeries` - deletes series matching the given `path`. See [these docs](https://graphite.readthedocs.io/en/stable/tags.html#removing-series-from-the-tagdb).\n\n* URL for query stats across all tenants: `http://<vmselect>:8481/api/v1/status/top_queries`. It lists with the most frequently executed queries and queries taking the most duration.\n\n* URL for time series deletion: `http://<vmselect>:8481/delete/<accountID>/prometheus/api/v1/admin/tsdb/delete_series?match[]=<timeseries_selector_for_delete>`.\n Note that the `delete_series` handler should be used only in exceptional cases such as deletion of accidentally ingested incorrect time series. It shouldn't\n be used on a regular basis, since it carries non-zero overhead.\n\n* `vmstorage` nodes provide the following HTTP endpoints on `8482` port:\n - `/internal/force_merge` - initiate [forced compactions](https://victoriametrics.github.io/#forced-merge) on the given `vmstorage` node.\n - `/snapshot/create` - create [instant snapshot](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282),\n which can be used for backups in background. Snapshots are created in `<storageDataPath>/snapshots` folder, where `<storageDataPath>` is the corresponding\n command-line flag value.\n - `/snapshot/list` - list available snasphots.\n - `/snapshot/delete?snapshot=<id>` - delete the given snapshot.\n - `/snapshot/delete_all` - delete all the snapshots.\n\n Snapshots may be created independently on each `vmstorage` node. There is no need in synchronizing snapshots' creation\n across `vmstorage` nodes.\n\n\n## Cluster resizing and scalability\n\nCluster performance and capacity scales with adding new nodes.\n\n* `vminsert` and `vmselect` nodes are stateless and may be added / removed at any time.\n Do not forget updating the list of these nodes on http load balancer.\n Adding more `vminsert` nodes scales data ingestion rate. See [this comment](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/175#issuecomment-536925841)\n about ingestion rate scalability.\n Adding more `vmselect` nodes scales select queries rate.\n* `vmstorage` nodes own the ingested data, so they cannot be removed without data loss.\n Adding more `vmstorage` nodes scales cluster capacity.\n\nSteps to add `vmstorage` node:\n\n1. Start new `vmstorage` node with the same `-retentionPeriod` as existing nodes in the cluster.\n2. Gradually restart all the `vmselect` nodes with new `-storageNode` arg containing `<new_vmstorage_host>:8401`.\n3. Gradually restart all the `vminsert` nodes with new `-storageNode` arg containing `<new_vmstorage_host>:8400`.\n\n\n## Updating / reconfiguring cluster nodes\n\nAll the node types - `vminsert`, `vmselect` and `vmstorage` - may be updated via graceful shutdown.\nSend `SIGINT` signal to the corresponding process, wait until it finishes and then start new version\nwith new configs.\n\nCluster should remain in working state if at least a single node of each type remains available during\nthe update process. See [cluster availability](#cluster-availability) section for details.\n\n\n## Cluster availability\n\n* HTTP load balancer must stop routing requests to unavailable `vminsert` and `vmselect` nodes.\n* The cluster remains available if at least a single `vmstorage` node exists:\n\n - `vminsert` re-routes incoming data from unavailable `vmstorage` nodes to healthy `vmstorage` nodes\n - `vmselect` continues serving partial responses if at least a single `vmstorage` node is available. If consistency over availability is preferred, then either pass `-search.denyPartialResponse` command-line flag to `vmselect` or pass `deny_partial_response=1` query arg in requests to `vmselect`.\n\nData replication can be used for increasing storage durability. See [these docs](#replication-and-data-safety) for details.\n\n\n## Capacity planning\n\nEach instance type - `vminsert`, `vmselect` and `vmstorage` - can run on the most suitable hardware.\n\n### vminsert\n\n* The recommended total number of vCPU cores for all the `vminsert` instances can be calculated from the ingestion rate: `vCPUs = ingestion_rate / 150K`.\n* The recommended number of vCPU cores per each `vminsert` instance should equal to the number of `vmstorage` instances in the cluster.\n* The amount of RAM per each `vminsert` instance should be 1GB or more. RAM is used as a buffer for spikes in ingestion rate.\n The maximum amount of used RAM per `vminsert` node can be tuned with `-memory.allowedPercent` or `-memory.allowedBytes` command-line flags.\n For instance, `-memory.allowedPercent=20` limits the maximum amount of used RAM to 20% of the available RAM on the host system.\n* Sometimes `-rpc.disableCompression` command-line flag on `vminsert` instances could increase ingestion capacity at the cost\n of higher network bandwidth usage between `vminsert` and `vmstorage`.\n\n### vmstorage\n\n* The recommended total number of vCPU cores for all the `vmstorage` instances can be calculated from the ingestion rate: `vCPUs = ingestion_rate / 150K`.\n* The recommended total amount of RAM for all the `vmstorage` instances can be calculated from the number of active time series: `RAM = 2 * active_time_series * 1KB`.\n Time series is active if it received at least a single data point during the last hour or if it has been queried during the last hour.\n The required RAM per each `vmstorage` should be multiplied by `-replicationFactor` if [replication](#replication-and-data-safety) is enabled.\n Additional RAM can be required for query processing.\n Calculated RAM requrements may differ from actual RAM requirements due to various factors:\n * The average number of labels per time series. More labels require more RAM.\n * The average length of label names and label values. Longer labels require more RAM.\n * The type of queries. Heavy queries that scan big number of time series over long time ranges require more RAM.\n* The recommended total amount of storage space for all the `vmstorage` instances can be calculated\n from the ingestion rate and retention: `storage_space = ingestion_rate * retention_seconds`.\n\n### vmselect\n\nThe recommended hardware for `vmselect` instances highly depends on the type of queries. Lightweight queries over small number of time series usually require\nsmall number of vCPU cores and small amount of RAM on `vmselect`, while heavy queries over big number of time series (>10K) usually require\nbigger number of vCPU cores and bigger amounts of RAM.\n\nIn general it is recommended increasing the number of vCPU cores and RAM per `vmselect` node for higher query performance,\nwhile adding new `vmselect` nodes only when old nodes are overloaded with incoming query stream.\n\n\n## High availability\n\nIt is recommended to run all the components for a single cluster in the same subnetwork with high bandwidth, low latency and low error rates.\nThis improves cluster performance and availability.\nIt isn't recommended spreading components for a single cluster across multiple availability zones, since cross-AZ network usually has lower bandwidth, higher latency\nand higher error rates comparing the network inside AZ.\n\nIf you need multi-AZ setup, then it is recommended running independed clusters in each AZ and setting up\n[vmagent](https://victoriametrics.github.io/vmagent.html) in front of these clusters, so it could replicate incoming data\ninto all the cluster. Then [promxy](https://github.com/jacksontj/promxy) could be used for querying the data from multiple clusters.\n\n\n## Helm\n\nHelm chart simplifies managing cluster version of VictoriaMetrics in Kubernetes.\nIt is available in the [helm-charts](https://github.com/VictoriaMetrics/helm-charts) repository.\n\n\n## Kubernetes operator\n\n[K8s operator](https://github.com/VictoriaMetrics/operator) simplifies managing VictoriaMetrics components in Kubernetes.\n\n\n## Replication and data safety\n\nBy default VictoriaMetrics offloads replication to the underlying storage pointed by `-storageDataPath`.\n\nThe replication can be enabled by passing `-replicationFactor=N` command-line flag to `vminsert`.\nThis guarantees that all the data remains available for querying if up to `N-1` `vmstorage` nodes are unavailable.\nThe cluster must contain at least `2*N-1` `vmstorage` nodes, where `N`\nis replication factor, in order to maintain the given replication factor for newly ingested data when `N-1` of storage nodes are lost.\nFor example, when `-replicationFactor=3` is passed to `vminsert`, then it replicates all the ingested data to 3 distinct `vmstorage` nodes,\nso up to 2 `vmstorage` nodes can be lost without data loss. The minimum number of `vmstorage` nodes should be equal to `2*3-1 = 5`, so when 2 `vmstorage` nodes are lost,\nthe remaining 3 `vmstorage` nodes could provide the `-replicationFactor=3` for newly ingested data.\n\nWhen the replication is enabled, `-replicationFactor=N` and `-dedup.minScrapeInterval=1ms` command-line flag must be passed to `vmselect` nodes.\nThe `-replicationFactor=N` improves query performance when a part of vmstorage nodes respond slowly and/or temporarily unavailable.\nThe `-dedup.minScrapeInterval=1ms` de-duplicates replicated data during queries. It is OK if `-dedup.minScrapeInterval` exceeds 1ms\nwhen [deduplication](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#deduplication) is used additionally to replication.\n\nNote that [replication doesn't save from disaster](https://medium.com/@valyala/speeding-up-backups-for-big-time-series-databases-533c1a927883),\nso it is recommended performing regular backups. See [these docs](#backups) for details.\n\nNote that the replication increases resource usage - CPU, RAM, disk space, network bandwidth - by up to `-replicationFactor` times. So it may be worth\noffloading the replication to underlying storage pointed by `-storageDataPath` such as [Google Compute Engine persistent disk](https://cloud.google.com/compute/docs/disks/#pdspecs),\nwhich is protected from data loss and data corruption. It also provide consistently high performance\nand [may be resized](https://cloud.google.com/compute/docs/disks/add-persistent-disk) without downtime.\nHDD-based persistent disks should be enough for the majority of use cases.\n\nIt is recommended using durable replicated persistent volumes in Kubernetes.\n\n\n## Backups\n\nIt is recommended performing periodical backups from [instant snapshots](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)\nfor protecting from user errors such as accidental data deletion.\n\nThe following steps must be performed for each `vmstorage` node for creating a backup:\n\n1. Create an instant snapshot by navigating to `/snapshot/create` HTTP handler. It will create snapshot and return its name.\n2. Archive the created snapshot from `<-storageDataPath>/snapshots/<snapshot_name>` folder using [vmbackup](https://victoriametrics.github.io/vbackup.html).\n The archival process doesn't interfere with `vmstorage` work, so it may be performed at any suitable time.\n3. Delete unused snapshots via `/snapshot/delete?snapshot=<snapshot_name>` or `/snapshot/delete_all` in order to free up occupied storage space.\n\nThere is no need in synchronizing backups among all the `vmstorage` nodes.\n\nRestoring from backup:\n\n1. Stop `vmstorage` node with `kill -INT`.\n2. Restore data from backup using [vmrestore](https://victoriametrics.github.io/vmrestore.html) into `-storageDataPath` directory.\n3. Start `vmstorage` node.\n\n\n## Profiling\n\nAll the cluster components provide the following handlers for [profiling](https://blog.golang.org/profiling-go-programs):\n\n* `http://vminsert:8480/debug/pprof/heap` for memory profile and `http://vminsert:8480/debug/pprof/profile` for CPU profile\n* `http://vmselect:8481/debug/pprof/heap` for memory profile and `http://vmselect:8481/debug/pprof/profile` for CPU profile\n* `http://vmstorage:8482/debug/pprof/heap` for memory profile and `http://vmstorage:8482/debug/pprof/profile` for CPU profile\n\nExample command for collecting cpu profile from `vmstorage`:\n\n```bash\ncurl -s http://vmstorage:8482/debug/pprof/profile > cpu.pprof\n```\n\nExample command for collecting memory profile from `vminsert`:\n\n```bash\ncurl -s http://vminsert:8480/debug/pprof/heap > mem.pprof\n```\n\n\n## Community and contributions\n\nWe are open to third-party pull requests provided they follow [KISS design principle](https://en.wikipedia.org/wiki/KISS_principle):\n\n- Prefer simple code and architecture.\n- Avoid complex abstractions.\n- Avoid magic code and fancy algorithms.\n- Avoid [big external dependencies](https://medium.com/@valyala/stripping-dependency-bloat-in-victoriametrics-docker-image-983fb5912b0d).\n- Minimize the number of moving parts in the distributed system.\n- Avoid automated decisions, which may hurt cluster availability, consistency or performance.\n\nAdhering `KISS` principle simplifies the resulting code and architecture, so it can be reviewed, understood and verified by many people.\n\nDue to `KISS` cluster version of VictoriaMetrics has no the following \"features\" popular in distributed computing world:\n\n- Fragile gossip protocols. See [failed attempt in Thanos](https://github.com/improbable-eng/thanos/blob/030bc345c12c446962225221795f4973848caab5/docs/proposals/completed/201809_gossip-removal.md).\n- Hard-to-understand-and-implement-properly [Paxos protocols](https://www.quora.com/In-distributed-systems-what-is-a-simple-explanation-of-the-Paxos-algorithm).\n- Complex replication schemes, which may go nuts in unforesseen edge cases. See [replication docs](#replication-and-data-safety) for details.\n- Automatic data reshuffling between storage nodes, which may hurt cluster performance and availability.\n- Automatic cluster resizing, which may cost you a lot of money if improperly configured.\n- Automatic discovering and addition of new nodes in the cluster, which may mix data between dev and prod clusters :)\n- Automatic leader election, which may result in split brain disaster on network errors.\n\n\n## Reporting bugs\n\nReport bugs and propose new features [here](https://github.com/VictoriaMetrics/VictoriaMetrics/issues).\n\n\n## Victoria Metrics Logo\n\n[Zip](VM_logo.zip) contains three folders with different image orientation (main color and inverted version).\n\nFiles included in each folder:\n\n* 2 JPEG Preview files\n* 2 PNG Preview files with transparent background\n* 2 EPS Adobe Illustrator EPS10 files\n\n\n### Logo Usage Guidelines\n\n#### Font used:\n\n* Lato Black\n* Lato Regular\n\n#### Color Palette:\n\n* HEX [#110f0f](https://www.color-hex.com/color/110f0f)\n* HEX [#ffffff](https://www.color-hex.com/color/ffffff)\n\n### We kindly ask:\n\n- Please don't use any other font instead of suggested.\n- There should be sufficient clear space around the logo.\n- Do not change spacing, alignment, or relative locations of the design elements.\n- Do not change the proportions of any of the design elements or the design itself. You may resize as needed but must retain all proportions.\n","dir":"/","name":"Cluster-VictoriaMetrics.md","path":"Cluster-VictoriaMetrics.md","url":"/Cluster-VictoriaMetrics.html"},{"layout":"default","title":"MetricsQL","content":"# MetricsQL\n\nThe page has been moved to [MetricsQL](https://victoriametrics.github.io/MetricsQL.html).\n","dir":"/","name":"ExtendedPromQL.md","path":"ExtendedPromQL.md","url":"/ExtendedPromQL.html"},{"layout":"default","title":"Docs","content":"# Docs\n\n* [Quick start](Quick-Start)\n* [`WITH` templates playground](https://play.victoriametrics.com/promql/expand-with-exprs)\n* [Grafana playground](http://play-grafana.victoriametrics.com:3000/d/4ome8yJmz/node-exporter-on-victoriametrics-demo)\n* [MetricsQL](MetricsQL)\n* [Single-node version](Single-server-VictoriaMetrics)\n* [FAQ](FAQ)\n* [Cluster version](Cluster-VictoriaMetrics)\n* [Articles](Articles)\n* [Case Studies](CaseStudies)\n* [vmbackup](vmbackup)\n* [vmrestore](vmrestore)\n* [vmagent](vmagent)\n","dir":"/","name":"Home.md","path":"Home.md","url":"/Home.html"},{"layout":"default","title":"Quick Start","content":"# Quick Start\n\n1. If you run Ubuntu, then just run `snap install victoriametrics` command in order to install and start VictoriaMetrics, then read [these docs](https://snapcraft.io/victoriametrics).\n Otherwise download the latest VictoriaMetrics release from [releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases),\n from [Docker hub](https://hub.docker.com/r/victoriametrics/victoria-metrics/)\n or [build it from sources](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-build-from-sources).\n\n2. This step isn't needed if you run VictoriaMetrics via `snap install victoriametrics` as described above.\n Otherwise run the binary or Docker image with the desired command-line flags. Pass `-help` in order to see description for all the available flags\n and their default values. Default flag values should fit the majoirty of cases. The minimum required flags to configure are:\n\n * `-storageDataPath` - path to directory where VictoriaMetrics stores all the data.\n * `-retentionPeriod` - data retention.\n\n For instance:\n\n `./victoria-metrics-prod -storageDataPath=/var/lib/victoria-metrics-data -retentionPeriod=3`\n\n See [these instructions](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/43) in order to configure VictoriaMetrics as OS service.\n It is recommended setting up [VictoriaMetrics monitoring](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#monitoring).\n\n3. Configure [vmagent](https://victoriametrics.github.io/vmagent.html) or Prometheus to write data to VictoriaMetrics.\n It is recommended to use `vmagent` instead of Prometheus, since it is more resource efficient. If you still prefer Prometheus, then\n see [these instructions](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#prometheus-setup)\n for details on how to configure Prometheus.\n\n4. Configure Grafana to query VictoriaMetrics instead of Prometheus.\n See [these instructions](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#grafana-setup).\n\n\nThere is also [cluster version](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster) and [SaaS playground](https://play.victoriametrics.com/signIn).\n","dir":"/","name":"Quick-Start.md","path":"Quick-Start.md","url":"/Quick-Start.html"},{"permalink":"//","layout":"default","title":"starter-slim","content":"# starter-slim\n\n1. [Generate with the same files and folders](https://github.com/rundocs/starter-slim/generate) from this repository\n2. Set up your GitHub Pages to source(`/`)\n3. Now you can view your documentation in your site\n\n## site.pages\n\n<!-- prettier-ignore-start -->\n\n| source | link |\n| --------------- | -------------------------------------------------------------- |\n{% for page in site.pages -%}\n| {{ page.path }} | [{{ page.url | relative_url }}]({{ page.url | relative_url }}) |\n{% endfor %}\n\n<!-- prettier-ignore-end -->\n\n## Documents\n\nhttps://jekyll-rtd-theme.rundocs.io\n\n## Local debug\n\n```sh\ngem install jekyll bundler\n\nbundle install\n\nJEKYLL_GITHUB_TOKEN=blank PAGES_API_URL=http://0.0.0.0 bundle exec jekyll server --livereload\n```\n\n## The license\n\nThe theme is available as open source under the terms of the MIT License\n","dir":"/","name":"README.md","path":"README.md","url":"/"},{"layout":"default","title":null,"content":"Release process guidance\n\n## Release version and Docker images\n\n0. Document all the changes for new release in [CHANGELOG.md](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/docs/CHANGELOG.md).\n1. Create release tag with `git tag v1.xx.y` in `master` branch and `git tag v1.xx.y-cluster` in `cluster` branch.\n2. Run `make release` for creating `*.tar.gz` release archive with the corresponding `_checksums.txt` inside `bin` directory.\n3. Run `make publish` for creating and publishing Docker images.\n4. Repeat steps 3-4 for `cluster` branch.\n5. Push release tag to https://github.com/VictoriaMetrics/VictoriaMetrics : `git push origin v1.xx.y`.\n6. Go to https://github.com/VictoriaMetrics/VictoriaMetrics/releases , create new release from the pushed tag on step 5 and upload `*.tar.gz` archive with the corresponding `_checksums.txt` from step 2.\n\n## Building snap package.\n\n pre-requirements: \n- snapcraft binary, can be installed with commands:\n for MacOS `brew install snapcraft` and [install mutipass](https://discourse.ubuntu.com/t/installing-multipass-on-macos/8329),\n for Ubuntu - `sudo snap install snapcraft --classic`\n- login with `snapcraft login`\n- already created release at github (it operates `git describe` version, so git tag must be annotated).\n\n0. checkout to the latest git tag for single-node version.\n1. execute `make release-snap` - it must build and upload snap package.\n2. promote release to current, if needed manually at release page [snapcraft-releases](https://snapcraft.io/victoriametrics/releases)\n\n### Public Announcement \n\n1. Publish message in slack (victoriametrics.slack.com, general channel)\n2. Post twit with release notes URL\n3. Post in subreddit https://www.reddit.com/r/VictoriaMetrics/ \n4. Post in linkedin\n\n## Helm Charts\n\nThe helm chart repository [https://github.com/VictoriaMetrics/helm-charts/](https://github.com/VictoriaMetrics/helm-charts/)\n\n\n### Bump the version of images. \nIn that case, don't need to bump the helm chart version\n\n1. Need to update [`values.yaml`](https://github.com/VictoriaMetrics/helm-charts/blob/master/charts/victoria-metrics-cluster/values.yaml), bump version for `vmselect`, `vminsert` and `vmstorage`\n2. Specify the correct version in [`Chart.yaml`](https://github.com/VictoriaMetrics/helm-charts/blob/master/charts/victoria-metrics-cluster/Chart.yaml)\n3. Update version [README.md](https://github.com/VictoriaMetrics/helm-charts/blob/master/charts/victoria-metrics-cluster/README.md), specify the new version in the documentation\n4. Push changes to master. `master` is a source of truth\n5. Rebase `master` into `gh-pages` branch\n6. Run `make package` which creates or updates zip file with the packed chart\n7. Run `make merge`. It creates or updates metadata for charts in index.yaml \n8. Push the changes to `gh-pages` branch \n\n### Updating the chart.\n1. Update chart version in [`Chart.yaml`](https://github.com/VictoriaMetrics/helm-charts/blob/master/charts/victoria-metrics-cluster/Chart.yaml)\n2. Update [README.md](https://github.com/VictoriaMetrics/helm-charts/blob/master/charts/victoria-metrics-cluster/README.md) file, reflect changes in the documentation.\n3. Repeat the procedure from step _4_ previous section.\n\n\n## Wiki pages\n\nAll changes from `docs` folder and `.md` extension automatically push to Wiki\n\n**_Note_**: no vice versa, direct changes on Wiki will be overitten after any changes in `docs/*.md` \n\n## Github pages\n\nAll changes in `README.md`, `docs` folder and `.md` extension automatically push to Wiki\n","dir":"/","name":"Release-Guide.md","path":"Release-Guide.md","url":"/Release-Guide.html"},{"layout":"default","title":"Sample size calculations","content":"# Sample size calculations\n\nThese calculations are for the “Lowest sample size” graph at https://victoriametrics.com/ .\n\nHow many metrics can be stored in 2tb disk for 2 years?\n\nSeconds in 2 years:\n2 years * 365 days * 24 hours * 60 minutes * 60 seconds = 63072000 seconds\n\nResolution = 1 point per 10 second\n\nThat means each metric will contain 6307200 points.\n\n2tb disk contains\n2 (tb) * 1024 (gb) * 1024 (mb) * 1024 (kb) * 1024 (b) = 2199023255552 bytes\n\n# VictoriaMetrics\nBased on production data from our customers, sample size is 0.4 byte\nThat means one metric with 10 seconds resolution will need\n6307200 points * 0.4 bytes/point = 2522880 bytes or 2.4 megabytes.\nCalculation for number of metrics can be stored in 2 tb disk:\n2199023255552 (disk size) / 2522880 (one metric for 2 year) = 871632 metrics\nSo in 2tb we can store 871 632 metrics\n\n# Graphite\nBased on https://m30m.github.io/whisper-calculator/ sample size of graphite metrics is 12b + 28b for each metric\nThat means, one metric with 10 second resolution will need 75686428 bytes or 72.18 megabytes\nCalculation for number of metrics can be stored in 2 tb disk:\n2199023255552 / 75686428 = 29 054 metrics\n\n# OpenTSDB\nLet's check official openTSDB site\nhttp://opentsdb.net/faq.html\n16 bytes of HBase overhead, 3 bytes for the metric, 4 bytes for the timestamp, 6 bytes per tag, 2 bytes of OpenTSDB overhead, up to 8 bytes for the value. Integers are stored with variable length encoding and can consume 1, 2, 4 or 8 bytes.\nThat means, one metric with 10 second resolution will need\n6307200 * (1 + 4) + 3 + 16 + 2 = 31536021 bytes or 30 megabytes in the best scenario and\n6307200 * (8 + 4) + 3 + 16 + 2 = 75686421 bytes or 72 megabytes in the worst scenario.\n\nCalculation for number of metrics can be stored in 2 tb disk:\n\n2199023255552 / 31536021 = 69 730 metrics for best scenario\n2199023255552 / 75686421 = 29 054 metrics for worst scenario\n\nAlso, openTSDB allows to use compression\n\" LZO is able to achieve a compression factor of 4.2x \"\nSo, let's multiply numbers on 4.2\n69 730 * 4,2 = 292 866 metrics for best scenario\n29 054 * 4,2 = 122 026 metrics for worst scenario\n# m3db\nLet's look at official m3db site https://m3db.github.io/m3/m3db/architecture/engine/\nThey can achieve a sample size of 1.45 bytes/datapoint\nThat means, one metric with 10 second resolution will need 9145440 bytes or 8,72177124 megabytes\nCalculation for number of metrics can be stored in 2 tb disk:\n2199023255552 / 9145440 = 240 450 metrics\n\n# InfluxDB\nBased on official influxDB site https://docs.influxdata.com/influxdb/v1.8/guides/hardware_sizing/#bytes-and-compression\n\"Non-string values require approximately three bytes\". That means, one metric with 10 second resolution will need\n6307200 * 3 = 18921600 bytes or 18 megabytes\nCalculation for number of metrics can be stored in 2 tb disk:\n\n2199023255552 / 18921600 = 116 217 metrics\n\n# Prometheus\nLet's check official site: https://prometheus.io/docs/prometheus/latest/storage/\n\"On average, Prometheus uses only around 1-2 bytes per sample.\"\nThat means, one metric with 10 second resolution will need\n6307200 * 1 = 6307200 bytes in best scenario\n6307200 * 2 = 12614400 bytes in worst scenario.\n\nCalculation for number of metrics can be stored in 2 tb disk:\n\n2199023255552 / 6307200 = 348 652 metrics for the best case\n2199023255552 / 12614400 = 174 326 metrics for the worst cases\n","dir":"/","name":"SampleSizeCalculations.md","path":"SampleSizeCalculations.md","url":"/SampleSizeCalculations.html"},{"layout":"default","title":null,"content":"[![Latest Release](https://img.shields.io/github/release/VictoriaMetrics/VictoriaMetrics.svg?style=flat-square)](https://github.com/VictoriaMetrics/VictoriaMetrics/releases/latest)\n[![Docker Pulls](https://img.shields.io/docker/pulls/victoriametrics/victoria-metrics.svg?maxAge=604800)](https://hub.docker.com/r/victoriametrics/victoria-metrics)\n[![Slack](https://img.shields.io/badge/join%20slack-%23victoriametrics-brightgreen.svg)](http://slack.victoriametrics.com/)\n[![GitHub license](https://img.shields.io/github/license/VictoriaMetrics/VictoriaMetrics.svg)](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/LICENSE)\n[![Go Report](https://goreportcard.com/badge/github.com/VictoriaMetrics/VictoriaMetrics)](https://goreportcard.com/report/github.com/VictoriaMetrics/VictoriaMetrics)\n[![Build Status](https://github.com/VictoriaMetrics/VictoriaMetrics/workflows/main/badge.svg)](https://github.com/VictoriaMetrics/VictoriaMetrics/actions)\n[![codecov](https://codecov.io/gh/VictoriaMetrics/VictoriaMetrics/branch/master/graph/badge.svg)](https://codecov.io/gh/VictoriaMetrics/VictoriaMetrics)\n\n![Victoria Metrics logo](logo.png \"Victoria Metrics\")\n\n## VictoriaMetrics\n\nVictoriaMetrics is a fast, cost-effective and scalable monitoring solution and time series database.\n\nIt is available in [binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases),\n[docker images](https://hub.docker.com/r/victoriametrics/victoria-metrics/), [Snap package](https://snapcraft.io/victoriametrics)\nand in [source code](https://github.com/VictoriaMetrics/VictoriaMetrics). Just download VictoriaMetrics and see [how to start it](#how-to-start-victoriametrics).\nIf you use Ubuntu, then just run `snap install victoriametrics` in order to install and run it.\nThen read [Prometheus setup](#prometheus-setup) and [Grafana setup](#grafana-setup) docs.\n\nCluster version is available [here](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster).\n\nSee additional docs at our [Wiki](https://github.com/VictoriaMetrics/VictoriaMetrics/wiki).\n\n[Contact us](mailto:info@victoriametrics.com) if you need paid enterprise support for VictoriaMetrics.\nSee [features available for enterprise customers](https://victoriametrics.com/enterprise.html).\n\n\n## Case studies and talks\n\nAlphabetically sorted links to case studies:\n\n* [adidas](https://victoriametrics.github.io/CaseStudies.html#adidas)\n* [Adsterra](https://victoriametrics.github.io/CaseStudies.html#adsterra)\n* [ARNES](https://victoriametrics.github.io/CaseStudies.html#arnes)\n* [Brandwatch](https://victoriametrics.github.io/CaseStudies.html#brandwatch)\n* [CERN](https://victoriametrics.github.io/CaseStudies.html#cern)\n* [COLOPL](https://victoriametrics.github.io/CaseStudies.html#colopl)\n* [Dreamteam](https://victoriametrics.github.io/CaseStudies.html#dreamteam)\n* [Idealo.de](https://victoriametrics.github.io/CaseStudies.html#idealode)\n* [MHI Vestas Offshore Wind](https://victoriametrics.github.io/CaseStudies.html#mhi-vestas-offshore-wind)\n* [Synthesio](https://victoriametrics.github.io/CaseStudies.html#synthesio)\n* [Wedos.com](https://victoriametrics.github.io/CaseStudies.html#wedoscom)\n* [Wix.com](https://victoriametrics.github.io/CaseStudies.html#wixcom)\n* [Zerodha](https://victoriametrics.github.io/CaseStudies.html#zerodha)\n* [zhihu](https://victoriametrics.github.io/CaseStudies.html#zhihu)\n\n\n## Prominent features\n\n* VictoriaMetrics can be used as long-term storage for Prometheus or for [vmagent](https://victoriametrics.github.io/vmagent.html).\n See [these docs](#prometheus-setup) for details.\n* VictoriaMetrics supports [Prometheus querying API](https://prometheus.io/docs/prometheus/latest/querying/api/), so it can be used as Prometheus drop-in replacement in Grafana.\n* VictoriaMetrics implements [MetricsQL](https://victoriametrics.github.io/MetricsQL.html) query language backwards compatible with PromQL.\n* VictoriaMetrics provides global query view. Multiple Prometheus instances or any other data sources may ingest data into VictoriaMetrics.\n Later this data may be queried via a single query.\n* High performance and good scalability for both [inserts](https://medium.com/@valyala/high-cardinality-tsdb-benchmarks-victoriametrics-vs-timescaledb-vs-influxdb-13e6ee64dd6b)\n and [selects](https://medium.com/@valyala/when-size-matters-benchmarking-victoriametrics-vs-timescale-and-influxdb-6035811952d4).\n [Outperforms InfluxDB and TimescaleDB by up to 20x](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae).\n* [Uses 10x less RAM than InfluxDB](https://medium.com/@valyala/insert-benchmarks-with-inch-influxdb-vs-victoriametrics-e31a41ae2893)\n and [up to 7x less RAM than Prometheus, Thanos or Cortex](https://valyala.medium.com/prometheus-vs-victoriametrics-benchmark-on-node-exporter-metrics-4ca29c75590f)\n when dealing with millions of unique time series (aka high cardinality).\n* Optimized for time series with high churn rate. Think about [prometheus-operator](https://github.com/coreos/prometheus-operator) metrics from frequent deployments in Kubernetes.\n* High data compression, so [up to 70x more data points](https://medium.com/@valyala/when-size-matters-benchmarking-victoriametrics-vs-timescale-and-influxdb-6035811952d4)\n may be crammed into limited storage comparing to TimescaleDB\n and [up to 7x less storage space is required comparing to Prometheus, Thanos or Cortex](https://valyala.medium.com/prometheus-vs-victoriametrics-benchmark-on-node-exporter-metrics-4ca29c75590f).\n* Optimized for storage with high-latency IO and low IOPS (HDD and network storage in AWS, Google Cloud, Microsoft Azure, etc).\n See [graphs from these benchmarks](https://medium.com/@valyala/high-cardinality-tsdb-benchmarks-victoriametrics-vs-timescaledb-vs-influxdb-13e6ee64dd6b).\n* A single-node VictoriaMetrics may substitute moderately sized clusters built with competing solutions such as Thanos, M3DB, Cortex, InfluxDB or TimescaleDB.\n See [vertical scalability benchmarks](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae),\n [comparing Thanos to VictoriaMetrics cluster](https://medium.com/@valyala/comparing-thanos-to-victoriametrics-cluster-b193bea1683)\n and [Remote Write Storage Wars](https://promcon.io/2019-munich/talks/remote-write-storage-wars/) talk\n from [PromCon 2019](https://promcon.io/2019-munich/talks/remote-write-storage-wars/).\n* Easy operation:\n * VictoriaMetrics consists of a single [small executable](https://medium.com/@valyala/stripping-dependency-bloat-in-victoriametrics-docker-image-983fb5912b0d) without external dependencies.\n * All the configuration is done via explicit command-line flags with reasonable defaults.\n * All the data is stored in a single directory pointed by `-storageDataPath` command-line flag.\n * Easy and fast backups from [instant snapshots](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)\n to S3 or GCS with [vmbackup](https://victoriametrics.github.io/vmbackup.html) / [vmrestore](https://victoriametrics.github.io/vmrestore.html).\n See [this article](https://medium.com/@valyala/speeding-up-backups-for-big-time-series-databases-533c1a927883) for more details.\n* Storage is protected from corruption on unclean shutdown (i.e. OOM, hardware reset or `kill -9`) thanks to [the storage architecture](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).\n* Supports metrics' scraping, ingestion and [backfilling](#backfilling) via the following protocols:\n * [Metrics from Prometheus exporters](https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format)\n such as [node_exporter](https://github.com/prometheus/node_exporter). See [these docs](#how-to-scrape-prometheus-exporters-such-as-node-exporter) for details.\n * [Prometheus remote write API](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write)\n * [InfluxDB line protocol](#how-to-send-data-from-influxdb-compatible-agents-such-as-telegraf) over HTTP, TCP and UDP.\n * [Graphite plaintext protocol](#how-to-send-data-from-graphite-compatible-agents-such-as-statsd) with [tags](https://graphite.readthedocs.io/en/latest/tags.html#carbon)\n if `-graphiteListenAddr` is set.\n * [OpenTSDB put message](#sending-data-via-telnet-put-protocol) if `-opentsdbListenAddr` is set.\n * [HTTP OpenTSDB /api/put requests](#sending-opentsdb-data-via-http-apiput-requests) if `-opentsdbHTTPListenAddr` is set.\n * [JSON line format](#how-to-import-data-in-json-line-format).\n * [Native binary format](#how-to-import-data-in-native-format).\n * [Prometheus exposition format](#how-to-import-data-in-prometheus-exposition-format).\n * [Arbitrary CSV data](#how-to-import-csv-data).\n* Supports metrics' relabeling. See [these docs](#relabeling) for details.\n* Ideally works with big amounts of time series data from Kubernetes, IoT sensors, connected cars, industrial telemetry, financial data and various Enterprise workloads.\n* Has open source [cluster version](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster).\n* See also technical [Articles about VictoriaMetrics](https://victoriametrics.github.io/Articles.html).\n\n\n## Operation\n\n### Table of contents\n\n* [How to start VictoriaMetrics](#how-to-start-victoriametrics)\n * [Environment variables](#environment-variables)\n * [Configuration with snap package](#configuration-with-snap-package)\n* [Prometheus setup](#prometheus-setup)\n* [Grafana setup](#grafana-setup)\n* [How to upgrade VictoriaMetrics](#how-to-upgrade-victoriametrics)\n* [How to apply new config to VictoriaMetrics](#how-to-apply-new-config-to-victoriametrics)\n* [How to scrape Prometheus exporters such as node_exporter](#how-to-scrape-prometheus-exporters-such-as-node-exporter)\n* [How to send data from InfluxDB-compatible agents such as Telegraf](#how-to-send-data-from-influxdb-compatible-agents-such-as-telegraf)\n* [How to send data from Graphite-compatible agents such as StatsD](#how-to-send-data-from-graphite-compatible-agents-such-as-statsd)\n* [Querying Graphite data](#querying-graphite-data)\n* [How to send data from OpenTSDB-compatible agents](#how-to-send-data-from-opentsdb-compatible-agents)\n* [Prometheus querying API usage](#prometheus-querying-api-usage)\n * [Prometheus querying API enhancements](#prometheus-querying-api-enhancements)\n* [Graphite API usage](#graphite-api-usage)\n * [Graphite Metrics API usage](#graphite-metrics-api-usage)\n * [Graphite Tags API usage](#graphite-tags-api-usage)\n* [How to build from sources](#how-to-build-from-sources)\n * [Development build](#development-build)\n * [Production build](#production-build)\n * [ARM build](#arm-build)\n * [Pure Go build (CGO_ENABLED=0)](#pure-go-build-cgo_enabled0)\n * [Building docker images](#building-docker-images)\n* [Start with docker-compose](#start-with-docker-compose)\n* [Setting up service](#setting-up-service)\n* [How to work with snapshots](#how-to-work-with-snapshots)\n* [How to delete time series](#how-to-delete-time-series)\n* [Forced merge](#forced-merge)\n* [How to export time series](#how-to-export-time-series)\n * [How to export data in native format](#how-to-export-data-in-native-format)\n * [How to export data in JSON line format](#how-to-export-data-in-json-line-format)\n * [How to export CSV data](#how-to-export-csv-data)\n* [How to import time series data](#how-to-import-time-series-data)\n * [How to import data in native format](#how-to-import-data-in-native-format)\n * [How to import data in json line format](#how-to-import-data-in-json-line-format)\n * [How to import CSV data](#how-to-import-csv-data)\n * [How to import data in Prometheus exposition format](#how-to-import-data-in-prometheus-exposition-format)\n* [Relabeling](#relabeling)\n* [Federation](#federation)\n* [Capacity planning](#capacity-planning)\n* [High availability](#high-availability)\n* [Deduplication](#deduplication)\n* [Retention](#retention)\n* [Multiple retentions](#multiple-retentions)\n* [Downsampling](#downsampling)\n* [Multi-tenancy](#multi-tenancy)\n* [Scalability and cluster version](#scalability-and-cluster-version)\n* [Alerting](#alerting)\n* [Security](#security)\n* [Tuning](#tuning)\n* [Monitoring](#monitoring)\n* [Troubleshooting](#troubleshooting)\n* [Data migration](#data-migration)\n* [Backfilling](#backfilling)\n* [Data updates](#data-updates)\n* [Replication](#replication)\n* [Backups](#backups)\n* [Profiling](#profiling)\n* [Integrations](#integrations)\n* [Third-party contributions](#third-party-contributions)\n* [Contacts](#contacts)\n* [Community and contributions](#community-and-contributions)\n* [Reporting bugs](#reporting-bugs)\n* [Victoria Metrics Logo](#victoria-metrics-logo)\n * [Logo Usage Guidelines](#logo-usage-guidelines)\n * [Font used](#font-used)\n * [Color Palette](#color-palette)\n * [We kindly ask](#we-kindly-ask)\n\n\n## How to start VictoriaMetrics\n\nStart VictoriaMetrics [executable](https://github.com/VictoriaMetrics/VictoriaMetrics/releases)\nor [docker image](https://hub.docker.com/r/victoriametrics/victoria-metrics/) with the desired command-line flags.\n\nThe following command-line flags are used the most:\n\n* `-storageDataPath` - path to data directory. VictoriaMetrics stores all the data in this directory. Default path is `victoria-metrics-data` in the current working directory.\n* `-retentionPeriod` - retention for stored data. Older data is automatically deleted. Default retention is 1 month. See [these docs](#retention) for more details.\n\nOther flags have good enough default values, so set them only if you really need this. Pass `-help` to see all the available flags with description and default values.\n\nSee how to [ingest data to VictoriaMetrics](#how-to-import-time-series-data), how to [query VictoriaMetrics](#grafana-setup)\nand how to [handle alerts](#alerting).\nVictoriaMetrics accepts [Prometheus querying API requests](#prometheus-querying-api-usage) on port `8428` by default.\n\nIt is recommended setting up [monitoring](#monitoring) for VictoriaMetrics.\n\n### Environment variables\n\nEach flag value can be set via environment variables according to these rules:\n\n* The `-envflag.enable` flag must be set\n* Each `.` char in flag name must be substituted by `_` (for example `-insert.maxQueueDuration <duration>` will translate to `insert_maxQueueDuration=<duration>`)\n* For repeating flags an alternative syntax can be used by joining the different values into one using `,` char as separator (for example `-storageNode <nodeA> -storageNode <nodeB>` will translate to `storageNode=<nodeA>,<nodeB>`)\n* It is possible setting prefix for environment vars with `-envflag.prefix`. For instance, if `-envflag.prefix=VM_`, then env vars must be prepended with `VM_`\n\n### Configuration with snap package\n\n\n Command-line flags can be changed with following command:\n\n```text\necho 'FLAGS=\"-selfScrapeInterval=10s -search.logSlowQueryDuration=20s\"' > $SNAP_DATA/var/snap/victoriametrics/current/extra_flags\nsnap restart victoriametrics\n```\n Or add needed command-line flags to the file `$SNAP_DATA/var/snap/victoriametrics/current/extra_flags`.\n\n Note you cannot change value for `-storageDataPath` flag, for safety snap package has limited access to host system.\n\n\n Changing scrape configuration is possible with text editor:\n ```text\n vi $SNAP_DATA/var/snap/victoriametrics/current/etc/victoriametrics-scrape-config.yaml\n ```\n After changes was made, trigger config re-read with command `curl 127.0.0.1:8248/-/reload`.\n\n\n## Prometheus setup\n\nPrometheus must be configured with [remote_write](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write)\nin order to send data to VictoriaMetrics. Add the following lines\nto Prometheus config file (it is usually located at `/etc/prometheus/prometheus.yml`):\n\n```yml\nremote_write:\n - url: http://<victoriametrics-addr>:8428/api/v1/write\n```\n\nSubstitute `<victoriametrics-addr>` with hostname or IP address of VictoriaMetrics.\nThen apply new config via the following command:\n\n```bash\nkill -HUP `pidof prometheus`\n```\n\nPrometheus writes incoming data to local storage and replicates it to remote storage in parallel.\nThis means that data remains available in local storage for `--storage.tsdb.retention.time` duration\neven if remote storage is unavailable.\n\nIf you plan to send data to VictoriaMetrics from multiple Prometheus instances, then add the following lines into `global` section\nof [Prometheus config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file):\n\n```yml\nglobal:\n external_labels:\n datacenter: dc-123\n```\n\nThis instructs Prometheus to add `datacenter=dc-123` label to each time series sent to remote storage.\nThe label name can be arbitrary - `datacenter` is just an example. The label value must be unique\nacross Prometheus instances, so those time series may be filtered and grouped by this label.\n\nFor highly loaded Prometheus instances (400k+ samples per second) the following tuning may be applied:\n\n```yaml\nremote_write:\n - url: http://<victoriametrics-addr>:8428/api/v1/write\n queue_config:\n max_samples_per_send: 10000\n capacity: 20000\n max_shards: 30\n```\n\nUsing remote write increases memory usage for Prometheus up to ~25% and depends on the shape of data. If you are experiencing issues with\ntoo high memory consumption try to lower `max_samples_per_send` and `capacity` params (keep in mind that these two params are tightly connected).\nRead more about tuning remote write for Prometheus [here](https://prometheus.io/docs/practices/remote_write).\n\nIt is recommended upgrading Prometheus to [v2.12.0](https://github.com/prometheus/prometheus/releases) or newer, since previous versions may have issues with `remote_write`.\n\nTake a look also at [vmagent](https://victoriametrics.github.io/vmagent.html)\nand [vmalert](https://victoriametrics.github.io/vmalert.html),\nwhich can be used as faster and less resource-hungry alternative to Prometheus.\n\n\n## Grafana setup\n\nCreate [Prometheus datasource](http://docs.grafana.org/features/datasources/prometheus/) in Grafana with the following url:\n\n```url\nhttp://<victoriametrics-addr>:8428\n```\n\nSubstitute `<victoriametrics-addr>` with the hostname or IP address of VictoriaMetrics.\n\nThen build graphs with the created datasource using [PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/)\nor [MetricsQL](https://victoriametrics.github.io/MetricsQL.html). VictoriaMetrics supports [Prometheus querying API](#prometheus-querying-api-usage),\nwhich is used by Grafana.\n\n\n## How to upgrade VictoriaMetrics\n\nIt is safe upgrading VictoriaMetrics to new versions unless [release notes](https://github.com/VictoriaMetrics/VictoriaMetrics/releases)\nsay otherwise. It is safe skipping multiple versions during the upgrade unless [release notes](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) say otherwise.\nIt is recommended performing regular upgrades to the latest version, since it may contain important bug fixes, performance optimizations or new features.\n\nIt is also safe downgrading to the previous version unless [release notes](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) say otherwise.\n\nThe following steps must be performed during the upgrade / downgrade:\n\n* Send `SIGINT` signal to VictoriaMetrics process in order to gracefully stop it.\n* Wait until the process stops. This can take a few seconds.\n* Start the upgraded VictoriaMetrics.\n\nPrometheus doesn't drop data during VictoriaMetrics restart.\nSee [this article](https://grafana.com/blog/2019/03/25/whats-new-in-prometheus-2.8-wal-based-remote-write/) for details.\n\n\n## How to apply new config to VictoriaMetrics\n\nVictoriaMetrics is configured via command-line flags, so it must be restarted when new command-line flags should be applied:\n\n* Send `SIGINT` signal to VictoriaMetrics process in order to gracefully stop it.\n* Wait until the process stops. This can take a few seconds.\n* Start VictoriaMetrics with the new command-line flags.\n\nPrometheus doesn't drop data during VictoriaMetrics restart.\nSee [this article](https://grafana.com/blog/2019/03/25/whats-new-in-prometheus-2.8-wal-based-remote-write/) for details.\n\n\n## How to scrape Prometheus exporters such as [node-exporter](https://github.com/prometheus/node_exporter)\n\nVictoriaMetrics can be used as drop-in replacement for Prometheus for scraping targets configured in `prometheus.yml` config file according to [the specification](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file).\nJust set `-promscrape.config` command-line flag to the path to `prometheus.yml` config - and VictoriaMetrics should start scraping the configured targets.\nCurrently the following [scrape_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) types are supported:\n\n* [static_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#static_config)\n* [file_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#file_sd_config)\n* [kubernetes_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config)\n* [ec2_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config)\n* [gce_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#gce_sd_config)\n* [consul_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config)\n* [dns_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dns_sd_config)\n* [openstack_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#openstack_sd_config)\n* [dockerswarm_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dockerswarm_sd_config)\n* [eureka_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#eureka_sd_config)\n\n\nOther `*_sd_config` types will be supported in the future.\n\nThe file pointed by `-promscrape.config` may contain `%{ENV_VAR}` placeholders, which are substituted by the corresponding `ENV_VAR` environment variable values.\n\nVictoriaMetrics also supports [importing data in Prometheus exposition format](#how-to-import-data-in-prometheus-exposition-format).\n\nSee also [vmagent](https://victoriametrics.github.io/vmagent.html), which can be used as drop-in replacement for Prometheus.\n\n\n## How to send data from InfluxDB-compatible agents such as [Telegraf](https://www.influxdata.com/time-series-platform/telegraf/)\n\nUse `http://<victoriametric-addr>:8428` url instead of InfluxDB url in agents' configs.\nFor instance, put the following lines into `Telegraf` config, so it sends data to VictoriaMetrics instead of InfluxDB:\n\n```toml\n[[outputs.influxdb]]\n urls = [\"http://<victoriametrics-addr>:8428\"]\n```\n\nAnother option is to enable TCP and UDP receiver for Influx line protocol via `-influxListenAddr` command-line flag\nand stream plain Influx line protocol data to the configured TCP and/or UDP addresses.\n\nVictoriaMetrics maps Influx data using the following rules:\n\n* [`db` query arg](https://docs.influxdata.com/influxdb/v1.7/tools/api/#write-http-endpoint) is mapped into `db` label value\n unless `db` tag exists in the Influx line.\n* Field names are mapped to time series names prefixed with `{measurement}{separator}` value,\n where `{separator}` equals to `_` by default. It can be changed with `-influxMeasurementFieldSeparator` command-line flag.\n See also `-influxSkipSingleField` command-line flag.\n If `{measurement}` is empty or `-influxSkipMeasurement` command-line flag is set, then time series names correspond to field names.\n* Field values are mapped to time series values.\n* Tags are mapped to Prometheus labels as-is.\n\nFor example, the following Influx line:\n\n```raw\nfoo,tag1=value1,tag2=value2 field1=12,field2=40\n```\n\nis converted into the following Prometheus data points:\n\n```raw\nfoo_field1{tag1=\"value1\", tag2=\"value2\"} 12\nfoo_field2{tag1=\"value1\", tag2=\"value2\"} 40\n```\n\nExample for writing data with [Influx line protocol](https://docs.influxdata.com/influxdb/v1.7/write_protocols/line_protocol_tutorial/)\nto local VictoriaMetrics using `curl`:\n\n```bash\ncurl -d 'measurement,tag1=value1,tag2=value2 field1=123,field2=1.23' -X POST 'http://localhost:8428/write'\n```\n\nAn arbitrary number of lines delimited by '\\n' (aka newline char) can be sent in a single request.\nAfter that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:\n\n```bash\ncurl -G 'http://localhost:8428/api/v1/export' -d 'match={__name__=~\"measurement_.*\"}'\n```\n\nThe `/api/v1/export` endpoint should return the following response:\n\n```jsonl\n{\"metric\":{\"__name__\":\"measurement_field1\",\"tag1\":\"value1\",\"tag2\":\"value2\"},\"values\":[123],\"timestamps\":[1560272508147]}\n{\"metric\":{\"__name__\":\"measurement_field2\",\"tag1\":\"value1\",\"tag2\":\"value2\"},\"values\":[1.23],\"timestamps\":[1560272508147]}\n```\n\nNote that Influx line protocol expects [timestamps in *nanoseconds* by default](https://docs.influxdata.com/influxdb/v1.7/write_protocols/line_protocol_tutorial/#timestamp),\nwhile VictoriaMetrics stores them with *milliseconds* precision.\n\nExtra labels may be added to all the written time series by passing `extra_label=name=value` query args.\nFor example, `/write?extra_label=foo=bar` would add `{foo=\"bar\"}` label to all the ingested metrics.\n\n## How to send data from Graphite-compatible agents such as [StatsD](https://github.com/etsy/statsd)\n\nEnable Graphite receiver in VictoriaMetrics by setting `-graphiteListenAddr` command line flag. For instance,\nthe following command will enable Graphite receiver in VictoriaMetrics on TCP and UDP port `2003`:\n\n```bash\n/path/to/victoria-metrics-prod -graphiteListenAddr=:2003\n```\n\nUse the configured address in Graphite-compatible agents. For instance, set `graphiteHost`\nto the VictoriaMetrics host in `StatsD` configs.\n\nExample for writing data with Graphite plaintext protocol to local VictoriaMetrics using `nc`:\n\n```bash\necho \"foo.bar.baz;tag1=value1;tag2=value2 123 `date +%s`\" | nc -N localhost 2003\n```\n\nVictoriaMetrics sets the current time if the timestamp is omitted.\nAn arbitrary number of lines delimited by `\\n` (aka newline char) can be sent in one go.\nAfter that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:\n\n```bash\ncurl -G 'http://localhost:8428/api/v1/export' -d 'match=foo.bar.baz'\n```\n\nThe `/api/v1/export` endpoint should return the following response:\n\n```bash\n{\"metric\":{\"__name__\":\"foo.bar.baz\",\"tag1\":\"value1\",\"tag2\":\"value2\"},\"values\":[123],\"timestamps\":[1560277406000]}\n```\n\n## Querying Graphite data\n\nData sent to VictoriaMetrics via `Graphite plaintext protocol` may be read via the following APIs:\n\n* [Graphite API](#graphite-api-usage)\n* [Prometheus querying API](#prometheus-querying-api-usage). Graphite metric names may special chars such as `-`, which may clash\n with [MetricsQL operations](https://victoriametrics.github.io/MetricsQL.html). Such metrics can be queries via `{__name__=\"foo-bar.baz\"}`.\n VictoriaMetrics supports `__graphite__` pseudo-label for selecting time series with Graphite-compatible filters in [MetricsQL](https://victoriametrics.github.io/MetricsQL.html).\n For example, `{__graphite__=\"foo.*.bar\"}` is equivalent to `{__name__=~\"foo[.][^.]*[.]bar\"}`, but it works faster\n and it is easier to use when migrating from Graphite to VictoriaMetrics.\n* [go-graphite/carbonapi](https://github.com/go-graphite/carbonapi/blob/main/cmd/carbonapi/carbonapi.example.victoriametrics.yaml)\n\n## How to send data from OpenTSDB-compatible agents\n\nVictoriaMetrics supports [telnet put protocol](http://opentsdb.net/docs/build/html/api_telnet/put.html)\nand [HTTP /api/put requests](http://opentsdb.net/docs/build/html/api_http/put.html) for ingesting OpenTSDB data.\nThe same protocol is used for [ingesting data in KairosDB](https://kairosdb.github.io/docs/build/html/PushingData.html).\n\n### Sending data via `telnet put` protocol\n\nEnable OpenTSDB receiver in VictoriaMetrics by setting `-opentsdbListenAddr` command line flag. For instance,\nthe following command enables OpenTSDB receiver in VictoriaMetrics on TCP and UDP port `4242`:\n\n```bash\n/path/to/victoria-metrics-prod -opentsdbListenAddr=:4242\n```\n\nSend data to the given address from OpenTSDB-compatible agents.\n\nExample for writing data with OpenTSDB protocol to local VictoriaMetrics using `nc`:\n\n```bash\necho \"put foo.bar.baz `date +%s` 123 tag1=value1 tag2=value2\" | nc -N localhost 4242\n```\n\nAn arbitrary number of lines delimited by `\\n` (aka newline char) can be sent in one go.\nAfter that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:\n\n```bash\ncurl -G 'http://localhost:8428/api/v1/export' -d 'match=foo.bar.baz'\n```\n\nThe `/api/v1/export` endpoint should return the following response:\n\n```bash\n{\"metric\":{\"__name__\":\"foo.bar.baz\",\"tag1\":\"value1\",\"tag2\":\"value2\"},\"values\":[123],\"timestamps\":[1560277292000]}\n```\n\n### Sending OpenTSDB data via HTTP `/api/put` requests\n\nEnable HTTP server for OpenTSDB `/api/put` requests by setting `-opentsdbHTTPListenAddr` command line flag. For instance,\nthe following command enables OpenTSDB HTTP server on port `4242`:\n\n```bash\n/path/to/victoria-metrics-prod -opentsdbHTTPListenAddr=:4242\n```\n\nSend data to the given address from OpenTSDB-compatible agents.\n\nExample for writing a single data point:\n\n```bash\ncurl -H 'Content-Type: application/json' -d '{\"metric\":\"x.y.z\",\"value\":45.34,\"tags\":{\"t1\":\"v1\",\"t2\":\"v2\"}}' http://localhost:4242/api/put\n```\n\nExample for writing multiple data points in a single request:\n\n```bash\ncurl -H 'Content-Type: application/json' -d '[{\"metric\":\"foo\",\"value\":45.34},{\"metric\":\"bar\",\"value\":43}]' http://localhost:4242/api/put\n```\n\nAfter that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:\n\n```bash\ncurl -G 'http://localhost:8428/api/v1/export' -d 'match[]=x.y.z' -d 'match[]=foo' -d 'match[]=bar'\n```\n\nThe `/api/v1/export` endpoint should return the following response:\n\n```bash\n{\"metric\":{\"__name__\":\"foo\"},\"values\":[45.34],\"timestamps\":[1566464846000]}\n{\"metric\":{\"__name__\":\"bar\"},\"values\":[43],\"timestamps\":[1566464846000]}\n{\"metric\":{\"__name__\":\"x.y.z\",\"t1\":\"v1\",\"t2\":\"v2\"},\"values\":[45.34],\"timestamps\":[1566464763000]}\n```\n\nExtra labels may be added to all the imported time series by passing `extra_label=name=value` query args.\nFor example, `/api/put?extra_label=foo=bar` would add `{foo=\"bar\"}` label to all the ingested metrics.\n\n\n## Prometheus querying API usage\n\nVictoriaMetrics supports the following handlers from [Prometheus querying API](https://prometheus.io/docs/prometheus/latest/querying/api/):\n\n* [/api/v1/query](https://prometheus.io/docs/prometheus/latest/querying/api/#instant-queries)\n* [/api/v1/query_range](https://prometheus.io/docs/prometheus/latest/querying/api/#range-queries)\n* [/api/v1/series](https://prometheus.io/docs/prometheus/latest/querying/api/#finding-series-by-label-matchers)\n* [/api/v1/labels](https://prometheus.io/docs/prometheus/latest/querying/api/#getting-label-names)\n* [/api/v1/label/.../values](https://prometheus.io/docs/prometheus/latest/querying/api/#querying-label-values)\n* [/api/v1/status/tsdb](https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-stats). VictoriaMetrics accepts optional `topN=N` and `date=YYYY-MM-DD`\n query args for this handler, where `N` is the number of top entries to return in the response and `YYYY-MM-DD` is the date for collecting the stats.\n By default top 10 entries are returned and the stats is collected for the current day.\n* [/api/v1/targets](https://prometheus.io/docs/prometheus/latest/querying/api/#targets) - see [these docs](#how-to-scrape-prometheus-exporters-such-as-node-exporter) for more details.\n\nThese handlers can be queried from Prometheus-compatible clients such as Grafana or curl.\nAll the Prometheus querying API handlers can be prepended with `/prometheus` prefix. For example, both `/prometheus/api/v1/query` and `/api/v1/query` should work.\n\n\n### Prometheus querying API enhancements\n\nVictoriaMetrics accepts optional `extra_label=<label_name>=<label_value>` query arg, which can be used for enforcing additional label filters for queries. For example,\n`/api/v1/query_range?extra_label=user_id=123&query=<query>` would automatically add `{user_id=\"123\"}` label filter to the given `<query>`. This functionality can be used\nfor limiting the scope of time series visible to the given tenant. It is expected that the `extra_label` query arg is automatically set by auth proxy sitting\nin front of VictoriaMetrics. [Contact us](mailto:sales@victoriametrics.com) if you need assistance with such a proxy.\n\nVictoriaMetrics accepts relative times in `time`, `start` and `end` query args additionally to unix timestamps and [RFC3339](https://www.ietf.org/rfc/rfc3339.txt).\nFor example, the following query would return data for the last 30 minutes: `/api/v1/query_range?start=-30m&query=...`.\n\nBy default, VictoriaMetrics returns time series for the last 5 minutes from `/api/v1/series`, while the Prometheus API defaults to all time. Use `start` and `end` to select a different time range.\n\nVictoriaMetrics accepts additional args for `/api/v1/labels` and `/api/v1/label/.../values` handlers.\nSee [this feature request](https://github.com/prometheus/prometheus/issues/6178) for details:\n\n* Any number [time series selectors](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors) via `match[]` query arg.\n* Optional `start` and `end` query args for limiting the time range for the selected labels or label values.\n\nAdditionally VictoriaMetrics provides the following handlers:\n\n* `/api/v1/series/count` - returns the total number of time series in the database. Some notes:\n * the handler scans all the inverted index, so it can be slow if the database contains tens of millions of time series;\n * the handler may count [deleted time series](#how-to-delete-time-series) additionally to normal time series due to internal implementation restrictions;\n* `/api/v1/labels/count` - returns a list of `label: values_count` entries. It can be used for determining labels with the maximum number of values.\n* `/api/v1/status/active_queries` - returns a list of currently running queries.\n* `/api/v1/status/top_queries` - returns the following query lists:\n * the most frequently executed queries - `topByCount`\n * queries with the biggest average execution duration - `topByAvgDuration`\n * queries that took the most time for execution - `topBySumDuration`\n\n The number of returned queries can be limited via `topN` query arg. Old queries can be filtered out with `maxLifetime` query arg.\n For example, request to `/api/v1/status/top_queries?topN=5&maxLifetime=30s` would return up to 5 queries per list, which were executed during the last 30 seconds.\n VictoriaMetrics tracks the last `-search.queryStats.lastQueriesCount` queries with durations at least `-search.queryStats.minQueryDuration`.\n\n\n## Graphite API usage\n\nVictoriaMetrics supports the following Graphite APIs, which are needed for [Graphite datasource in Grafana](https://grafana.com/docs/grafana/latest/datasources/graphite/):\n\n* Render API - see [these docs](#graphite-render-api-usage).\n* Metrics API - see [these docs](#graphite-metrics-api-usage).\n* Tags API - see [these docs](#graphite-tags-api-usage).\n\nAll the Graphite handlers can be pre-pended with `/graphite` prefix. For example, both `/graphite/metrics/find` and `/metrics/find` should work.\n\nVictoriaMetrics supports `__graphite__` pseudo-label for filtering time series with Graphite-compatible filters in [MetricsQL](https://victoriametrics.github.io/MetricsQL.html).\nFor example, `{__graphite__=\"foo.*.bar\"}` is equivalent to `{__name__=~\"foo[.][^.]*[.]bar\"}`, but it works faster\nand it is easier to use when migrating from Graphite to VictoriaMetrics.\n\n\n### Graphite Render API usage\n\n[VictoriaMetrics Enterprise](https://victoriametrics.com/enterprise.html) supports [Graphite Render API](https://graphite.readthedocs.io/en/stable/render_api.html) subset\nat `/render` endpoint, which is used by [Graphite datasource in Grafana](https://grafana.com/docs/grafana/latest/datasources/graphite/).\nIt supports `Storage-Step` http request header, which must be set to a step between data points stored in VictoriaMetrics when configuring Graphite datasource in Grafana.\n\n\n### Graphite Metrics API usage\n\nVictoriaMetrics supports the following handlers from [Graphite Metrics API](https://graphite-api.readthedocs.io/en/latest/api.html#the-metrics-api):\n\n* [/metrics/find](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-find)\n* [/metrics/expand](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-expand)\n* [/metrics/index.json](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-index-json)\n\nVictoriaMetrics accepts the following additional query args at `/metrics/find` and `/metrics/expand`:\n * `label` - for selecting arbitrary label values. By default `label=__name__`, i.e. metric names are selected.\n * `delimiter` - for using different delimiters in metric name hierachy. For example, `/metrics/find?delimiter=_&query=node_*` would return all the metric name prefixes\n that start with `node_`. By default `delimiter=.`.\n\n\n### Graphite Tags API usage\n\nVictoriaMetrics supports the following handlers from [Graphite Tags API](https://graphite.readthedocs.io/en/stable/tags.html):\n\n* [/tags/tagSeries](https://graphite.readthedocs.io/en/stable/tags.html#adding-series-to-the-tagdb)\n* [/tags/tagMultiSeries](https://graphite.readthedocs.io/en/stable/tags.html#adding-series-to-the-tagdb)\n* [/tags](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags)\n* [/tags/{tag_name}](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags)\n* [/tags/findSeries](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags)\n* [/tags/autoComplete/tags](https://graphite.readthedocs.io/en/stable/tags.html#auto-complete-support)\n* [/tags/autoComplete/values](https://graphite.readthedocs.io/en/stable/tags.html#auto-complete-support)\n* [/tags/delSeries](https://graphite.readthedocs.io/en/stable/tags.html#removing-series-from-the-tagdb)\n\n\n## How to build from sources\n\nWe recommend using either [binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) or\n[docker images](https://hub.docker.com/r/victoriametrics/victoria-metrics/) instead of building VictoriaMetrics\nfrom sources. Building from sources is reasonable when developing additional features specific\nto your needs or when testing bugfixes.\n\n### Development build\n\n1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.13.\n2. Run `make victoria-metrics` from the root folder of the repository.\n It builds `victoria-metrics` binary and puts it into the `bin` folder.\n\n### Production build\n\n1. [Install docker](https://docs.docker.com/install/).\n2. Run `make victoria-metrics-prod` from the root folder of the repository.\n It builds `victoria-metrics-prod` binary and puts it into the `bin` folder.\n\n### ARM build\n\nARM build may run on Raspberry Pi or on [energy-efficient ARM servers](https://blog.cloudflare.com/arm-takes-wing/).\n\n### Development ARM build\n\n1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.13.\n2. Run `make victoria-metrics-arm` or `make victoria-metrics-arm64` from the root folder of the repository.\n It builds `victoria-metrics-arm` or `victoria-metrics-arm64` binary respectively and puts it into the `bin` folder.\n\n### Production ARM build\n\n1. [Install docker](https://docs.docker.com/install/).\n2. Run `make victoria-metrics-arm-prod` or `make victoria-metrics-arm64-prod` from the root folder of the repository.\n It builds `victoria-metrics-arm-prod` or `victoria-metrics-arm64-prod` binary respectively and puts it into the `bin` folder.\n\n### Pure Go build (CGO_ENABLED=0)\n\n`Pure Go` mode builds only Go code without [cgo](https://golang.org/cmd/cgo/) dependencies.\nThis is an experimental mode, which may result in a lower compression ratio and slower decompression performance.\nUse it with caution!\n\n1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.13.\n2. Run `make victoria-metrics-pure` from the root folder of the repository.\n It builds `victoria-metrics-pure` binary and puts it into the `bin` folder.\n\n### Building docker images\n\nRun `make package-victoria-metrics`. It builds `victoriametrics/victoria-metrics:<PKG_TAG>` docker image locally.\n`<PKG_TAG>` is auto-generated image tag, which depends on source code in the repository.\nThe `<PKG_TAG>` may be manually set via `PKG_TAG=foobar make package-victoria-metrics`.\n\nThe base docker image is [alpine](https://hub.docker.com/_/alpine) but it is possible to use any other base image\nby setting it via `<ROOT_IMAGE>` environment variable.\nFor example, the following command builds the image on top of [scratch](https://hub.docker.com/_/scratch) image:\n\n```bash\nROOT_IMAGE=scratch make package-victoria-metrics\n```\n\n## Start with docker-compose\n\n[Docker-compose](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/deployment/docker/docker-compose.yml)\nhelps to spin up VictoriaMetrics, [vmagent](https://victoriametrics.github.io/vmagent.html) and Grafana with one command.\nMore details may be found [here](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/master/deployment/docker#folder-contains-basic-images-and-tools-for-building-and-running-victoria-metrics-in-docker).\n\n\n## Setting up service\n\nRead [these instructions](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/43) on how to set up VictoriaMetrics as a service in your OS.\nThere is also [snap package for Ubuntu](https://snapcraft.io/victoriametrics).\n\n\n## How to work with snapshots\n\nVictoriaMetrics can create [instant snapshots](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)\nfor all the data stored under `-storageDataPath` directory.\nNavigate to `http://<victoriametrics-addr>:8428/snapshot/create` in order to create an instant snapshot.\nThe page will return the following JSON response:\n\n```json\n{\"status\":\"ok\",\"snapshot\":\"<snapshot-name>\"}\n```\n\nSnapshots are created under `<-storageDataPath>/snapshots` directory, where `<-storageDataPath>`\nis the command-line flag value. Snapshots can be archived to backup storage at any time\nwith [vmbackup](https://victoriametrics.github.io/vmbackup.html).\n\nThe `http://<victoriametrics-addr>:8428/snapshot/list` page contains the list of available snapshots.\n\nNavigate to `http://<victoriametrics-addr>:8428/snapshot/delete?snapshot=<snapshot-name>` in order\nto delete `<snapshot-name>` snapshot.\n\nNavigate to `http://<victoriametrics-addr>:8428/snapshot/delete_all` in order to delete all the snapshots.\n\nSteps for restoring from a snapshot:\n\n1. Stop VictoriaMetrics with `kill -INT`.\n2. Restore snapshot contents from backup with [vmrestore](https://victoriametrics.github.io/vmrestore.html)\n to the directory pointed by `-storageDataPath`.\n3. Start VictoriaMetrics.\n\n## How to delete time series\n\nSend a request to `http://<victoriametrics-addr>:8428/api/v1/admin/tsdb/delete_series?match[]=<timeseries_selector_for_delete>`,\nwhere `<timeseries_selector_for_delete>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)\nfor metrics to delete. After that all the time series matching the given selector are deleted. Storage space for\nthe deleted time series isn't freed instantly - it is freed during subsequent [background merges of data files](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).\nNote that background merges may never occur for data from previous months, so storage space won't be freed for historical data.\nIn this case [forced merge](#forced-merge) may help freeing up storage space.\n\nIt is recommended verifying which metrics will be deleted with the call to `http://<victoria-metrics-addr>:8428/api/v1/series?match[]=<timeseries_selector_for_delete>`\nbefore actually deleting the metrics. By default this query will only scan active series in the past 5 minutes, so you may need to\nadjust `start` and `end` to a suitable range to achieve match hits.\n\nThe `/api/v1/admin/tsdb/delete_series` handler may be protected with `authKey` if `-deleteAuthKey` command-line flag is set.\n\nThe delete API is intended mainly for the following cases:\n\n* One-off deleting of accidentally written invalid (or undesired) time series.\n* One-off deleting of user data due to [GDPR](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation).\n\nIt isn't recommended using delete API for the following cases, since it brings non-zero overhead:\n\n* Regular cleanups for unneeded data. Just prevent writing unneeded data into VictoriaMetrics.\n This can be done with [relabeling](#relabeling).\n See [this article](https://www.robustperception.io/relabelling-can-discard-targets-timeseries-and-alerts) for details.\n* Reducing disk space usage by deleting unneeded time series. This doesn't work as expected, since the deleted\n time series occupy disk space until the next merge operation, which can never occur when deleting too old data.\n [Forced merge](#forced-merge) may be used for freeing up disk space occupied by old data.\n\nIt is better using `-retentionPeriod` command-line flag for efficient pruning of old data.\n\n\n## Forced merge\n\nVictoriaMetrics performs [data compactions in background](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)\nin order to keep good performance characteristics when accepting new data. These compactions (merges) are performed independently on per-month partitions.\nThis means that compactions are stopped for per-month partitions if no new data is ingested into these partitions.\nSometimes it is necessary to trigger compactions for old partitions. For instance, in order to free up disk space occupied by [deleted time series](#how-to-delete-time-series).\nIn this case forced compaction may be initiated on the specified per-month partition by sending request to `/internal/force_merge?partition_prefix=YYYY_MM`,\nwhere `YYYY_MM` is per-month partition name. For example, `http://victoriametrics:8428/internal/force_merge?partition_prefix=2020_08` would initiate forced\nmerge for August 2020 partition. The call to `/internal/force_merge` returns immediately, while the corresponding forced merge continues running in background.\n\nForced merges may require additional CPU, disk IO and storage space resources. It is unnecessary to run forced merge under normal conditions,\nsince VictoriaMetrics automatically performs [optimal merges in background](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)\nwhen new data is ingested into it.\n\n\n## How to export time series\n\nVictoriaMetrics provides the following handlers for exporting data:\n\n* `/api/v1/export/native` for exporting data in native binary format. This is the most efficient format for data export.\n See [these docs](#how-to-export-data-in-native-format) for details.\n* `/api/v1/export` for exporing data in JSON line format. See [these docs](#how-to-export-data-in-json-line-format) for details.\n* `/api/v1/export/csv` for exporting data in CSV. See [these docs](#how-to-export-csv-data) for details.\n\n\n### How to export data in native format\n\nSend a request to `http://<victoriametrics-addr>:8428/api/v1/export/native?match[]=<timeseries_selector_for_export>`,\nwhere `<timeseries_selector_for_export>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)\nfor metrics to export. Use `{__name__=~\".*\"}` selector for fetching all the time series.\n\nOn large databases you may experience problems with limit on unique timeseries (default value is 300000). In this case you need to adjust `-search.maxUniqueTimeseries` parameter:\n\n```bash\n# count unique timeseries in database\nwget -O- -q 'http://your_victoriametrics_instance:8428/api/v1/series/count' | jq '.data[0]'\n\n# relaunch victoriametrics with search.maxUniqueTimeseries more than value from previous command\n```\n\nOptional `start` and `end` args may be added to the request in order to limit the time frame for the exported data. These args may contain either\nunix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values.\n\nThe exported data can be imported to VictoriaMetrics via [/api/v1/import/native](#how-to-import-data-in-native-format).\n\n\n### How to export data in JSON line format\n\nSend a request to `http://<victoriametrics-addr>:8428/api/v1/export?match[]=<timeseries_selector_for_export>`,\nwhere `<timeseries_selector_for_export>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)\nfor metrics to export. Use `{__name__!=\"\"}` selector for fetching all the time series.\nThe response would contain all the data for the selected time series in [JSON streaming format](https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON).\nEach JSON line contains samples for a single time series. An example output:\n\n```jsonl\n{\"metric\":{\"__name__\":\"up\",\"job\":\"node_exporter\",\"instance\":\"localhost:9100\"},\"values\":[0,0,0],\"timestamps\":[1549891472010,1549891487724,1549891503438]}\n{\"metric\":{\"__name__\":\"up\",\"job\":\"prometheus\",\"instance\":\"localhost:9090\"},\"values\":[1,1,1],\"timestamps\":[1549891461511,1549891476511,1549891491511]}\n```\n\nOptional `start` and `end` args may be added to the request in order to limit the time frame for the exported data. These args may contain either\nunix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values.\n\nOptional `max_rows_per_line` arg may be added to the request for limiting the maximum number of rows exported per each JSON line.\nOptional `reduce_mem_usage=1` arg may be added to the request for reducing memory usage when exporting big number of time series.\nIn this case the output may contain multiple lines with distinct samples for the same time series.\n\nPass `Accept-Encoding: gzip` HTTP header in the request to `/api/v1/export` in order to reduce network bandwidth during exporing big amounts\nof time series data. This enables gzip compression for the exported data. Example for exporting gzipped data:\n\n```bash\ncurl -H 'Accept-Encoding: gzip' http://localhost:8428/api/v1/export -d 'match[]={__name__!=\"\"}' > data.jsonl.gz\n```\n\nThe maximum duration for each request to `/api/v1/export` is limited by `-search.maxExportDuration` command-line flag.\n\nExported data can be imported via POST'ing it to [/api/v1/import](#how-to-import-data-in-json-line-format).\n\n\n### How to export CSV data\n\nSend a request to `http://<victoriametrics-addr>:8428/api/v1/export/csv?format=<format>&match=<timeseries_selector_for_export>`,\nwhere:\n\n* `<format>` must contain comma-delimited label names for the exported CSV. The following special label names are supported:\n * `__name__` - metric name\n * `__value__` - sample value\n * `__timestamp__:<ts_format>` - sample timestamp. `<ts_format>` can have the following values:\n * `unix_s` - unix seconds\n * `unix_ms` - unix milliseconds\n * `unix_ns` - unix nanoseconds\n * `rfc3339` - [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) time\n * `custom:<layout>` - custom layout for time that is supported by [time.Format](https://golang.org/pkg/time/#Time.Format) function from Go.\n\n* `<timeseries_selector_for_export>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)\nfor metrics to export.\n\nOptional `start` and `end` args may be added to the request in order to limit the time frame for the exported data. These args may contain either\nunix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values.\n\nThe exported CSV data can be imported to VictoriaMetrics via [/api/v1/import/csv](#how-to-import-csv-data).\n\n\n## How to import time series data\n\nTime series data can be imported via any supported ingestion protocol:\n\n* [Prometheus remote_write API](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write). See [these docs](#prometheus-setup) for details.\n* Influx line protocol. See [these docs](#how-to-send-data-from-influxdb-compatible-agents-such-as-telegraf) for details.\n* Graphite plaintext protocol. See [these docs](#how-to-send-data-from-graphite-compatible-agents-such-as-statsd) for details.\n* OpenTSDB telnet put protocol. See [these docs](#sending-data-via-telnet-put-protocol) for details.\n* OpenTSDB http `/api/put` protocol. See [these docs](#sending-opentsdb-data-via-http-apiput-requests) for details.\n* `/api/v1/import` for importing data obtained from [/api/v1/export](#how-to-export-data-in-json-line-format).\n See [these docs](#how-to-import-data-in-json-line-format) for details.\n* `/api/v1/import/native` for importing data obtained from [/api/v1/export/native](#how-to-export-data-in-native-format).\n See [these docs](#how-to-import-data-in-native-format) for details.\n* `/api/v1/import/csv` for importing arbitrary CSV data. See [these docs](#how-to-import-csv-data) for details.\n* `/api/v1/import/prometheus` for importing data in Prometheus exposition format. See [these docs](#how-to-import-data-in-prometheus-exposition-format) for details.\n\n\n### How to import data in native format\n\nThe specification of VictoriaMetrics' native format may yet change and is not formally documented yet. So currently we do not recommend that external clients attempt to pack their own metrics in native format file.\n\nIf you have a native format file obtained via [/api/v1/export/native](#how-to-export-data-in-native-format) however this is the most efficient protocol for importing data in.\n\n```bash\n# Export the data from <source-victoriametrics>:\ncurl http://source-victoriametrics:8428/api/v1/export/native -d 'match={__name__!=\"\"}' > exported_data.bin\n\n# Import the data to <destination-victoriametrics>:\ncurl -X POST http://destination-victoriametrics:8428/api/v1/import/native -T exported_data.bin\n```\n\nPass `Content-Encoding: gzip` HTTP request header to `/api/v1/import/native` for importing gzipped data:\n\n```bash\n# Export gzipped data from <source-victoriametrics>:\ncurl -H 'Accept-Encoding: gzip' http://source-victoriametrics:8428/api/v1/export/native -d 'match={__name__!=\"\"}' > exported_data.bin.gz\n\n# Import gzipped data to <destination-victoriametrics>:\ncurl -X POST -H 'Content-Encoding: gzip' http://destination-victoriametrics:8428/api/v1/import/native -T exported_data.bin.gz\n```\n\nExtra labels may be added to all the imported time series by passing `extra_label=name=value` query args.\nFor example, `/api/v1/import/native?extra_label=foo=bar` would add `\"foo\":\"bar\"` label to all the imported time series.\n\nNote that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.\n\n\n### How to import data in JSON line format\n\nExample for importing data obtained via [/api/v1/export](#how-to-export-data-in-json-line-format):\n\n```bash\n# Export the data from <source-victoriametrics>:\ncurl http://source-victoriametrics:8428/api/v1/export -d 'match={__name__!=\"\"}' > exported_data.jsonl\n\n# Import the data to <destination-victoriametrics>:\ncurl -X POST http://destination-victoriametrics:8428/api/v1/import -T exported_data.jsonl\n```\n\nPass `Content-Encoding: gzip` HTTP request header to `/api/v1/import` for importing gzipped data:\n\n```bash\n# Export gzipped data from <source-victoriametrics>:\ncurl -H 'Accept-Encoding: gzip' http://source-victoriametrics:8428/api/v1/export -d 'match={__name__!=\"\"}' > exported_data.jsonl.gz\n\n# Import gzipped data to <destination-victoriametrics>:\ncurl -X POST -H 'Content-Encoding: gzip' http://destination-victoriametrics:8428/api/v1/import -T exported_data.jsonl.gz\n```\n\nExtra labels may be added to all the imported time series by passing `extra_label=name=value` query args.\nFor example, `/api/v1/import?extra_label=foo=bar` would add `\"foo\":\"bar\"` label to all the imported time series.\n\nNote that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.\n\n\n### How to import CSV data\n\nArbitrary CSV data can be imported via `/api/v1/import/csv`. The CSV data is imported according to the provided `format` query arg.\nThe `format` query arg must contain comma-separated list of parsing rules for CSV fields. Each rule consists of three parts delimited by a colon:\n\n```\n<column_pos>:<type>:<context>\n```\n\n* `<column_pos>` is the position of the CSV column (field). Column numbering starts from 1. The order of parsing rules may be arbitrary.\n* `<type>` describes the column type. Supported types are:\n * `metric` - the corresponding CSV column at `<column_pos>` contains metric value, which must be integer or floating-point number.\n The metric name is read from the `<context>`. CSV line must have at least a single metric field. Multiple metric fields per CSV line is OK.\n * `label` - the corresponding CSV column at `<column_pos>` contains label value. The label name is read from the `<context>`.\n CSV line may have arbitrary number of label fields. All these labels are attached to all the configured metrics.\n * `time` - the corresponding CSV column at `<column_pos>` contains metric time. CSV line may contain either one or zero columns with time.\n If CSV line has no time, then the current time is used. The time is applied to all the configured metrics.\n The format of the time is configured via `<context>`. Supported time formats are:\n * `unix_s` - unix timestamp in seconds.\n * `unix_ms` - unix timestamp in milliseconds.\n * `unix_ns` - unix timestamp in nanoseconds. Note that VictoriaMetrics rounds the timestamp to milliseconds.\n * `rfc3339` - timestamp in [RFC3339](https://tools.ietf.org/html/rfc3339) format, i.e. `2006-01-02T15:04:05Z`.\n * `custom:<layout>` - custom layout for the timestamp. The `<layout>` may contain arbitrary time layout according to [time.Parse rules in Go](https://golang.org/pkg/time/#Parse).\n\nEach request to `/api/v1/import/csv` may contain arbitrary number of CSV lines.\n\nExample for importing CSV data via `/api/v1/import/csv`:\n\n```bash\ncurl -d \"GOOG,1.23,4.56,NYSE\" 'http://localhost:8428/api/v1/import/csv?format=2:metric:ask,3:metric:bid,1:label:ticker,4:label:market'\ncurl -d \"MSFT,3.21,1.67,NASDAQ\" 'http://localhost:8428/api/v1/import/csv?format=2:metric:ask,3:metric:bid,1:label:ticker,4:label:market'\n```\n\nAfter that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:\n\n```bash\ncurl -G 'http://localhost:8428/api/v1/export' -d 'match[]={ticker!=\"\"}'\n```\n\nThe following response should be returned:\n```bash\n{\"metric\":{\"__name__\":\"bid\",\"market\":\"NASDAQ\",\"ticker\":\"MSFT\"},\"values\":[1.67],\"timestamps\":[1583865146520]}\n{\"metric\":{\"__name__\":\"bid\",\"market\":\"NYSE\",\"ticker\":\"GOOG\"},\"values\":[4.56],\"timestamps\":[1583865146495]}\n{\"metric\":{\"__name__\":\"ask\",\"market\":\"NASDAQ\",\"ticker\":\"MSFT\"},\"values\":[3.21],\"timestamps\":[1583865146520]}\n{\"metric\":{\"__name__\":\"ask\",\"market\":\"NYSE\",\"ticker\":\"GOOG\"},\"values\":[1.23],\"timestamps\":[1583865146495]}\n```\n\nExtra labels may be added to all the imported lines by passing `extra_label=name=value` query args.\nFor example, `/api/v1/import/csv?extra_label=foo=bar` would add `\"foo\":\"bar\"` label to all the imported lines.\n\nNote that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.\n\n\n### How to import data in Prometheus exposition format\n\nVictoriaMetrics accepts data in [Prometheus exposition format](https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format)\nand in [OpenMetrics format](https://github.com/OpenObservability/OpenMetrics/blob/master/specification/OpenMetrics.md)\nvia `/api/v1/import/prometheus` path. For example, the following line imports a single line in Prometheus exposition format into VictoriaMetrics:\n\n```bash\ncurl -d 'foo{bar=\"baz\"} 123' -X POST 'http://localhost:8428/api/v1/import/prometheus'\n```\n\nThe following command may be used for verifying the imported data:\n\n```bash\ncurl -G 'http://localhost:8428/api/v1/export' -d 'match={__name__=~\"foo\"}'\n```\n\nIt should return something like the following:\n\n```\n{\"metric\":{\"__name__\":\"foo\",\"bar\":\"baz\"},\"values\":[123],\"timestamps\":[1594370496905]}\n```\n\nExtra labels may be added to all the imported metrics by passing `extra_label=name=value` query args.\nFor example, `/api/v1/import/prometheus?extra_label=foo=bar` would add `{foo=\"bar\"}` label to all the imported metrics.\n\nIf timestamp is missing in `<metric> <value> <timestamp>` Prometheus exposition format line, then the current timestamp is used during data ingestion.\nIt can be overriden by passing unix timestamp in *milliseconds* via `timestamp` query arg. For example, `/api/v1/import/prometheus?timestamp=1594370496905`.\n\nVictoriaMetrics accepts arbitrary number of lines in a single request to `/api/v1/import/prometheus`, i.e. it supports data streaming.\n\nNote that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.\n\nVictoriaMetrics also may scrape Prometheus targets - see [these docs](#how-to-scrape-prometheus-exporters-such-as-node-exporter).\n\n\n\n## Relabeling\n\nVictoriaMetrics supports Prometheus-compatible relabeling for all the ingested metrics if `-relabelConfig` command-line flag points\nto a file containing a list of [relabel_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config) entries.\nSee [this article with relabeling tips and tricks](https://valyala.medium.com/how-to-use-relabeling-in-prometheus-and-victoriametrics-8b90fc22c4b2).\n\nExample contents for `-relabelConfig` file:\n```yml\n# Add {cluster=\"dev\"} label.\n- target_label: cluster\n replacement: dev\n\n# Drop the metric (or scrape target) with `{__meta_kubernetes_pod_container_init=\"true\"}` label.\n- action: drop\n source_labels: [__meta_kubernetes_pod_container_init]\n regex: true\n```\n\nVictoriaMetrics provides the following extra actions for relabeling rules:\n\n* `replace_all`: replaces all the occurences of `regex` in the values of `source_labels` with the `replacement` and stores the result in the `target_label`.\n* `labelmap_all`: replaces all the occurences of `regex` in all the label names with the `replacement`.\n* `keep_if_equal`: keeps the entry if all label values from `source_labels` are equal.\n* `drop_if_equal`: drops the entry if all the label values from `source_labels` are equal.\n\nSee also [relabeling in vmagent](https://victoriametrics.github.io/vmagent.html#relabeling).\n\n\n## Federation\n\nVictoriaMetrics exports [Prometheus-compatible federation data](https://prometheus.io/docs/prometheus/latest/federation/)\nat `http://<victoriametrics-addr>:8428/federate?match[]=<timeseries_selector_for_federation>`.\n\nOptional `start` and `end` args may be added to the request in order to scrape the last point for each selected time series on the `[start ... end]` interval.\n`start` and `end` may contain either unix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values. By default, the last point\non the interval `[now - max_lookback ... now]` is scraped for each time series. The default value for `max_lookback` is `5m` (5 minutes), but it can be overridden.\nFor instance, `/federate?match[]=up&max_lookback=1h` would return last points on the `[now - 1h ... now]` interval. This may be useful for time series federation\nwith scrape intervals exceeding `5m`.\n\n## Capacity planning\n\nA rough estimation of the required resources for ingestion path:\n\n* RAM size: less than 1KB per active time series. So, ~1GB of RAM is required for 1M active time series.\n Time series is considered active if new data points have been added to it recently or if it has been recently queried.\n The number of active time series may be obtained from `vm_cache_entries{type=\"storage/hour_metric_ids\"}` metric\n exported on the `/metrics` page.\n VictoriaMetrics stores various caches in RAM. Memory size for these caches may be limited with `-memory.allowedPercent` or `-memory.allowedBytes` flags.\n\n* CPU cores: a CPU core per 300K inserted data points per second. So, ~4 CPU cores are required for processing\n the insert stream of 1M data points per second. The ingestion rate may be lower for high cardinality data or for time series with high number of labels.\n See [this article](https://medium.com/@valyala/insert-benchmarks-with-inch-influxdb-vs-victoriametrics-e31a41ae2893) for details.\n If you see lower numbers per CPU core, then it is likely active time series info doesn't fit caches,\n so you need more RAM for lowering CPU usage.\n\n* Storage space: less than a byte per data point on average. So, ~260GB is required for storing a month-long insert stream\n of 100K data points per second.\n The actual storage size heavily depends on data randomness (entropy). Higher randomness means higher storage size requirements.\n Read [this article](https://medium.com/faun/victoriametrics-achieving-better-compression-for-time-series-data-than-gorilla-317bc1f95932)\n for details.\n\n* Network usage: outbound traffic is negligible. Ingress traffic is ~100 bytes per ingested data point via\n [Prometheus remote_write API](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write).\n The actual ingress bandwidth usage depends on the average number of labels per ingested metric and the average size\n of label values. The higher number of per-metric labels and longer label values mean the higher ingress bandwidth.\n\nThe required resources for query path:\n\n* RAM size: depends on the number of time series to scan in each query and the `step`\n argument passed to [/api/v1/query_range](https://prometheus.io/docs/prometheus/latest/querying/api/#range-queries).\n The higher number of scanned time series and lower `step` argument results in the higher RAM usage.\n\n* CPU cores: a CPU core per 30 millions of scanned data points per second.\n This means that heavy queries that touch big number of time series (over 10K) and/or big number data points (over 100M)\n usually require more CPU resources than tiny queries that touch a few time series with small number of data points.\n\n* Network usage: depends on the frequency and the type of incoming requests. Typical Grafana dashboards usually\n require negligible network bandwidth.\n\n\n## High availability\n\n* Install multiple VictoriaMetrics instances in distinct datacenters (availability zones).\n* Pass addresses of these instances to [vmagent](https://victoriametrics.github.io/vmagent.html) via `-remoteWrite.url` command-line flag:\n\n```bash\n/path/to/vmagent -remoteWrite.url=http://<victoriametrics-addr-1>:8428/api/v1/write -remoteWrite.url=http://<victoriametrics-addr-2>:8428/api/v1/write\n```\n\nAlternatively these addresses may be passed to `remote_write` section in Prometheus config:\n\n```yml\nremote_write:\n - url: http://<victoriametrics-addr-1>:8428/api/v1/write\n queue_config:\n max_samples_per_send: 10000\n # ...\n - url: http://<victoriametrics-addr-N>:8428/api/v1/write\n queue_config:\n max_samples_per_send: 10000\n```\n\n* Apply the updated config:\n\n```bash\nkill -HUP `pidof prometheus`\n```\n\nIt is recommended to use [vmagent](https://victoriametrics.github.io/vmagent.html) instead of Prometheus for highly loaded setups.\n\n* Now Prometheus should write data into all the configured `remote_write` urls in parallel.\n* Set up [Promxy](https://github.com/jacksontj/promxy) in front of all the VictoriaMetrics replicas.\n* Set up Prometheus datasource in Grafana that points to Promxy.\n\nIf you have Prometheus HA pairs with replicas `r1` and `r2` in each pair, then configure each `r1`\nto write data to `victoriametrics-addr-1`, while each `r2` should write data to `victoriametrics-addr-2`.\n\nAnother option is to write data simultaneously from Prometheus HA pair to a pair of VictoriaMetrics instances\nwith the enabled de-duplication. See [this section](#deduplication) for details.\n\n\n## Deduplication\n\nVictoriaMetrics de-duplicates data points if `-dedup.minScrapeInterval` command-line flag\nis set to positive duration. For example, `-dedup.minScrapeInterval=60s` would de-duplicate data points\non the same time series if they fall within the same discrete 60s bucket. The earliest data point will be kept. In the case of equal timestamps, an arbitrary data point will be kept.\n\nThe recommended value for `-dedup.minScrapeInterval` must equal to `scrape_interval` config from Prometheus configs.\n\nThe de-duplication reduces disk space usage if multiple identically configured Prometheus instances in HA pair\nwrite data to the same VictoriaMetrics instance. Note that these Prometheus instances must have identical\n`external_labels` section in their configs, so they write data to the same time series.\n\n\n## Retention\n\nRetention is configured with `-retentionPeriod` command-line flag. For instance, `-retentionPeriod=3` means\nthat the data will be stored for 3 months and then deleted.\nData is split in per-month subdirectories inside `<-storageDataPath>/data/small` and `<-storageDataPath>/data/big` folders.\nDirectories for months outside the configured retention are deleted on the first day of new month.\nIn order to keep data according to `-retentionPeriod` max disk space usage is going to be `-retentionPeriod` + 1 month.\nFor example if `-retentionPeriod` is set to 1, data for January is deleted on March 1st.\nIt is safe to extend `-retentionPeriod` on existing data. If `-retentionPeriod` is set to lower\nvalue than before then data outside the configured period will be eventually deleted.\n\nVictoriaMetrics supports retention smaller than 1 month. For example, `-retentionPeriod=5d` would set data retention for 5 days.\nOlder data is eventually deleted during [background merge](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).\n\n\n## Multiple retentions\n\nJust start multiple VictoriaMetrics instances with distinct values for the following flags:\n\n* `-retentionPeriod`\n* `-storageDataPath`, so the data for each retention period is saved in a separate directory\n* `-httpListenAddr`, so clients may reach VictoriaMetrics instance with proper retention\n\nThen set up [vmauth](https://victoriametrics.github.io/vmauth.html) in front of VictoriaMetrics instances,\nso it could route requests from particular user to VictoriaMetrics with the desired retention.\nThe same scheme could be implemented for multiple tenants in [VictoriaMetrics cluster](https://victoriametrics.github.io/Cluster-VictoriaMetrics.html).\n\n\n## Downsampling\n\nThere is no downsampling support at the moment, but:\n\n* VictoriaMetrics is optimized for querying big amounts of raw data. See benchmark results for heavy queries\n in [this article](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae).\n* VictoriaMetrics has good compression for on-disk data. See [this article](https://medium.com/@valyala/victoriametrics-achieving-better-compression-for-time-series-data-than-gorilla-317bc1f95932)\n for details.\n\nThese properties reduce the need of downsampling. We plan to implement downsampling in the future.\nSee [this issue](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/36) for details.\n\nIt is possible to (ab)use [-dedup.minScrapeInterval](#deduplication) for basic downsampling.\nFor instance, if interval between the ingested data points is 15s, then `-dedup.minScrapeInterval=5m` will leave\nonly a single data point out of 20 initial data points per each 5m interval.\n\n\n## Multi-tenancy\n\nSingle-node VictoriaMetrics doesn't support multi-tenancy. Use [cluster version](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster) instead.\n\n\n## Scalability and cluster version\n\nThough single-node VictoriaMetrics cannot scale to multiple nodes, it is optimized for resource usage - storage size / bandwidth / IOPS, RAM, CPU.\nThis means that a single-node VictoriaMetrics may scale vertically and substitute a moderately sized cluster built with competing solutions\nsuch as Thanos, Uber M3, InfluxDB or TimescaleDB. See [vertical scalability benchmarks](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae).\n\nSo try single-node VictoriaMetrics at first and then [switch to cluster version](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster) if you still need\nhorizontally scalable long-term remote storage for really large Prometheus deployments.\n[Contact us](mailto:info@victoriametrics.com) for paid support.\n\n\n## Alerting\n\nIt is recommended using [vmalert](https://victoriametrics.github.io/vmalert.html) for alerting.\n\nAdditionally, alerting can be set up with the following tools:\n\n* With Prometheus - see [the corresponding docs](https://prometheus.io/docs/alerting/overview/).\n* With Promxy - see [the corresponding docs](https://github.com/jacksontj/promxy/blob/master/README.md#how-do-i-use-alertingrecording-rules-in-promxy).\n* With Grafana - see [the corresponding docs](https://grafana.com/docs/alerting/rules/).\n\n\n## Security\n\nDo not forget protecting sensitive endpoints in VictoriaMetrics when exposing it to untrusted networks such as the internet.\nConsider setting the following command-line flags:\n\n* `-tls`, `-tlsCertFile` and `-tlsKeyFile` for switching from HTTP to HTTPS.\n* `-httpAuth.username` and `-httpAuth.password` for protecting all the HTTP endpoints\n with [HTTP Basic Authentication](https://en.wikipedia.org/wiki/Basic_access_authentication).\n* `-deleteAuthKey` for protecting `/api/v1/admin/tsdb/delete_series` endpoint. See [how to delete time series](#how-to-delete-time-series).\n* `-snapshotAuthKey` for protecting `/snapshot*` endpoints. See [how to work with snapshots](#how-to-work-with-snapshots).\n* `-forceMergeAuthKey` for protecting `/internal/force_merge` endpoint. See [force merge docs](#forced-merge).\n* `-search.resetCacheAuthKey` for protecting `/internal/resetRollupResultCache` endpoint. See [backfilling](#backfilling) for more details.\n\nExplicitly set internal network interface for TCP and UDP ports for data ingestion with Graphite and OpenTSDB formats.\nFor example, substitute `-graphiteListenAddr=:2003` with `-graphiteListenAddr=<internal_iface_ip>:2003`.\n\nPrefer authorizing all the incoming requests from untrusted networks with [vmauth](https://victoriametrics.github.io/vmauth.html)\nor similar auth proxy.\n\n\n## Tuning\n\n* There is no need for VictoriaMetrics tuning since it uses reasonable defaults for command-line flags,\n which are automatically adjusted for the available CPU and RAM resources.\n* There is no need for Operating System tuning since VictoriaMetrics is optimized for default OS settings.\n The only option is increasing the limit on [the number of open files in the OS](https://medium.com/@muhammadtriwibowo/set-permanently-ulimit-n-open-files-in-ubuntu-4d61064429a),\n so Prometheus instances could establish more connections to VictoriaMetrics.\n* The recommended filesystem is `ext4`, the recommended persistent storage is [persistent HDD-based disk on GCP](https://cloud.google.com/compute/docs/disks/#pdspecs),\n since it is protected from hardware failures via internal replication and it can be [resized on the fly](https://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_pd).\n If you plan to store more than 1TB of data on `ext4` partition or plan extending it to more than 16TB,\n then the following options are recommended to pass to `mkfs.ext4`:\n\n```bash\nmkfs.ext4 ... -O 64bit,huge_file,extent -T huge\n```\n\n## Monitoring\n\nVictoriaMetrics exports internal metrics in Prometheus format at `/metrics` page.\nThese metrics may be collected by [vmagent](https://victoriametrics.github.io/vmagent.html)\nor Prometheus by adding the corresponding scrape config to it.\nAlternatively they can be self-scraped by setting `-selfScrapeInterval` command-line flag to duration greater than 0.\nFor example, `-selfScrapeInterval=10s` would enable self-scraping of `/metrics` page with 10 seconds interval.\n\nThere are officials Grafana dashboards for [single-node VictoriaMetrics](https://grafana.com/dashboards/10229) and [clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11176).\nThere is also an [alternative dashboard for clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11831).\n\nThe most interesting metrics are:\n\n* `vm_cache_entries{type=\"storage/hour_metric_ids\"}` - the number of time series with new data points during the last hour\n aka active time series.\n* `increase(vm_new_timeseries_created_total[1h])` - time series churn rate during the previous hour.\n* `sum(vm_rows{type=~\"storage/.*\"})` - total number of `(timestamp, value)` data points in the database.\n* `sum(rate(vm_rows_inserted_total[5m]))` - ingestion rate, i.e. how many samples are inserted int the database per second.\n* `vm_free_disk_space_bytes` - free space left at `-storageDataPath`.\n* `sum(vm_data_size_bytes)` - the total size of data on disk.\n* `increase(vm_slow_row_inserts_total[5m])` - the number of slow inserts during the last 5 minutes.\n If this number remains high during extended periods of time, then it is likely more RAM is needed for optimal handling\n of the current number of active time series.\n* `increase(vm_slow_metric_name_loads_total[5m])` - the number of slow loads of metric names during the last 5 minutes.\n If this number remains high during extended periods of time, then it is likely more RAM is needed for optimal handling\n of the current number of active time series.\n\nVictoriaMetrics also exposes currently running queries with their execution times at `/api/v1/status/active_queries` page.\n\nSee the example of alerting rules for VM components [here](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/deployment/docker/alerts.yml).\n\n## Troubleshooting\n\n* It is recommended to use default command-line flag values (i.e. don't set them explicitly) until the need\n of tweaking these flag values arises.\n\n* It is recommended upgrading to the latest available release from [this page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases),\n since the encountered issue could be already fixed there.\n\n* It is recommended inspecting logs during troubleshooting, since they may contain useful information.\n\n* VictoriaMetrics buffers incoming data in memory for up to a few seconds before flushing it to persistent storage.\n This may lead to the following \"issues\":\n * Data becomes available for querying in a few seconds after inserting. It is possible to flush in-memory buffers to persistent storage\n by requesting `/internal/force_flush` http handler. This handler is mostly needed for testing and debugging purposes.\n * The last few seconds of inserted data may be lost on unclean shutdown (i.e. OOM, `kill -9` or hardware reset).\n See [this article for technical details](https://valyala.medium.com/wal-usage-looks-broken-in-modern-time-series-databases-b62a627ab704).\n\n* If VictoriaMetrics works slowly and eats more than a CPU core per 100K ingested data points per second,\n then it is likely you have too many active time series for the current amount of RAM.\n VictoriaMetrics [exposes](#monitoring) `vm_slow_*` metrics, which could be used as an indicator of low amounts of RAM.\n It is recommended increasing the amount of RAM on the node with VictoriaMetrics in order to improve\n ingestion and query performance in this case.\n\n* VictoriaMetrics prioritizes data ingestion over data querying. So if it has no enough resources for data ingestion,\n then data querying may slow down significantly.\n\n* VictoriaMetrics requires free disk space for [merging data files to bigger ones](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).\n It may slow down when there is no enough free space left. So make sure `-storageDataPath` directory\n has at least 20% of free space comparing to disk size. The remaining amount of free space\n can be [monitored](#monitoring) via `vm_free_disk_space_bytes` metric. The total size of data\n stored on the disk can be monitored via sum of `vm_data_size_bytes` metrics.\n See also `vm_merge_need_free_disk_space` metrics, which are set to values higher than 0\n if background merge cannot be initiated due to free disk space shortage. The value shows the number of per-month partitions,\n which would start background merge if they had more free disk space.\n\n* If VictoriaMetrics doesn't work because of certain parts are corrupted due to disk errors,\n then just remove directories with broken parts. It is safe removing subdirectories under `<-storageDataPath>/data/{big,small}/YYYY_MM` directories\n when VictoriaMetrics isn't running. This recovers VictoriaMetrics at the cost of data loss stored in the deleted broken parts.\n In the future, `vmrecover` tool will be created for automatic recovering from such errors.\n\n* If you see gaps on the graphs, try resetting the cache by sending request to `/internal/resetRollupResultCache`.\n If this removes gaps on the graphs, then it is likely data with timestamps older than `-search.cacheTimestampOffset`\n is ingested into VictoriaMetrics. Make sure that data sources have synchronized time with VictoriaMetrics.\n\n If the gaps are related to irregular intervals between samples, then try adjusting `-search.minStalenessInterval` command-line flag\n to value close to the maximum interval between samples.\n\n* If you are switching from InfluxDB or TimescaleDB, then take a look at `-search.maxStalenessInterval` command-line flag.\n It may be needed in order to suppress default gap filling algorithm used by VictoriaMetrics - by default it assumes\n each time series is continuous instead of discrete, so it fills gaps between real samples with regular intervals.\n\n* Metrics and labels leading to high cardinality or high churn rate can be determined at `/api/v1/status/tsdb` page.\n See [these docs](https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-stats) for details.\n VictoriaMetrics accepts optional `date=YYYY-MM-DD` and `topN=42` args on this page. By default `date` equals to the current date,\n while `topN` equals to 10.\n\n* VictoriaMetrics limits the number of labels per each metric with `-maxLabelsPerTimeseries` command-line flag.\n This prevents from ingesting metrics with too many labels. It is recommended [monitoring](#monitoring) `vm_metrics_with_dropped_labels_total`\n metric in order to determine whether `-maxLabelsPerTimeseries` must be adjusted for your workload.\n\n* If you store Graphite metrics like `foo.bar.baz` in VictoriaMetrics, then use `{__graphite__=\"foo.*.baz\"}` syntax for selecting such metrics.\n This expression is equivalent to `{__name__=~\"foo[.][^.]*[.]baz\"}`, but it works faster and it is easier to use when migrating from Graphite.\n\n* VictoriaMetrics ignores `NaN` values during data ingestion.\n\n\n## Data migration\n\nUse [vmctl](https://victoriametrics.github.io/vmctl.html) for data migration. It supports the following data migration types:\n\n* From Prometheus to VictoriaMetrics\n* From InfluxDB to VictoriaMetrics\n* From VictoriaMetrics to VictoriaMetrics\n\nSee [vmctl docs](https://victoriametrics.github.io/vmctl.html) for more details.\n\n\n## Backfilling\n\nVictoriaMetrics accepts historical data in arbitrary order of time via [any supported ingestion method](#how-to-import-time-series-data).\nMake sure that configured `-retentionPeriod` covers timestamps for the backfilled data.\n\nIt is recommended disabling query cache with `-search.disableCache` command-line flag when writing\nhistorical data with timestamps from the past, since the cache assumes that the data is written with\nthe current timestamps. Query cache can be enabled after the backfilling is complete.\n\nAn alternative solution is to query `/internal/resetRollupResultCache` url after backfilling is complete. This will reset\nthe query cache, which could contain incomplete data cached during the backfilling.\n\nYet another solution is to increase `-search.cacheTimestampOffset` flag value in order to disable caching\nfor data with timestamps close to the current time. Single-node VictoriaMetrics automatically resets response\ncache when samples with timestamps older than `now - search.cacheTimestampOffset` are ingested to it.\n\n\n## Data updates\n\nVictoriaMetrics doesn't support updating already existing sample values to new ones. It stores all the ingested data points\nfor the same time series with identical timestamps. While it is possible substituting old time series with new time series via\n[removal of old time series](#how-to-delete-timeseries) and then [writing new time series](#backfilling), this approach\nshould be used only for one-off updates. It shouldn't be used for frequent updates because of non-zero overhead related to data removal.\n\n\n## Replication\n\nSingle-node VictoriaMetrics doesn't support application-level replication. Use cluster version instead.\nSee [these docs](https://victoriametrics.github.io/Cluster-VictoriaMetrics.html#replication-and-data-safety) for details.\n\nStorage-level replication may be offloaded to durable persistent storage such as [Google Cloud disks](https://cloud.google.com/compute/docs/disks#pdspecs).\n\nSee also [high availability docs](#high-availability) and [backup docs](#backups).\n\n\n## Backups\n\nVictoriaMetrics supports backups via [vmbackup](https://victoriametrics.github.io/vmbackup.html)\nand [vmrestore](https://victoriametrics.github.io/vmrestore.html) tools.\nWe also provide `vmbackupmanager` tool for paid enterprise subscribers - see [this issue](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/466) for details.\n\n\n## Profiling\n\nVictoriaMetrics provides handlers for collecting the following [Go profiles](https://blog.golang.org/profiling-go-programs):\n\n* Memory profile. It can be collected with the following command:\n\n```bash\ncurl -s http://<victoria-metrics-host>:8428/debug/pprof/heap > mem.pprof\n```\n\n* CPU profile. It can be collected with the following command:\n\n```bash\ncurl -s http://<victoria-metrics-host>:8428/debug/pprof/profile > cpu.pprof\n```\n\nThe command for collecting CPU profile waits for 30 seconds before returning.\n\nThe collected profiles may be analyzed with [go tool pprof](https://github.com/google/pprof).\n\n\n## Integrations\n\n* [Helm charts for single-node and cluster versions of VictoriaMetrics](https://github.com/VictoriaMetrics/helm-charts).\n* [Kubernetes operator for VictoriaMetrics](https://github.com/VictoriaMetrics/operator).\n* [netdata](https://github.com/netdata/netdata) can push data into VictoriaMetrics via `Prometheus remote_write API`.\n See [these docs](https://github.com/netdata/netdata#integrations).\n* [go-graphite/carbonapi](https://github.com/go-graphite/carbonapi) can use VictoriaMetrics as time series backend.\n See [this example](https://github.com/go-graphite/carbonapi/blob/main/cmd/carbonapi/carbonapi.example.victoriametrics.yaml).\n* [Ansible role for installing single-node VictoriaMetrics](https://github.com/dreamteam-gg/ansible-victoriametrics-role).\n* [Ansible role for installing cluster VictoriaMetrics](https://github.com/Slapper/ansible-victoriametrics-cluster-role).\n* [Snap package for VictoriaMetrics](https://snapcraft.io/victoriametrics).\n* [vmalert-cli](https://github.com/aorfanos/vmalert-cli) - a CLI application for managing [vmalert](https://victoriametrics.github.io/vmalert.html).\n\n\n## Third-party contributions\n\n* [Unofficial yum repository](https://copr.fedorainfracloud.org/coprs/antonpatsev/VictoriaMetrics/) ([source code](https://github.com/patsevanton/victoriametrics-rpm))\n* [Prometheus -> VictoriaMetrics exporter #1](https://github.com/ryotarai/prometheus-tsdb-dump)\n* [Prometheus -> VictoriaMetrics exporter #2](https://github.com/AnchorFree/tsdb-remote-write)\n* [Prometheus Oauth proxy](https://gitlab.com/optima_public/prometheus_oauth_proxy) - see [this article](https://medium.com/@richard.holly/powerful-saas-solution-for-detection-metrics-c67b9208d362) for details.\n\n\n## Contacts\n\nContact us with any questions regarding VictoriaMetrics at [info@victoriametrics.com](mailto:info@victoriametrics.com).\n\n\n## Community and contributions\n\nFeel free asking any questions regarding VictoriaMetrics:\n\n* [slack](http://slack.victoriametrics.com/)\n* [reddit](https://www.reddit.com/r/VictoriaMetrics/)\n* [telegram-en](https://t.me/VictoriaMetrics_en)\n* [telegram-ru](https://t.me/VictoriaMetrics_ru1)\n* [google groups](https://groups.google.com/forum/#!forum/victorametrics-users)\n\nIf you like VictoriaMetrics and want to contribute, then we need the following:\n\n* Filing issues and feature requests [here](https://github.com/VictoriaMetrics/VictoriaMetrics/issues).\n* Spreading a word about VictoriaMetrics: conference talks, articles, comments, experience sharing with colleagues.\n* Updating documentation.\n\nWe are open to third-party pull requests provided they follow [KISS design principle](https://en.wikipedia.org/wiki/KISS_principle):\n\n* Prefer simple code and architecture.\n* Avoid complex abstractions.\n* Avoid magic code and fancy algorithms.\n* Avoid [big external dependencies](https://medium.com/@valyala/stripping-dependency-bloat-in-victoriametrics-docker-image-983fb5912b0d).\n* Minimize the number of moving parts in the distributed system.\n* Avoid automated decisions, which may hurt cluster availability, consistency or performance.\n\nAdhering `KISS` principle simplifies the resulting code and architecture, so it can be reviewed, understood and verified by many people.\n\n## Reporting bugs\n\nReport bugs and propose new features [here](https://github.com/VictoriaMetrics/VictoriaMetrics/issues).\n\n\n## Victoria Metrics Logo\n\n[Zip](VM_logo.zip) contains three folders with different image orientations (main color and inverted version).\n\nFiles included in each folder:\n\n* 2 JPEG Preview files\n* 2 PNG Preview files with transparent background\n* 2 EPS Adobe Illustrator EPS10 files\n\n### Logo Usage Guidelines\n\n#### Font used\n\n* Lato Black\n* Lato Regular\n\n#### Color Palette\n\n* HEX [#110f0f](https://www.color-hex.com/color/110f0f)\n* HEX [#ffffff](https://www.color-hex.com/color/ffffff)\n\n### We kindly ask\n\n* Please don't use any other font instead of suggested.\n* There should be sufficient clear space around the logo.\n* Do not change spacing, alignment, or relative locations of the design elements.\n* Do not change the proportions of any of the design elements or the design itself. You may resize as needed but must retain all proportions.\n","dir":"/","name":"Single-server-VictoriaMetrics.md","path":"Single-server-VictoriaMetrics.md","url":"/Single-server-VictoriaMetrics.html"},{"layout":"default","title":"vmagent","content":"## vmagent\n\n`vmagent` is a tiny but brave agent, which helps you collect metrics from various sources\nand stores them in [VictoriaMetrics](https://github.com/VictoriaMetrics/VictoriaMetrics)\nor any other Prometheus-compatible storage system that supports the `remote_write` protocol.\n\n<img alt=\"vmagent\" src=\"vmagent.png\">\n\n\n## Motivation\n\nWhile VictoriaMetrics provides an efficient solution to store and observe metrics, our users needed something fast\nand RAM friendly to scrape metrics from Prometheus-compatible exporters to VictoriaMetrics.\nAlso, we found that users’ infrastructure are snowflakes - no two are alike, and we decided to add more flexibility\nto `vmagent` (like the ability to push metrics instead of pulling them). We did our best and plan to do even more.\n\n\n## Features\n\n* Can be used as drop-in replacement for Prometheus for scraping targets such as [node_exporter](https://github.com/prometheus/node_exporter).\n See [Quick Start](#quick-start) for details.\n* Can add, remove and modify labels (aka tags) via Prometheus relabeling. Can filter data before sending it to remote storage. See [these docs](#relabeling) for details.\n* Accepts data via all the ingestion protocols supported by VictoriaMetrics:\n * Influx line protocol via `http://<vmagent>:8429/write`. See [these docs](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-send-data-from-influxdb-compatible-agents-such-as-telegraf).\n * Graphite plaintext protocol if `-graphiteListenAddr` command-line flag is set. See [these docs](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-send-data-from-graphite-compatible-agents-such-as-statsd).\n * OpenTSDB telnet and http protocols if `-opentsdbListenAddr` command-line flag is set. See [these docs](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-send-data-from-opentsdb-compatible-agents).\n * Prometheus remote write protocol via `http://<vmagent>:8429/api/v1/write`.\n * JSON lines import protocol via `http://<vmagent>:8429/api/v1/import`. See [these docs](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-import-data-in-json-line-format).\n * Native data import protocol via `http://<vmagent>:8429/api/v1/import/native`. See [these docs](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-import-data-in-native-format).\n * Data in Prometheus exposition format. See [these docs](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-import-data-in-prometheus-exposition-format) for details.\n * Arbitrary CSV data via `http://<vmagent>:8429/api/v1/import/csv`. See [these docs](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-import-csv-data).\n* Can replicate collected metrics simultaneously to multiple remote storage systems.\n* Works in environments with unstable connections to remote storage. If the remote storage is unavailable, the collected metrics\n are buffered at `-remoteWrite.tmpDataPath`. The buffered metrics are sent to remote storage as soon as connection\n to remote storage is recovered. The maximum disk usage for the buffer can be limited with `-remoteWrite.maxDiskUsagePerURL`.\n* Uses lower amounts of RAM, CPU, disk IO and network bandwidth compared to Prometheus.\n\n\n## Quick Start\n\nJust download `vmutils-*` archive from [releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases), unpack it\nand pass the following flags to `vmagent` binary in order to start scraping Prometheus targets:\n\n* `-promscrape.config` with the path to Prometheus config file (it is usually located at `/etc/prometheus/prometheus.yml`)\n* `-remoteWrite.url` with the remote storage endpoint such as VictoriaMetrics. The `-remoteWrite.url` argument can be specified multiple times in order to replicate data concurrently to an arbitrary number of remote storage systems.\n\nExample command line:\n\n```\n/path/to/vmagent -promscrape.config=/path/to/prometheus.yml -remoteWrite.url=https://victoria-metrics-host:8428/api/v1/write\n```\n\nIf you only need to collect Influx data, then the following is sufficient:\n\n```\n/path/to/vmagent -remoteWrite.url=https://victoria-metrics-host:8428/api/v1/write\n```\n\nThen send Influx data to `http://vmagent-host:8429`. See [these docs](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-send-data-from-influxdb-compatible-agents-such-as-telegraf) for more details.\n\n`vmagent` is also available in [docker images](https://hub.docker.com/r/victoriametrics/vmagent/tags).\n\nPass `-help` to `vmagent` in order to see the full list of supported command-line flags with their descriptions.\n\n\n## Configuration update\n\n`vmagent` should be restarted in order to update config options set via command-line args.\n\n`vmagent` supports multiple approaches for reloading configs from updated config files such as `-promscrape.config`, `-remoteWrite.relabelConfig` and `-remoteWrite.urlRelabelConfig`:\n\n* Sending `SUGHUP` signal to `vmagent` process:\n ```bash\n kill -SIGHUP `pidof vmagent`\n ```\n\n* Sending HTTP request to `http://vmagent:8429/-/reload` endpoint.\n\nThere is also `-promscrape.configCheckInterval` command-line option, which can be used for automatic reloading configs from updated `-promscrape.config` file.\n\n\n## Use cases\n\n\n### IoT and Edge monitoring\n\n`vmagent` can run and collect metrics in IoT and industrial networks with unreliable or scheduled connections to the remote storage.\nIt buffers the collected data in local files until the connection to remote storage becomes available and then sends the buffered\ndata to the remote storage. It re-tries sending the data to remote storage on any errors.\nThe maximum buffer size can be limited with `-remoteWrite.maxDiskUsagePerURL`.\n\n`vmagent` works on various architectures from IoT world - 32-bit arm, 64-bit arm, ppc64, 386, amd64.\nSee [the corresponding Makefile rules](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/app/vmagent/Makefile) for details.\n\n\n### Drop-in replacement for Prometheus\n\nIf you use Prometheus only for scraping metrics from various targets and forwarding these metrics to remote storage,\nthen `vmagent` can replace such Prometheus setup. Usually `vmagent` requires lower amounts of RAM, CPU and network bandwidth comparing to Prometheus for such a setup.\nSee [these docs](#how-to-collect-metrics-in-prometheus-format) for details.\n\n\n### Replication and high availability\n\n`vmagent` replicates the collected metrics among multiple remote storage instances configured via `-remoteWrite.url` args.\nIf a single remote storage instance temporarily is out of service, then the collected data remains available in another remote storage instances.\n`vmagent` buffers the collected data in files at `-remoteWrite.tmpDataPath` until the remote storage becomes available again.\nThen it sends the buffered data to the remote storage in order to prevent data gaps in the remote storage.\n\n\n### Relabeling and filtering\n\n`vmagent` can add, remove or update labels on the collected data before sending it to remote storage. Additionally,\nit can remove unwanted samples via Prometheus-like relabeling before sending the collected data to remote storage.\nSee [these docs](#relabeling) for details.\n\n\n### Splitting data streams among multiple systems\n\n`vmagent` supports splitting the collected data between muliple destinations with the help of `-remoteWrite.urlRelabelConfig`,\nwhich is applied independently for each configured `-remoteWrite.url` destination. For instance, it is possible to replicate or split\ndata among long-term remote storage, short-term remote storage and real-time analytical system [built on top of Kafka](https://github.com/Telefonica/prometheus-kafka-adapter).\nNote that each destination can receive its own subset of the collected data thanks to per-destination relabeling via `-remoteWrite.urlRelabelConfig`.\n\n\n### Prometheus remote_write proxy\n\n`vmagent` may be used as a proxy for Prometheus data sent via Prometheus `remote_write` protocol. It can accept data via `remote_write` API\nat `/api/v1/write` endpoint, apply relabeling and filtering and then proxy it to another `remote_write` systems.\nThe `vmagent` can be configured to encrypt the incoming `remote_write` requests with `-tls*` command-line flags.\nAdditionally, Basic Auth can be enabled for the incoming `remote_write` requests with `-httpAuth.*` command-line flags.\n\n\n### remote_write for clustered version\n\nDespite `vmagent` can accept data in several supported protocols (OpenTSDB, Influx, Prometheus, Graphite) and scrape data from various targets, writes always peformed in Promethes remote_write protocol. Therefore for clustered version `-remoteWrite.url` command-line flag should be configured as `<schema>://<vminsert-host>:8480/insert/<customer-id>/prometheus/api/v1/write`\n\n\n## How to collect metrics in Prometheus format\n\nPass the path to `prometheus.yml` to `-promscrape.config` command-line flag. `vmagent` takes into account the following\nsections from [Prometheus config file](https://prometheus.io/docs/prometheus/latest/configuration/configuration/):\n\n* `global`\n* `scrape_configs`\n\nAll the other sections are ignored, including [remote_write](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write) section.\nUse `-remoteWrite.*` command-line flags instead for configuring remote write settings.\n\nThe following scrape types in [scrape_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) section are supported:\n\n* `static_configs` - for scraping statically defined targets. See [these docs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#static_config) for details.\n* `file_sd_configs` - for scraping targets defined in external files aka file-based service discover.\n See [these docs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#file_sd_config) for details.\n* `kubernetes_sd_configs` - for scraping targets in Kubernetes (k8s).\n See [kubernetes_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config) for details.\n* `ec2_sd_configs` - for scraping targets in Amazon EC2.\n See [ec2_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config) for details.\n `vmagent` doesn't support `profile` config param and aws credentials file yet.\n* `gce_sd_configs` - for scraping targets in Google Compute Engine (GCE).\n See [gce_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#gce_sd_config) for details.\n `vmagent` provides the following additional functionality for `gce_sd_config`:\n * if `project` arg is missing, then `vmagent` uses the project for the instance where it runs;\n * if `zone` arg is missing, then `vmagent` uses the zone for the instance where it runs;\n * if `zone` arg equals to `\"*\"`, then `vmagent` discovers all the zones for the given project;\n * `zone` may contain arbitrary number of zones, i.e. `zone: [us-east1-a, us-east1-b]`.\n* `consul_sd_configs` - for scraping targets registered in Consul.\n See [consul_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config) for details.\n* `dns_sd_configs` - for scraping targets discovered from DNS records (SRV, A and AAAA).\n See [dns_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dns_sd_config) for details.\n* `openstack_sd_configs` - for scraping OpenStack targets.\n See [openstack_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#openstack_sd_config) for details.\n [OpenStack identity API v3](https://docs.openstack.org/api-ref/identity/v3/) is supported only.\n* `dockerswarm_sd_configs` - for scraping Docker Swarm targets.\n See [dockerswarm_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dockerswarm_sd_config) for details.\n* `eureka_sd_configs` - for scraping targets registered in [Netflix Eureka](https://github.com/Netflix/eureka).\n See [eureka_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#eureka_sd_config) for details.\n\nFile feature requests at [our issue tracker](https://github.com/VictoriaMetrics/VictoriaMetrics/issues) if you need other service discovery mechanisms to be supported by `vmagent`.\n\n`vmagent` also support the following additional options in `scrape_config` section:\n\n* `disable_compression: true` - for disabling response compression on a per-job basis. By default `vmagent` requests compressed responses from scrape targets\n in order to save network bandwidth.\n* `disable_keepalive: true` - for disabling [HTTP keep-alive connections](https://en.wikipedia.org/wiki/HTTP_persistent_connection) on a per-job basis.\n By default `vmagent` uses keep-alive connections to scrape targets in order to reduce overhead on connection re-establishing.\n\nNote that `vmagent` doesn't support `refresh_interval` option these scrape configs. Use the corresponding `-promscrape.*CheckInterval`\ncommand-line flag instead. For example, `-promscrape.consulSDCheckInterval=60s` sets `refresh_interval` for all the `consul_sd_configs`\nentries to 60s. Run `vmagent -help` in order to see default values for `-promscrape.*CheckInterval` flags.\n\nThe file pointed by `-promscrape.config` may contain `%{ENV_VAR}` placeholders, which are substituted by the corresponding `ENV_VAR` environment variable values.\n\n\n## Adding labels to metrics\n\nLabels can be added to metrics via the following mechanisms:\n\n* Via `global -> external_labels` section in `-promscrape.config` file. These labels are added only to metrics scraped from targets configured in `-promscrape.config` file.\n* Via `-remoteWrite.label` command-line flag. These labels are added to all the collected metrics before sending them to `-remoteWrite.url`.\n\n\n## Relabeling\n\n`vmagent` supports [Prometheus relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config).\nAdditionally it provides the following extra actions:\n\n* `replace_all`: replaces all the occurences of `regex` in the values of `source_labels` with the `replacement` and stores the result in the `target_label`.\n* `labelmap_all`: replaces all the occurences of `regex` in all the label names with the `replacement`.\n* `keep_if_equal`: keeps the entry if all label values from `source_labels` are equal.\n* `drop_if_equal`: drops the entry if all the label values from `source_labels` are equal.\n\nThe relabeling can be defined in the following places:\n\n* At `scrape_config -> relabel_configs` section in `-promscrape.config` file. This relabeling is applied to target labels.\n* At `scrape_config -> metric_relabel_configs` section in `-promscrape.config` file. This relabeling is applied to all the scraped metrics in the given `scrape_config`.\n* At `-remoteWrite.relabelConfig` file. This relabeling is aplied to all the collected metrics before sending them to remote storage.\n* At `-remoteWrite.urlRelabelConfig` files. This relabeling is applied to metrics before sending them to the corresponding `-remoteWrite.url`.\n\nRead more about relabeling in the following articles:\n\n* [How to use Relabeling in Prometheus and VictoriaMetrics](https://valyala.medium.com/how-to-use-relabeling-in-prometheus-and-victoriametrics-8b90fc22c4b2)\n* [Life of a label](https://www.robustperception.io/life-of-a-label)\n* [Discarding targets and timeseries with relabeling](https://www.robustperception.io/relabelling-can-discard-targets-timeseries-and-alerts)\n* [Dropping labels at scrape time](https://www.robustperception.io/dropping-metrics-at-scrape-time-with-prometheus)\n* [Extracting labels from legacy metric names](https://www.robustperception.io/extracting-labels-from-legacy-metric-names)\n* [relabel_configs vs metric_relabel_configs](https://www.robustperception.io/relabel_configs-vs-metric_relabel_configs)\n\n\n## Monitoring\n\n`vmagent` exports various metrics in Prometheus exposition format at `http://vmagent-host:8429/metrics` page. It is recommended setting up regular scraping of this page\neither via `vmagent` itself or via Prometheus, so the exported metrics could be analyzed later.\nUse official [Grafana dashboard](https://grafana.com/grafana/dashboards/12683) for `vmagent` state overview.\nIf you have suggestions, improvements or found a bug - feel free to open an issue on github or add review to the dashboard.\n\n`vmagent` also exports target statuses at the following handlers:\n\n* `http://vmagent-host:8429/targets`. This handler returns human-readable plaintext status for every active target.\nThis page is convenient to query from command line with `wget`, `curl` or similar tools.\nIt accepts optional `show_original_labels=1` query arg, which shows the original labels per each target before applying relabeling.\nThis information may be useful for debugging target relabeling.\n* `http://vmagent-host:8429/api/v1/targets`. This handler returns data compatible with [the corresponding page from Prometheus API](https://prometheus.io/docs/prometheus/latest/querying/api/#targets).\n\n* `http://vmagent-host:8429/ready`. This handler returns http 200 status code when `vmagent` finishes initialization for all service_discovery configs.\nIt may be useful for performing `vmagent` rolling update without scrape loss.\n\n\n## Troubleshooting\n\n* It is recommended [setting up the official Grafana dashboard](#monitoring) in order to monitor `vmagent` state.\n\n* It is recommended increasing the maximum number of open files in the system (`ulimit -n`) when scraping big number of targets,\n since `vmagent` establishes at least a single TCP connection per each target.\n\n* When `vmagent` scrapes many unreliable targets, it can flood error log with scrape errors. These errors can be suppressed\n by passing `-promscrape.suppressScrapeErrors` command-line flag to `vmagent`. The most recent scrape error per each target can be observed at `http://vmagent-host:8429/targets`\n and `http://vmagent-host:8429/api/v1/targets`.\n\n* The `/api/v1/targets` page could be useful for debugging relabeling process for scrape targets.\n This page contains original labels for targets dropped during relabeling (see \"droppedTargets\" section in the page output). By default up to `-promscrape.maxDroppedTargets` targets are shown here. If your setup drops more targets during relabeling, then increase `-promscrape.maxDroppedTargets` command-line flag value in order to see all the dropped targets. Note that tracking each dropped target requires up to 10Kb of RAM, so big values for `-promscrape.maxDroppedTargets` may result in increased memory usage if big number of scrape targets are dropped during relabeling.\n\n* If `vmagent` scrapes big number of targets, then `-promscrape.dropOriginalLabels` command-line option may be passed to `vmagent` in order to reduce memory usage.\n This option drops `\"discoveredLabels\"` and `\"droppedTargets\"` lists at `/api/v1/targets` page, which may result in reduced debuggability for improperly configured per-target relabeling.\n\n* If `vmagent` scrapes targets with millions of metrics per each target (for instance, when scraping [federation endpoints](https://prometheus.io/docs/prometheus/latest/federation/)),\n then it is recommended enabling `stream parsing mode` in order to reduce memory usage during scraping. This mode may be enabled either globally for all the scrape targets\n by passing `-promscrape.streamParse` command-line flag or on a per-scrape target basis with `stream_parse: true` option. For example:\n\n ```yml\n scrape_configs:\n - job_name: 'big-federate'\n stream_parse: true\n static_configs:\n - targets:\n - big-prometeus1\n - big-prometeus2\n honor_labels: true\n metrics_path: /federate\n params:\n 'match[]': ['{__name__!=\"\"}']\n ```\n\n Note that `sample_limit` option doesn't work if stream parsing is enabled, since the parsed data is pushed to remote storage as soon as it is parsed. So `sample_limit` option\n has no sense during stream parsing.\n\n* It is recommended to increase `-remoteWrite.queues` if `vmagent_remotewrite_pending_data_bytes` metric exported at `http://vmagent-host:8429/metrics` page constantly grows.\n\n* If you see gaps on the data pushed by `vmagent` to remote storage when `-remoteWrite.maxDiskUsagePerURL` is set, then try increasing `-remoteWrite.queues`.\n Such gaps may appear because `vmagent` cannot keep up with sending the collected data to remote storage, so it starts dropping the buffered data\n if the on-disk buffer size exceeds `-remoteWrite.maxDiskUsagePerURL`.\n\n* `vmagent` buffers scraped data at `-remoteWrite.tmpDataPath` directory until it is sent to `-remoteWrite.url`.\n The directory can grow large when remote storage is unavailable for extended periods of time and if `-remoteWrite.maxDiskUsagePerURL` isn't set.\n If you don't want to send all the data from the directory to remote storage, simply stop `vmagent` and delete the directory.\n\n* By default `vmagent` masks `-remoteWrite.url` with `secret-url` values in logs and at `/metrics` page because\n the url may contain sensitive information such as auth tokens or passwords.\n Pass `-remoteWrite.showURL` command-line flag when starting `vmagent` in order to see all the valid urls.\n\n* If scrapes must be aligned in time (for instance, if they must be performed at the beginning of every hour), then set `scrape_align_interval` option\n in the corresponding scrape config. For example, the following config aligns hourly scrapes to the nearest 10 minutes:\n\n ```yml\n scrape_configs:\n - job_name: foo\n scrape_interval: 1h\n scrape_align_interval: 10m\n ```\n\n* If you see `skipping duplicate scrape target with identical labels` errors when scraping Kubernetes pods, then it is likely these pods listen multiple ports\n or they use init container. These errors can be either fixed or suppressed with `-promscrape.suppressDuplicateScrapeTargetErrors` command-line flag.\n See available options below if you prefer fixing the root cause of the error:\n\n The following `relabel_configs` section may help determining `__meta_*` labels resulting in duplicate targets:\n ```yml\n - action: labelmap\n regex: __meta_(.*)\n ```\n\n The following relabeling rule may be added to `relabel_configs` section in order to filter out pods with unneeded ports:\n ```yml\n - action: keep_if_equal\n source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_container_port_number]\n ```\n\n The following relabeling rule may be added to `relabel_configs` section in order to filter out init container pods:\n ```yml\n - action: drop\n source_labels: [__meta_kubernetes_pod_container_init]\n regex: true\n ```\n\n\n## How to build from sources\n\nIt is recommended using [binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) - `vmagent` is located in `vmutils-*` archives there.\n\n\n### Development build\n\n1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.13.\n2. Run `make vmagent` from the root folder of the repository.\n It builds `vmagent` binary and puts it into the `bin` folder.\n\n### Production build\n\n1. [Install docker](https://docs.docker.com/install/).\n2. Run `make vmagent-prod` from the root folder of the repository.\n It builds `vmagent-prod` binary and puts it into the `bin` folder.\n\n### Building docker images\n\nRun `make package-vmagent`. It builds `victoriametrics/vmagent:<PKG_TAG>` docker image locally.\n`<PKG_TAG>` is auto-generated image tag, which depends on source code in the repository.\nThe `<PKG_TAG>` may be manually set via `PKG_TAG=foobar make package-vmagent`.\n\nThe base docker image is [alpine](https://hub.docker.com/_/alpine) but it is possible to use any other base image\nby setting it via `<ROOT_IMAGE>` environment variable. For example, the following command builds the image on top of [scratch](https://hub.docker.com/_/scratch) image:\n\n```bash\nROOT_IMAGE=scratch make package-vmagent\n```\n\n### ARM build\n\nARM build may run on Raspberry Pi or on [energy-efficient ARM servers](https://blog.cloudflare.com/arm-takes-wing/).\n\n### Development ARM build\n\n1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.13.\n2. Run `make vmagent-arm` or `make vmagent-arm64` from the root folder of the repository.\n It builds `vmagent-arm` or `vmagent-arm64` binary respectively and puts it into the `bin` folder.\n\n### Production ARM build\n\n1. [Install docker](https://docs.docker.com/install/).\n2. Run `make vmagent-arm-prod` or `make vmagent-arm64-prod` from the root folder of the repository.\n It builds `vmagent-arm-prod` or `vmagent-arm64-prod` binary respectively and puts it into the `bin` folder.\n\n\n## Profiling\n\n`vmagent` provides handlers for collecting the following [Go profiles](https://blog.golang.org/profiling-go-programs):\n\n* Memory profile. It can be collected with the following command:\n\n```bash\ncurl -s http://<vmagent-host>:8429/debug/pprof/heap > mem.pprof\n```\n\n* CPU profile. It can be collected with the following command:\n\n```bash\ncurl -s http://<vmagent-host>:8429/debug/pprof/profile > cpu.pprof\n```\n\nThe command for collecting CPU profile waits for 30 seconds before returning.\n\nThe collected profiles may be analyzed with [go tool pprof](https://github.com/google/pprof).\n\n\n## Advanced usage\n\n`vmagent` can be fine-tuned with various command-line flags. Run `./vmagent -help` in order to see the full list of these flags with their desciptions and default values:\n\n```\n./vmagent -help\n\nvmagent collects metrics data via popular data ingestion protocols and routes it to VictoriaMetrics.\n\nSee the docs at https://victoriametrics.github.io/vmagent.html .\n\n -csvTrimTimestamp duration\n \tTrim timestamps when importing csv data to this duration. Minimum practical duration is 1ms. Higher duration (i.e. 1s) may be used for reducing disk space usage for timestamp data (default 1ms)\n -dryRun\n \tWhether to check only config files without running vmagent. The following files are checked: -promscrape.config, -remoteWrite.relabelConfig, -remoteWrite.urlRelabelConfig . Unknown config entries are allowed in -promscrape.config by default. This can be changed with -promscrape.config.strictParse\n -enableTCP6\n \tWhether to enable IPv6 for listening and dialing. By default only IPv4 TCP is used\n -envflag.enable\n \tWhether to enable reading flags from environment variables additionally to command line. Command line flag values have priority over values from environment vars. Flags are read only from command line if this flag isn't set\n -envflag.prefix string\n \tPrefix for environment variables if -envflag.enable is set\n -fs.disableMmap\n \tWhether to use pread() instead of mmap() for reading data files. By default mmap() is used for 64-bit arches and pread() is used for 32-bit arches, since they cannot read data files bigger than 2^32 bytes in memory. mmap() is usually faster for reading small data chunks than pread()\n -graphiteListenAddr string\n \tTCP and UDP address to listen for Graphite plaintext data. Usually :2003 must be set. Doesn't work if empty\n -graphiteTrimTimestamp duration\n \tTrim timestamps for Graphite data to this duration. Minimum practical duration is 1s. Higher duration (i.e. 1m) may be used for reducing disk space usage for timestamp data (default 1s)\n -http.connTimeout duration\n \tIncoming http connections are closed after the configured timeout. This may help spreading incoming load among a cluster of services behind load balancer. Note that the real timeout may be bigger by up to 10% as a protection from Thundering herd problem (default 2m0s)\n -http.disableResponseCompression\n \tDisable compression of HTTP responses for saving CPU resources. By default compression is enabled to save network bandwidth\n -http.idleConnTimeout duration\n \tTimeout for incoming idle http connections (default 1m0s)\n -http.maxGracefulShutdownDuration duration\n \tThe maximum duration for graceful shutdown of HTTP server. Highly loaded server may require increased value for graceful shutdown (default 7s)\n -http.pathPrefix string\n \tAn optional prefix to add to all the paths handled by http server. For example, if '-http.pathPrefix=/foo/bar' is set, then all the http requests will be handled on '/foo/bar/*' paths. This may be useful for proxied requests. See https://www.robustperception.io/using-external-urls-and-proxies-with-prometheus\n -http.shutdownDelay duration\n \tOptional delay before http server shutdown. During this dealy the servier returns non-OK responses from /health page, so load balancers can route new requests to other servers\n -httpAuth.password string\n \tPassword for HTTP Basic Auth. The authentication is disabled if -httpAuth.username is empty\n -httpAuth.username string\n \tUsername for HTTP Basic Auth. The authentication is disabled if empty. See also -httpAuth.password\n -httpListenAddr string\n \tTCP address to listen for http connections. Set this flag to empty value in order to disable listening on any port. This mode may be useful for running multiple vmagent instances on the same server. Note that /targets and /metrics pages aren't available if -httpListenAddr='' (default \":8429\")\n -import.maxLineLen max_rows_per_line\n \tThe maximum length in bytes of a single line accepted by /api/v1/import; the line length can be limited with max_rows_per_line query arg passed to /api/v1/export\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 104857600)\n -influx.maxLineSize value\n \tThe maximum size in bytes for a single Influx line during parsing\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 262144)\n -influxListenAddr http://<vmagent>:8429/write\n \tTCP and UDP address to listen for Influx line protocol data. Usually :8189 must be set. Doesn't work if empty. This flag isn't needed when ingesting data over HTTP - just send it to http://<vmagent>:8429/write\n -influxMeasurementFieldSeparator string\n \tSeparator for '{measurement}{separator}{field_name}' metric name when inserted via Influx line protocol (default \"_\")\n -influxSkipMeasurement\n \tUses '{field_name}' as a metric name while ignoring '{measurement}' and '-influxMeasurementFieldSeparator'\n -influxSkipSingleField\n \tUses '{measurement}' instead of '{measurement}{separator}{field_name}' for metic name if Influx line contains only a single field\n -influxTrimTimestamp duration\n \tTrim timestamps for Influx line protocol data to this duration. Minimum practical duration is 1ms. Higher duration (i.e. 1s) may be used for reducing disk space usage for timestamp data (default 1ms)\n -insert.maxQueueDuration duration\n \tThe maximum duration for waiting in the queue for insert requests due to -maxConcurrentInserts (default 1m0s)\n -loggerDisableTimestamps\n \tWhether to disable writing timestamps in logs\n -loggerErrorsPerSecondLimit int\n \tPer-second limit on the number of ERROR messages. If more than the given number of errors are emitted per second, then the remaining errors are suppressed. Zero value disables the rate limit\n -loggerFormat string\n \tFormat for logs. Possible values: default, json (default \"default\")\n -loggerLevel string\n \tMinimum level of errors to log. Possible values: INFO, WARN, ERROR, FATAL, PANIC (default \"INFO\")\n -loggerOutput string\n \tOutput for the logs. Supported values: stderr, stdout (default \"stderr\")\n -loggerTimezone string\n \tTimezone to use for timestamps in logs. Local timezone can be used (default \"UTC\")\n -loggerWarnsPerSecondLimit int\n \tPer-second limit on the number of WARN messages. If more than the given number of warns are emitted per second, then the remaining warns are suppressed. Zero value disables the rate limit\n -maxConcurrentInserts int\n \tThe maximum number of concurrent inserts. Default value should work for most cases, since it minimizes the overhead for concurrent inserts. This option is tigthly coupled with -insert.maxQueueDuration (default 16)\n -maxInsertRequestSize value\n \tThe maximum size in bytes of a single Prometheus remote_write API request\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 33554432)\n -memory.allowedBytes value\n \tAllowed size of system memory VictoriaMetrics caches may occupy. This option overrides -memory.allowedPercent if set to non-zero value. Too low value may increase cache miss rate, which usually results in higher CPU and disk IO usage. Too high value may evict too much data from OS page cache, which will result in higher disk IO usage\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 0)\n -memory.allowedPercent float\n \tAllowed percent of system memory VictoriaMetrics caches may occupy. See also -memory.allowedBytes. Too low value may increase cache miss rate, which usually results in higher CPU and disk IO usage. Too high value may evict too much data from OS page cache, which will result in higher disk IO usage (default 60)\n -metricsAuthKey string\n \tAuth key for /metrics. It overrides httpAuth settings\n -opentsdbHTTPListenAddr string\n \tTCP address to listen for OpentTSDB HTTP put requests. Usually :4242 must be set. Doesn't work if empty\n -opentsdbListenAddr string\n \tTCP and UDP address to listen for OpentTSDB metrics. Telnet put messages and HTTP /api/put messages are simultaneously served on TCP port. Usually :4242 must be set. Doesn't work if empty\n -opentsdbTrimTimestamp duration\n \tTrim timestamps for OpenTSDB 'telnet put' data to this duration. Minimum practical duration is 1s. Higher duration (i.e. 1m) may be used for reducing disk space usage for timestamp data (default 1s)\n -opentsdbhttp.maxInsertRequestSize value\n \tThe maximum size of OpenTSDB HTTP put request\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 33554432)\n -opentsdbhttpTrimTimestamp duration\n \tTrim timestamps for OpenTSDB HTTP data to this duration. Minimum practical duration is 1ms. Higher duration (i.e. 1s) may be used for reducing disk space usage for timestamp data (default 1ms)\n -pprofAuthKey string\n \tAuth key for /debug/pprof. It overrides httpAuth settings\n -promscrape.config string\n \tOptional path to Prometheus config file with 'scrape_configs' section containing targets to scrape. See https://victoriametrics.github.io/#how-to-scrape-prometheus-exporters-such-as-node-exporter for details\n -promscrape.config.dryRun\n \tChecks -promscrape.config file for errors and unsupported fields and then exits. Returns non-zero exit code on parsing errors and emits these errors to stderr. See also -promscrape.config.strictParse command-line flag. Pass -loggerLevel=ERROR if you don't need to see info messages in the output.\n -promscrape.config.strictParse\n \tWhether to allow only supported fields in -promscrape.config . By default unsupported fields are silently skipped\n -promscrape.configCheckInterval duration\n \tInterval for checking for changes in '-promscrape.config' file. By default the checking is disabled. Send SIGHUP signal in order to force config check for changes\n -promscrape.consulSDCheckInterval consul_sd_configs\n \tInterval for checking for changes in Consul. This works only if consul_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config for details (default 30s)\n -promscrape.disableCompression\n \tWhether to disable sending 'Accept-Encoding: gzip' request headers to all the scrape targets. This may reduce CPU usage on scrape targets at the cost of higher network bandwidth utilization. It is possible to set 'disable_compression: true' individually per each 'scrape_config' section in '-promscrape.config' for fine grained control\n -promscrape.disableKeepAlive disable_keepalive: true\n \tWhether to disable HTTP keep-alive connections when scraping all the targets. This may be useful when targets has no support for HTTP keep-alive connection. It is possible to set disable_keepalive: true individually per each 'scrape_config` section in '-promscrape.config' for fine grained control. Note that disabling HTTP keep-alive may increase load on both vmagent and scrape targets\n -promscrape.discovery.concurrency int\n \tThe maximum number of concurrent requests to Prometheus autodiscovery API (Consul, Kubernetes, etc.) (default 100)\n -promscrape.discovery.concurrentWaitTime duration\n \tThe maximum duration for waiting to perform API requests if more than -promscrape.discovery.concurrency requests are simultaneously performed (default 1m0s)\n -promscrape.dnsSDCheckInterval dns_sd_configs\n \tInterval for checking for changes in dns. This works only if dns_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dns_sd_config for details (default 30s)\n -promscrape.dockerswarmSDCheckInterval dockerswarm_sd_configs\n \tInterval for checking for changes in dockerswarm. This works only if dockerswarm_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dockerswarm_sd_config for details (default 30s)\n -promscrape.dropOriginalLabels\n \tWhether to drop original labels for scrape targets at /targets and /api/v1/targets pages. This may be needed for reducing memory usage when original labels for big number of scrape targets occupy big amounts of memory. Note that this reduces debuggability for improper per-target relabeling configs\n -promscrape.ec2SDCheckInterval ec2_sd_configs\n \tInterval for checking for changes in ec2. This works only if ec2_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config for details (default 1m0s)\n -promscrape.eurekaSDCheckInterval eureka_sd_configs\n \tInterval for checking for changes in eureka. This works only if eureka_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#eureka_sd_config for details (default 30s)\n -promscrape.fileSDCheckInterval duration\n \tInterval for checking for changes in 'file_sd_config'. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#file_sd_config for details (default 30s)\n -promscrape.gceSDCheckInterval gce_sd_configs\n \tInterval for checking for changes in gce. This works only if gce_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#gce_sd_config for details (default 1m0s)\n -promscrape.kubernetesSDCheckInterval kubernetes_sd_configs\n \tInterval for checking for changes in Kubernetes API server. This works only if kubernetes_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config for details (default 30s)\n -promscrape.maxDroppedTargets droppedTargets\n \tThe maximum number of droppedTargets shown at /api/v1/targets page. Increase this value if your setup drops more scrape targets during relabeling and you need investigating labels for all the dropped targets. Note that the increased number of tracked dropped targets may result in increased memory usage (default 1000)\n -promscrape.maxScrapeSize value\n \tThe maximum size of scrape response in bytes to process from Prometheus targets. Bigger responses are rejected\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 16777216)\n -promscrape.openstackSDCheckInterval openstack_sd_configs\n \tInterval for checking for changes in openstack API server. This works only if openstack_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#openstack_sd_config for details (default 30s)\n -promscrape.streamParse stream_parse: true\n \tWhether to enable stream parsing for metrics obtained from scrape targets. This may be useful for reducing memory usage when millions of metrics are exposed per each scrape target. It is posible to set stream_parse: true individually per each `scrape_config` section in `-promscrape.config` for fine grained control\n -promscrape.suppressDuplicateScrapeTargetErrors duplicate scrape target\n \tWhether to suppress duplicate scrape target errors; see https://victoriametrics.github.io/vmagent.html#troubleshooting for details\n -promscrape.suppressScrapeErrors\n \tWhether to suppress scrape errors logging. The last error for each target is always available at '/targets' page even if scrape errors logging is suppressed\n -remoteWrite.basicAuth.password array\n \tOptional basic auth password to use for -remoteWrite.url. If multiple args are set, then they are applied independently for the corresponding -remoteWrite.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.basicAuth.username array\n \tOptional basic auth username to use for -remoteWrite.url. If multiple args are set, then they are applied independently for the corresponding -remoteWrite.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.bearerToken array\n \tOptional bearer auth token to use for -remoteWrite.url. If multiple args are set, then they are applied independently for the corresponding -remoteWrite.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.flushInterval duration\n \tInterval for flushing the data to remote storage. Higher value reduces network bandwidth usage at the cost of delayed push of scraped data to remote storage. Minimum supported interval is 1 second (default 1s)\n -remoteWrite.label array\n \tOptional label in the form 'name=value' to add to all the metrics before sending them to -remoteWrite.url. Pass multiple -remoteWrite.label flags in order to add multiple flags to metrics before sending them to remote storage\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.maxBlockSize value\n \tThe maximum size in bytes of unpacked request to send to remote storage. It shouldn't exceed -maxInsertRequestSize from VictoriaMetrics\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 8388608)\n -remoteWrite.maxDiskUsagePerURL value\n \tThe maximum file-based buffer size in bytes at -remoteWrite.tmpDataPath for each -remoteWrite.url. When buffer size reaches the configured maximum, then old data is dropped when adding new data to the buffer. Buffered data is stored in ~500MB chunks, so the minimum practical value for this flag is 500000000. Disk usage is unlimited if the value is set to 0\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 0)\n -remoteWrite.proxyURL array\n \tOptional proxy URL for writing data to -remoteWrite.url. Supported proxies: http, https, socks5. Example: -remoteWrite.proxyURL=socks5://proxy:1234\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.queues int\n \tThe number of concurrent queues to each -remoteWrite.url. Set more queues if default number of queues isn't enough for sending high volume of collected data to remote storage (default 4)\n -remoteWrite.rateLimit array\n \tOptional rate limit in bytes per second for data sent to -remoteWrite.url. By default the rate limit is disabled. It can be useful for limiting load on remote storage when big amounts of buffered data is sent after temporary unavailability of the remote storage\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.relabelConfig string\n \tOptional path to file with relabel_config entries. These entries are applied to all the metrics before sending them to -remoteWrite.url. See https://victoriametrics.github.io/vmagent.html#relabeling for details\n -remoteWrite.roundDigits array\n \tRound metric values to this number of decimal digits after the point before writing them to remote storage. Examples: -remoteWrite.roundDigits=2 would round 1.236 to 1.24, while -remoteWrite.roundDigits=-1 would round 126.78 to 130. By default digits rounding is disabled. Set it to 100 for disabling it for a particular remote storage. This option may be used for improving data compression for the stored metrics\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.sendTimeout array\n \tTimeout for sending a single block of data to -remoteWrite.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.showURL\n \tWhether to show -remoteWrite.url in the exported metrics. It is hidden by default, since it can contain sensitive info such as auth key\n -remoteWrite.significantFigures array\n \tThe number of significant figures to leave in metric values before writing them to remote storage. See https://en.wikipedia.org/wiki/Significant_figures . Zero value saves all the significant figures. This option may be used for improving data compression for the stored metrics. See also -remoteWrite.roundDigits\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.tlsCAFile array\n \tOptional path to TLS CA file to use for verifying connections to -remoteWrite.url. By default system CA is used. If multiple args are set, then they are applied independently for the corresponding -remoteWrite.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.tlsCertFile array\n \tOptional path to client-side TLS certificate file to use when connecting to -remoteWrite.url. If multiple args are set, then they are applied independently for the corresponding -remoteWrite.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.tlsInsecureSkipVerify array\n \tWhether to skip tls verification when connecting to -remoteWrite.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.tlsKeyFile array\n \tOptional path to client-side TLS certificate key to use when connecting to -remoteWrite.url. If multiple args are set, then they are applied independently for the corresponding -remoteWrite.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.tlsServerName array\n \tOptional TLS server name to use for connections to -remoteWrite.url. By default the server name from -remoteWrite.url is used. If multiple args are set, then they are applied independently for the corresponding -remoteWrite.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.tmpDataPath string\n \tPath to directory where temporary data for remote write component is stored (default \"vmagent-remotewrite-data\")\n -remoteWrite.url array\n \tRemote storage URL to write data to. It must support Prometheus remote_write API. It is recommended using VictoriaMetrics as remote storage. Example url: http://<victoriametrics-host>:8428/api/v1/write . Pass multiple -remoteWrite.url flags in order to write data concurrently to multiple remote storage systems\n \tSupports array of values separated by comma or specified via multiple flags.\n -remoteWrite.urlRelabelConfig array\n \tOptional path to relabel config for the corresponding -remoteWrite.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -tls\n \tWhether to enable TLS (aka HTTPS) for incoming requests. -tlsCertFile and -tlsKeyFile must be set if -tls is set\n -tlsCertFile string\n \tPath to file with TLS certificate. Used only if -tls is set. Prefer ECDSA certs instead of RSA certs, since RSA certs are slow\n -tlsKeyFile string\n \tPath to file with TLS key. Used only if -tls is set\n -version\n \tShow VictoriaMetrics version\n```\n","dir":"/","name":"vmagent.md","path":"vmagent.md","url":"/vmagent.html"},{"layout":"default","title":"vmalert","content":"## vmalert\n\n`vmalert` executes a list of given [alerting](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/)\nor [recording](https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/)\nrules against configured address.\n\n### Features:\n* Integration with [VictoriaMetrics](https://github.com/VictoriaMetrics/VictoriaMetrics) TSDB;\n* VictoriaMetrics [MetricsQL](https://victoriametrics.github.io/MetricsQL.html)\n support and expressions validation;\n* Prometheus [alerting rules definition format](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#defining-alerting-rules)\n support;\n* Integration with [Alertmanager](https://github.com/prometheus/alertmanager);\n* Keeps the alerts [state on restarts](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/master/app/vmalert#alerts-state-on-restarts);\n* Graphite datasource can be used for alerting and recording rules. See [these docs](#graphite) for details.\n* Lightweight without extra dependencies.\n\n### Limitations:\n* `vmalert` execute queries against remote datasource which has reliability risks because of network. \nIt is recommended to configure alerts thresholds and rules expressions with understanding that network request\nmay fail;\n* by default, rules execution is sequential within one group, but persisting of execution results to remote\nstorage is asynchronous. Hence, user shouldn't rely on recording rules chaining when result of previous\nrecording rule is reused in next one;\n* `vmalert` has no UI, just an API for getting groups and rules statuses.\n\n### QuickStart\n\nTo build `vmalert` from sources:\n```\ngit clone https://github.com/VictoriaMetrics/VictoriaMetrics\ncd VictoriaMetrics\nmake vmalert\n```\nThe build binary will be placed to `VictoriaMetrics/bin` folder.\n\nTo start using `vmalert` you will need the following things:\n* list of rules - PromQL/MetricsQL expressions to execute;\n* datasource address - reachable VictoriaMetrics instance for rules execution;\n* notifier address - reachable [Alert Manager](https://github.com/prometheus/alertmanager) instance for processing, \naggregating alerts and sending notifications.\n* remote write address - [remote write](https://prometheus.io/docs/prometheus/latest/storage/#remote-storage-integrations)\ncompatible storage address for storing recording rules results and alerts state in for of timeseries. This is optional.\n\nThen configure `vmalert` accordingly:\n```\n./bin/vmalert -rule=alert.rules \\\n -datasource.url=http://localhost:8428 \\ # PromQL compatible datasource\n -notifier.url=http://localhost:9093 \\ # AlertManager URL\n -notifier.url=http://127.0.0.1:9093 \\ # AlertManager replica URL\n -remoteWrite.url=http://localhost:8428 \\ # remote write compatible storage to persist rules\n -remoteRead.url=http://localhost:8428 \\ # PromQL compatible datasource to restore alerts state from\n -external.label=cluster=east-1 \\ # External label to be applied for each rule\n -external.label=replica=a \\ # Multiple external labels may be set\n -evaluationInterval=3s # Default evaluation interval if not specified in rules group\n```\n\nIf you run multiple `vmalert` services for the same datastore or AlertManager - do not forget\nto specify different `external.label` flags in order to define which `vmalert` generated rules or alerts. \n\nConfiguration for [recording](https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/) \nand [alerting](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/) rules is very \nsimilar to Prometheus rules and configured using YAML. Configuration examples may be found \nin [testdata](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/app/vmalert/config/testdata) folder.\nEvery `rule` belongs to `group` and every configuration file may contain arbitrary number of groups:\n```yaml\ngroups:\n [ - <rule_group> ]\n```\n\n#### Groups\n\nEach group has following attributes:\n```yaml\n# The name of the group. Must be unique within a file.\nname: <string>\n\n# How often rules in the group are evaluated.\n[ interval: <duration> | default = global.evaluation_interval ]\n\n# How many rules execute at once. Increasing concurrency may speed\n# up round execution speed. \n[ concurrency: <integer> | default = 1 ]\n\n# Optional type for expressions inside the rules. Supported values: \"graphite\" and \"prometheus\".\n# By default \"prometheus\" rule type is used.\n[ type: <string> ]\n\nrules:\n [ - <rule> ... ]\n```\n\n#### Rules\n\nThere are two types of Rules:\n* [alerting](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/) - \nAlerting rules allows to define alert conditions via [MetricsQL](https://victoriametrics.github.io/MetricsQL.html)\nand to send notifications about firing alerts to [Alertmanager](https://github.com/prometheus/alertmanager).\n* [recording](https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/) - \nRecording rules allow you to precompute frequently needed or computationally expensive expressions \nand save their result as a new set of time series.\n\n`vmalert` forbids to define duplicates - rules with the same combination of name, expression and labels\nwithin one group. \n\n##### Alerting rules\n\nThe syntax for alerting rule is following:\n```yaml\n# The name of the alert. Must be a valid metric name.\nalert: <string>\n\n# Optional type for the rule. Supported values: \"graphite\", \"prometheus\".\n# By default \"prometheus\" rule type is used.\n[ type: <string> ]\n\n# The expression to evaluate. The expression language depends on the type value.\n# By default MetricsQL expression is used. If type=\"graphite\", then the expression\n# must contain valid Graphite expression.\nexpr: <string>\n\n# Alerts are considered firing once they have been returned for this long.\n# Alerts which have not yet fired for long enough are considered pending.\n[ for: <duration> | default = 0s ]\n\n# Labels to add or overwrite for each alert.\nlabels:\n [ <labelname>: <tmpl_string> ]\n\n# Annotations to add to each alert.\nannotations:\n [ <labelname>: <tmpl_string> ]\n``` \n\n##### Recording rules\n\nThe syntax for recording rules is following:\n```yaml\n# The name of the time series to output to. Must be a valid metric name.\nrecord: <string>\n\n# Optional type for the rule. Supported values: \"graphite\", \"prometheus\".\n# By default \"prometheus\" rule type is used.\n[ type: <string> ]\n\n# The expression to evaluate. The expression language depends on the type value.\n# By default MetricsQL expression is used. If type=\"graphite\", then the expression\n# must contain valid Graphite expression.\nexpr: <string>\n\n# Labels to add or overwrite before storing the result.\nlabels:\n [ <labelname>: <labelvalue> ]\n```\n\nFor recording rules to work `-remoteWrite.url` must specified.\n\n\n#### Alerts state on restarts\n\n`vmalert` has no local storage, so alerts state is stored in the process memory. Hence, after reloading of `vmalert` \nthe process alerts state will be lost. To avoid this situation, `vmalert` should be configured via the following flags:\n* `-remoteWrite.url` - URL to VictoriaMetrics (Single) or VMInsert (Cluster). `vmalert` will persist alerts state \ninto the configured address in the form of time series named `ALERTS` and `ALERTS_FOR_STATE` via remote-write protocol. \nThese are regular time series and may be queried from VM just as any other time series. \nThe state stored to the configured address on every rule evaluation.\n* `-remoteRead.url` - URL to VictoriaMetrics (Single) or VMSelect (Cluster). `vmalert` will try to restore alerts state \nfrom configured address by querying time series with name `ALERTS_FOR_STATE`.\n\nBoth flags are required for the proper state restoring. Restore process may fail if time series are missing\nin configured `-remoteRead.url`, weren't updated in the last `1h` or received state doesn't match current `vmalert` \nrules configuration.\n\n\n#### WEB\n\n`vmalert` runs a web-server (`-httpListenAddr`) for serving metrics and alerts endpoints:\n* `http://<vmalert-addr>/api/v1/groups` - list of all loaded groups and rules;\n* `http://<vmalert-addr>/api/v1/alerts` - list of all active alerts;\n* `http://<vmalert-addr>/api/v1/<groupName>/<alertID>/status\" ` - get alert status by ID.\nUsed as alert source in AlertManager.\n* `http://<vmalert-addr>/metrics` - application metrics.\n* `http://<vmalert-addr>/-/reload` - hot configuration reload.\n\n\n### Graphite\n\nvmalert sends requests to `<-datasource.url>/render?format=json` during evaluation of alerting and recording rules\nif the corresponding group or rule contains `type: \"graphite\"` config option. It is expected that the `<-datasource.url>/render`\nimplements [Graphite Render API](https://graphite.readthedocs.io/en/stable/render_api.html) for `format=json`.\nWhen using vmalert with both `graphite` and `prometheus` rules configured against cluster version of VM do not forget\nto set `-datasource.appendTypePrefix` flag to `true`, so vmalert can adjust URL prefix automatically based on query type.\n\n\n### Configuration\n\nThe shortlist of configuration flags is the following:\n```\n -datasource.appendTypePrefix\n Whether to add type prefix to -datasource.url based on the query type. Set to true if sending different query types to VMSelect URL.\n -datasource.basicAuth.password string\n \tOptional basic auth password for -datasource.url\n -datasource.basicAuth.username string\n \tOptional basic auth username for -datasource.url\n -datasource.lookback duration\n \tLookback defines how far to look into past when evaluating queries. For example, if datasource.lookback=5m then param \"time\" with value now()-5m will be added to every query.\n -datasource.maxIdleConnections int\n \tDefines the number of idle (keep-alive connections) to configured datasource.Consider to set this value equal to the value: groups_total * group.concurrency. Too low value may result into high number of sockets in TIME_WAIT state. (default 100)\n -datasource.tlsCAFile string\n \tOptional path to TLS CA file to use for verifying connections to -datasource.url. By default system CA is used\n -datasource.tlsCertFile string\n \tOptional path to client-side TLS certificate file to use when connecting to -datasource.url\n -datasource.tlsInsecureSkipVerify\n \tWhether to skip tls verification when connecting to -datasource.url\n -datasource.tlsKeyFile string\n \tOptional path to client-side TLS certificate key to use when connecting to -datasource.url\n -datasource.tlsServerName string\n \tOptional TLS server name to use for connections to -datasource.url. By default the server name from -datasource.url is used\n -datasource.url string\n \tVictoria Metrics or VMSelect url. Required parameter. E.g. http://127.0.0.1:8428\n -dryRun -rule\n \tWhether to check only config files without running vmalert. The rules file are validated. The -rule flag must be specified.\n -enableTCP6\n \tWhether to enable IPv6 for listening and dialing. By default only IPv4 TCP is used\n -envflag.enable\n \tWhether to enable reading flags from environment variables additionally to command line. Command line flag values have priority over values from environment vars. Flags are read only from command line if this flag isn't set\n -envflag.prefix string\n \tPrefix for environment variables if -envflag.enable is set\n -evaluationInterval duration\n \tHow often to evaluate the rules (default 1m0s)\n -external.alert.source string\n \tExternal Alert Source allows to override the Source link for alerts sent to AlertManager for cases where you want to build a custom link to Grafana, Prometheus or any other service.\n \teg. 'explore?orgId=1&left=[\\\"now-1h\\\",\\\"now\\\",\\\"VictoriaMetrics\\\",{\\\"expr\\\": \\\"{{$expr|quotesEscape|crlfEscape|pathEscape}}\\\"},{\\\"mode\\\":\\\"Metrics\\\"},{\\\"ui\\\":[true,true,true,\\\"none\\\"]}]'.If empty '/api/v1/:groupID/alertID/status' is used\n -external.label array\n \tOptional label in the form 'name=value' to add to all generated recording rules and alerts. Pass multiple -label flags in order to add multiple label sets.\n \tSupports array of values separated by comma or specified via multiple flags.\n -external.url string\n \tExternal URL is used as alert's source for sent alerts to the notifier\n -http.connTimeout duration\n \tIncoming http connections are closed after the configured timeout. This may help spreading incoming load among a cluster of services behind load balancer. Note that the real timeout may be bigger by up to 10% as a protection from Thundering herd problem (default 2m0s)\n -http.disableResponseCompression\n \tDisable compression of HTTP responses for saving CPU resources. By default compression is enabled to save network bandwidth\n -http.idleConnTimeout duration\n \tTimeout for incoming idle http connections (default 1m0s)\n -http.maxGracefulShutdownDuration duration\n \tThe maximum duration for graceful shutdown of HTTP server. Highly loaded server may require increased value for graceful shutdown (default 7s)\n -http.pathPrefix string\n \tAn optional prefix to add to all the paths handled by http server. For example, if '-http.pathPrefix=/foo/bar' is set, then all the http requests will be handled on '/foo/bar/*' paths. This may be useful for proxied requests. See https://www.robustperception.io/using-external-urls-and-proxies-with-prometheus\n -http.shutdownDelay duration\n \tOptional delay before http server shutdown. During this dealy the servier returns non-OK responses from /health page, so load balancers can route new requests to other servers\n -httpAuth.password string\n \tPassword for HTTP Basic Auth. The authentication is disabled if -httpAuth.username is empty\n -httpAuth.username string\n \tUsername for HTTP Basic Auth. The authentication is disabled if empty. See also -httpAuth.password\n -httpListenAddr string\n \tAddress to listen for http connections (default \":8880\")\n -loggerDisableTimestamps\n \tWhether to disable writing timestamps in logs\n -loggerErrorsPerSecondLimit int\n \tPer-second limit on the number of ERROR messages. If more than the given number of errors are emitted per second, then the remaining errors are suppressed. Zero value disables the rate limit\n -loggerFormat string\n \tFormat for logs. Possible values: default, json (default \"default\")\n -loggerLevel string\n \tMinimum level of errors to log. Possible values: INFO, WARN, ERROR, FATAL, PANIC (default \"INFO\")\n -loggerOutput string\n \tOutput for the logs. Supported values: stderr, stdout (default \"stderr\")\n -loggerWarnsPerSecondLimit int\n \tPer-second limit on the number of WARN messages. If more than the given number of warns are emitted per second, then the remaining warns are suppressed. Zero value disables the rate limit\n -memory.allowedBytes value\n \tAllowed size of system memory VictoriaMetrics caches may occupy. This option overrides -memory.allowedPercent if set to non-zero value. Too low value may increase cache miss rate, which usually results in higher CPU and disk IO usage. Too high value may evict too much data from OS page cache, which will result in higher disk IO usage\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 0)\n -memory.allowedPercent float\n \tAllowed percent of system memory VictoriaMetrics caches may occupy. See also -memory.allowedBytes. Too low value may increase cache miss rate, which usually results in higher CPU and disk IO usage. Too high value may evict too much data from OS page cache, which will result in higher disk IO usage (default 60)\n -metricsAuthKey string\n \tAuth key for /metrics. It overrides httpAuth settings\n -notifier.basicAuth.password array\n \tOptional basic auth password for -notifier.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -notifier.basicAuth.username array\n \tOptional basic auth username for -notifier.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -notifier.tlsCAFile array\n \tOptional path to TLS CA file to use for verifying connections to -notifier.url. By default system CA is used\n \tSupports array of values separated by comma or specified via multiple flags.\n -notifier.tlsCertFile array\n \tOptional path to client-side TLS certificate file to use when connecting to -notifier.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -notifier.tlsInsecureSkipVerify array\n \tWhether to skip tls verification when connecting to -notifier.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -notifier.tlsKeyFile array\n \tOptional path to client-side TLS certificate key to use when connecting to -notifier.url\n \tSupports array of values separated by comma or specified via multiple flags.\n -notifier.tlsServerName array\n \tOptional TLS server name to use for connections to -notifier.url. By default the server name from -notifier.url is used\n \tSupports array of values separated by comma or specified via multiple flags.\n -notifier.url array\n \tPrometheus alertmanager URL. Required parameter. e.g. http://127.0.0.1:9093\n \tSupports array of values separated by comma or specified via multiple flags.\n -pprofAuthKey string\n \tAuth key for /debug/pprof. It overrides httpAuth settings\n -remoteRead.basicAuth.password string\n \tOptional basic auth password for -remoteRead.url\n -remoteRead.basicAuth.username string\n \tOptional basic auth username for -remoteRead.url\n -remoteRead.lookback duration\n \tLookback defines how far to look into past for alerts timeseries. For example, if lookback=1h then range from now() to now()-1h will be scanned. (default 1h0m0s)\n -remoteRead.tlsCAFile string\n \tOptional path to TLS CA file to use for verifying connections to -remoteRead.url. By default system CA is used\n -remoteRead.tlsCertFile string\n \tOptional path to client-side TLS certificate file to use when connecting to -remoteRead.url\n -remoteRead.tlsInsecureSkipVerify\n \tWhether to skip tls verification when connecting to -remoteRead.url\n -remoteRead.tlsKeyFile string\n \tOptional path to client-side TLS certificate key to use when connecting to -remoteRead.url\n -remoteRead.tlsServerName string\n \tOptional TLS server name to use for connections to -remoteRead.url. By default the server name from -remoteRead.url is used\n -remoteRead.url vmalert\n \tOptional URL to Victoria Metrics or VMSelect that will be used to restore alerts state. This configuration makes sense only if vmalert was configured with `remoteWrite.url` before and has been successfully persisted its state. E.g. http://127.0.0.1:8428\n -remoteWrite.basicAuth.password string\n \tOptional basic auth password for -remoteWrite.url\n -remoteWrite.basicAuth.username string\n \tOptional basic auth username for -remoteWrite.url\n -remoteWrite.concurrency int\n \tDefines number of writers for concurrent writing into remote querier (default 1)\n -remoteWrite.flushInterval duration\n \tDefines interval of flushes to remote write endpoint (default 5s)\n -remoteWrite.maxBatchSize int\n \tDefines defines max number of timeseries to be flushed at once (default 1000)\n -remoteWrite.maxQueueSize int\n \tDefines the max number of pending datapoints to remote write endpoint (default 100000)\n -remoteWrite.tlsCAFile string\n \tOptional path to TLS CA file to use for verifying connections to -remoteWrite.url. By default system CA is used\n -remoteWrite.tlsCertFile string\n \tOptional path to client-side TLS certificate file to use when connecting to -remoteWrite.url\n -remoteWrite.tlsInsecureSkipVerify\n \tWhether to skip tls verification when connecting to -remoteWrite.url\n -remoteWrite.tlsKeyFile string\n \tOptional path to client-side TLS certificate key to use when connecting to -remoteWrite.url\n -remoteWrite.tlsServerName string\n \tOptional TLS server name to use for connections to -remoteWrite.url. By default the server name from -remoteWrite.url is used\n -remoteWrite.url string\n \tOptional URL to Victoria Metrics or VMInsert where to persist alerts state and recording rules results in form of timeseries. E.g. http://127.0.0.1:8428\n -rule array\n \tPath to the file with alert rules. \n \tSupports patterns. Flag can be specified multiple times. \n \tExamples:\n \t -rule=\"/path/to/file\". Path to a single file with alerting rules\n \t -rule=\"dir/*.yaml\" -rule=\"/*.yaml\". Relative path to all .yaml files in \"dir\" folder, \n \tabsolute path to all .yaml files in root.\n \tRule files may contain %{ENV_VAR} placeholders, which are substituted by the corresponding env vars.\n \tSupports array of values separated by comma or specified via multiple flags.\n -rule.validateExpressions\n \tWhether to validate rules expressions via MetricsQL engine (default true)\n -rule.validateTemplates\n \tWhether to validate annotation and label templates (default true)\n -tls\n \tWhether to enable TLS (aka HTTPS) for incoming requests. -tlsCertFile and -tlsKeyFile must be set if -tls is set\n -tlsCertFile string\n \tPath to file with TLS certificate. Used only if -tls is set. Prefer ECDSA certs instead of RSA certs, since RSA certs are slow\n -tlsKeyFile string\n \tPath to file with TLS key. Used only if -tls is set\n -version\n \tShow VictoriaMetrics version\n```\n\nPass `-help` to `vmalert` in order to see the full list of supported \ncommand-line flags with their descriptions.\n\nTo reload configuration without `vmalert` restart send SIGHUP signal\nor send GET request to `/-/reload` endpoint.\n\n### Contributing\n\n`vmalert` is mostly designed and built by VictoriaMetrics community.\nFeel free to share your experience and ideas for improving this \nsoftware. Please keep simplicity as the main priority.\n\n### How to build from sources\n\nIt is recommended using \n[binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) \n- `vmalert` is located in `vmutils-*` archives there.\n\n\n#### Development build\n\n1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.13.\n2. Run `make vmalert` from the root folder of the repository.\n It builds `vmalert` binary and puts it into the `bin` folder.\n\n#### Production build\n\n1. [Install docker](https://docs.docker.com/install/).\n2. Run `make vmalert-prod` from the root folder of the repository.\n It builds `vmalert-prod` binary and puts it into the `bin` folder.\n\n\n#### ARM build\n\nARM build may run on Raspberry Pi or on [energy-efficient ARM servers](https://blog.cloudflare.com/arm-takes-wing/).\n\n#### Development ARM build\n\n1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.13.\n2. Run `make vmalert-arm` or `make vmalert-arm64` from the root folder of the repository.\n It builds `vmalert-arm` or `vmalert-arm64` binary respectively and puts it into the `bin` folder.\n\n#### Production ARM build\n\n1. [Install docker](https://docs.docker.com/install/).\n2. Run `make vmalert-arm-prod` or `make vmalert-arm64-prod` from the root folder of the repository.\n It builds `vmalert-arm-prod` or `vmalert-arm64-prod` binary respectively and puts it into the `bin` folder.\n","dir":"/","name":"vmalert.md","path":"vmalert.md","url":"/vmalert.html"},{"layout":"default","title":"vmauth","content":"## vmauth\n\n`vmauth` is a simple auth proxy and router for [VictoriaMetrics](https://github.com/VictoriaMetrics/VictoriaMetrics).\nIt reads username and password from [Basic Auth headers](https://en.wikipedia.org/wiki/Basic_access_authentication),\nmatches them against configs pointed by `-auth.config` command-line flag and proxies incoming HTTP requests to the configured per-user `url_prefix` on successful match.\n\n\n## Quick start\n\nJust download `vmutils-*` archive from [releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases), unpack it\nand pass the following flag to `vmauth` binary in order to start authorizing and routing requests:\n\n```\n/path/to/vmauth -auth.config=/path/to/auth/config.yml\n```\n\nAfter that `vmauth` starts accepting HTTP requests on port `8427` and routing them according to the provided [-auth.config](#auth-config).\nThe port can be modified via `-httpListenAddr` command-line flag.\n\nThe auth config can be reloaded by passing `SIGHUP` signal to `vmauth`.\n\nDocker images for `vmauth` are available [here](https://hub.docker.com/r/victoriametrics/vmauth/tags).\n\nPass `-help` to `vmauth` in order to see all the supported command-line flags with their descriptions.\n\nFeel free [contacting us](mailto:info@victoriametrics.com) if you need customized auth proxy for VictoriaMetrics with the support of LDAP, SSO, RBAC, SAML, accounting, limits, etc.\n\n\n## Auth config\n\nAuth config is represented in the following simple `yml` format:\n\n```yml\n\n# Arbitrary number of usernames may be put here.\n# Usernames must be unique.\n\nusers:\n\n # The user for querying local single-node VictoriaMetrics.\n # All the requests to http://vmauth:8427 with the given Basic Auth (username:password)\n # will be routed to http://localhost:8428 .\n # For example, http://vmauth:8427/api/v1/query is routed to http://localhost:8428/api/v1/query\n- username: \"local-single-node\"\n password: \"***\"\n url_prefix: \"http://localhost:8428\"\n\n # The user for querying account 123 in VictoriaMetrics cluster\n # See https://victoriametrics.github.io/Cluster-VictoriaMetrics.html#url-format\n # All the requests to http://vmauth:8427 with the given Basic Auth (username:password)\n # will be routed to http://vmselect:8481/select/123/prometheus .\n # For example, http://vmauth:8427/api/v1/query is routed to http://vmselect:8481/select/123/prometheus/api/v1/select\n- username: \"cluster-select-account-123\"\n password: \"***\"\n url_prefix: \"http://vmselect:8481/select/123/prometheus\"\n\n # The user for inserting Prometheus data into VictoriaMetrics cluster under account 42\n # See https://victoriametrics.github.io/Cluster-VictoriaMetrics.html#url-format\n # All the requests to http://vmauth:8427 with the given Basic Auth (username:password)\n # will be routed to http://vminsert:8480/insert/42/prometheus .\n # For example, http://vmauth:8427/api/v1/write is routed to http://vminsert:8480/insert/42/prometheus/api/v1/write\n- username: \"cluster-insert-account-42\"\n password: \"***\"\n url_prefix: \"http://vminsert:8480/insert/42/prometheus\"\n\n\n # A single user for querying and inserting data:\n # - Requests to http://vmauth:8427/api/v1/query or http://vmauth:8427/api/v1/query_range\n # are routed to http://vmselect:8481/select/42/prometheus.\n # For example, http://vmauth:8427/api/v1/query is routed to http://vmselect:8480/select/42/prometheus/api/v1/query\n # - Requests to http://vmauth:8427/api/v1/write are routed to http://vminsert:8480/insert/42/prometheus/api/v1/write\n- username: \"foobar\"\n url_map:\n - src_paths: [\"/api/v1/query\", \"/api/v1/query_range\"]\n url_prefix: \"http://vmselect:8481/select/42/prometheus\"\n - src_paths: [\"/api/v1/write\"]\n url_prefix: \"http://vminsert:8480/insert/42/prometheus\"\n```\n\nThe config may contain `%{ENV_VAR}` placeholders, which are substituted by the corresponding `ENV_VAR` environment variable values.\nThis may be useful for passing secrets to the config.\n\n\n## Security\n\nDo not transfer Basic Auth headers in plaintext over untrusted networks. Enable https. This can be done by passing the following `-tls*` command-line flags to `vmauth`:\n\n```\n -tls\n \tWhether to enable TLS (aka HTTPS) for incoming requests. -tlsCertFile and -tlsKeyFile must be set if -tls is set\n -tlsCertFile string\n \tPath to file with TLS certificate. Used only if -tls is set. Prefer ECDSA certs instead of RSA certs, since RSA certs are slow\n -tlsKeyFile string\n \tPath to file with TLS key. Used only if -tls is set\n```\n\nAlternatively, [https termination proxy](https://en.wikipedia.org/wiki/TLS_termination_proxy) may be put in front of `vmauth`.\n\n\n## Monitoring\n\n`vmauth` exports various metrics in Prometheus exposition format at `http://vmauth-host:8427/metrics` page. It is recommended setting up regular scraping of this page\neither via [vmagent](https://victoriametrics.github.io/vmagent.html) or via Prometheus, so the exported metrics could be analyzed later.\n\n\n## How to build from sources\n\nIt is recommended using [binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) - `vmauth` is located in `vmutils-*` archives there.\n\n\n### Development build\n\n1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.13.\n2. Run `make vmauth` from the root folder of the repository.\n It builds `vmauth` binary and puts it into the `bin` folder.\n\n### Production build\n\n1. [Install docker](https://docs.docker.com/install/).\n2. Run `make vmauth-prod` from the root folder of the repository.\n It builds `vmauth-prod` binary and puts it into the `bin` folder.\n\n### Building docker images\n\nRun `make package-vmauth`. It builds `victoriametrics/vmauth:<PKG_TAG>` docker image locally.\n`<PKG_TAG>` is auto-generated image tag, which depends on source code in the repository.\nThe `<PKG_TAG>` may be manually set via `PKG_TAG=foobar make package-vmauth`.\n\nThe base docker image is [alpine](https://hub.docker.com/_/alpine) but it is possible to use any other base image\nby setting it via `<ROOT_IMAGE>` environment variable. For example, the following command builds the image on top of [scratch](https://hub.docker.com/_/scratch) image:\n\n```bash\nROOT_IMAGE=scratch make package-vmauth\n```\n\n\n## Profiling\n\n`vmauth` provides handlers for collecting the following [Go profiles](https://blog.golang.org/profiling-go-programs):\n\n* Memory profile. It can be collected with the following command:\n\n```bash\ncurl -s http://<vmauth-host>:8427/debug/pprof/heap > mem.pprof\n```\n\n* CPU profile. It can be collected with the following command:\n\n```bash\ncurl -s http://<vmauth-host>:8427/debug/pprof/profile > cpu.pprof\n```\n\nThe command for collecting CPU profile waits for 30 seconds before returning.\n\nThe collected profiles may be analyzed with [go tool pprof](https://github.com/google/pprof).\n\n\n## Advanced usage\n\nPass `-help` command-line arg to `vmauth` in order to see all the configuration options:\n\n```\n./vmauth -help\n\nvmauth authenticates and authorizes incoming requests and proxies them to VictoriaMetrics.\n\nSee the docs at https://victoriametrics.github.io/vmauth.html .\n\n -auth.config string\n \tPath to auth config. See https://victoriametrics.github.io/vmauth.html for details on the format of this auth config\n -enableTCP6\n \tWhether to enable IPv6 for listening and dialing. By default only IPv4 TCP is used\n -envflag.enable\n \tWhether to enable reading flags from environment variables additionally to command line. Command line flag values have priority over values from environment vars. Flags are read only from command line if this flag isn't set\n -envflag.prefix string\n \tPrefix for environment variables if -envflag.enable is set\n -http.connTimeout duration\n \tIncoming http connections are closed after the configured timeout. This may help spreading incoming load among a cluster of services behind load balancer. Note that the real timeout may be bigger by up to 10% as a protection from Thundering herd problem (default 2m0s)\n -http.disableResponseCompression\n \tDisable compression of HTTP responses for saving CPU resources. By default compression is enabled to save network bandwidth\n -http.idleConnTimeout duration\n \tTimeout for incoming idle http connections (default 1m0s)\n -http.maxGracefulShutdownDuration duration\n \tThe maximum duration for graceful shutdown of HTTP server. Highly loaded server may require increased value for graceful shutdown (default 7s)\n -http.pathPrefix string\n \tAn optional prefix to add to all the paths handled by http server. For example, if '-http.pathPrefix=/foo/bar' is set, then all the http requests will be handled on '/foo/bar/*' paths. This may be useful for proxied requests. See https://www.robustperception.io/using-external-urls-and-proxies-with-prometheus\n -http.shutdownDelay duration\n \tOptional delay before http server shutdown. During this dealy the servier returns non-OK responses from /health page, so load balancers can route new requests to other servers\n -httpAuth.password string\n \tPassword for HTTP Basic Auth. The authentication is disabled if -httpAuth.username is empty\n -httpAuth.username string\n \tUsername for HTTP Basic Auth. The authentication is disabled if empty. See also -httpAuth.password\n -httpListenAddr string\n \tTCP address to listen for http connections (default \":8427\")\n -loggerErrorsPerSecondLimit int\n \tPer-second limit on the number of ERROR messages. If more than the given number of errors are emitted per second, then the remaining errors are suppressed. Zero value disables the rate limit (default 10)\n -loggerFormat string\n \tFormat for logs. Possible values: default, json (default \"default\")\n -loggerLevel string\n \tMinimum level of errors to log. Possible values: INFO, WARN, ERROR, FATAL, PANIC (default \"INFO\")\n -loggerOutput string\n \tOutput for the logs. Supported values: stderr, stdout (default \"stderr\")\n -memory.allowedBytes value\n \tAllowed size of system memory VictoriaMetrics caches may occupy. This option overrides -memory.allowedPercent if set to non-zero value. Too low value may increase cache miss rate, which usually results in higher CPU and disk IO usage. Too high value may evict too much data from OS page cache, which will result in higher disk IO usage\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 0)\n -memory.allowedPercent float\n \tAllowed percent of system memory VictoriaMetrics caches may occupy. See also -memory.allowedBytes. Too low value may increase cache miss rate, which usually results in higher CPU and disk IO usage. Too high value may evict too much data from OS page cache, which will result in higher disk IO usage (default 60)\n -metricsAuthKey string\n \tAuth key for /metrics. It overrides httpAuth settings\n -pprofAuthKey string\n \tAuth key for /debug/pprof. It overrides httpAuth settings\n -tls\n \tWhether to enable TLS (aka HTTPS) for incoming requests. -tlsCertFile and -tlsKeyFile must be set if -tls is set\n -tlsCertFile string\n \tPath to file with TLS certificate. Used only if -tls is set. Prefer ECDSA certs instead of RSA certs, since RSA certs are slow\n -tlsKeyFile string\n \tPath to file with TLS key. Used only if -tls is set\n -version\n \tShow VictoriaMetrics version\n```\n","dir":"/","name":"vmauth.md","path":"vmauth.md","url":"/vmauth.html"},{"layout":"default","title":"vmbackup","content":"## vmbackup\n\n`vmbackup` creates VictoriaMetrics data backups from [instant snapshots](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-work-with-snapshots).\n\nSupported storage systems for backups:\n\n* [GCS](https://cloud.google.com/storage/). Example: `gcs://<bucket>/<path/to/backup>`\n* [S3](https://aws.amazon.com/s3/). Example: `s3://<bucket>/<path/to/backup>`\n* Any S3-compatible storage such as [MinIO](https://github.com/minio/minio), [Ceph](https://docs.ceph.com/docs/mimic/radosgw/s3/) or [Swift](https://www.swiftstack.com/docs/admin/middleware/s3_middleware.html). See [these docs](#advanced-usage) for details.\n* Local filesystem. Example: `fs://</absolute/path/to/backup>`\n\n`vmbackup` supports incremental and full backups. Incremental backups created automatically if the destination path already contains data from the previous backup.\nFull backups can be sped up with `-origin` pointing to already existing backup on the same remote storage. In this case `vmbackup` makes server-side copy for the shared\ndata between the existing backup and new backup. It saves time and costs on data transfer.\n\nBackup process can be interrupted at any time. It is automatically resumed from the interruption point when restarting `vmbackup` with the same args.\n\nBacked up data can be restored with [vmrestore](https://victoriametrics.github.io/vmrestore.html).\n\nSee [this article](https://medium.com/@valyala/speeding-up-backups-for-big-time-series-databases-533c1a927883) for more details.\n\nSee also [vmbackupmanager](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/466) tool built on top of `vmbackup`. This tool simplifies\ncreation of hourly, daily, weekly and monthly backups.\n\n\n## Use cases\n\n### Regular backups\n\nRegular backup can be performed with the following command:\n\n```\nvmbackup -storageDataPath=</path/to/victoria-metrics-data> -snapshotName=<local-snapshot> -dst=gcs://<bucket>/<path/to/new/backup>\n```\n\n* `</path/to/victoria-metrics-data>` - path to VictoriaMetrics data pointed by `-storageDataPath` command-line flag in single-node VictoriaMetrics or in cluster `vmstorage`.\n There is no need to stop VictoriaMetrics for creating backups, since they are performed from immutable [instant snapshots](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-work-with-snapshots).\n* `<local-snapshot>` is the snapshot to back up. See [how to create instant snapshots](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-work-with-snapshots).\n* `<bucket>` is an already existing name for [GCS bucket](https://cloud.google.com/storage/docs/creating-buckets).\n* `<path/to/new/backup>` is the destination path where new backup will be placed.\n\n\n### Regular backups with server-side copy from existing backup\n\nIf the destination GCS bucket already contains the previous backup at `-origin` path, then new backup can be sped up\nwith the following command:\n\n```\nvmbackup -storageDataPath=</path/to/victoria-metrics-data> -snapshotName=<local-snapshot> -dst=gcs://<bucket>/<path/to/new/backup> -origin=gcs://<bucket>/<path/to/existing/backup>\n```\n\nIt saves time and network bandwidth costs by performing server-side copy for the shared data from the `-origin` to `-dst`.\n\n\n### Incremental backups\n\nIncremental backups performed if `-dst` points to an already existing backup. In this case only new data uploaded to remote storage.\nIt saves time and network bandwidth costs when working with big backups:\n\n```\nvmbackup -storageDataPath=</path/to/victoria-metrics-data> -snapshotName=<local-snapshot> -dst=gcs://<bucket>/<path/to/existing/backup>\n```\n\n\n### Smart backups\n\nSmart backups mean storing full daily backups into `YYYYMMDD` folders and creating incremental hourly backup into `latest` folder:\n\n* Run the following command every hour:\n\n```\nvmbackup -snapshotName=<latest-snapshot> -dst=gcs://<bucket>/latest\n```\n\nWhere `<latest-snapshot>` is the latest [snapshot](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-work-with-snapshots).\nThe command will upload only changed data to `gcs://<bucket>/latest`.\n\n* Run the following command once a day:\n\n```\nvmbackup -snapshotName=<daily-snapshot> -dst=gcs://<bucket>/<YYYYMMDD> -origin=gcs://<bucket>/latest\n```\n\nWhere `<daily-snapshot>` is the snapshot for the last day `<YYYYMMDD>`.\n\n\nThis apporach saves network bandwidth costs on hourly backups (since they are incremental) and allows recovering data from either the last hour (`latest` backup)\nor from any day (`YYYYMMDD` backups). Note that hourly backup shouldn't run when creating daily backup.\n\nDo not forget removing old snapshots and backups when they are no longer needed for saving storage costs.\n\nSee also [vmbackupmanager tool](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/466) for automating smart backups.\n\n\n## How does it work?\n\nThe backup algorithm is the following:\n\n1. Collect information about files in the `-snapshotName`, in the `-dst` and in the `-origin`.\n2. Determine files in `-dst`, which are missing in `-snapshotName`, and delete them. These are usually small files, which are already merged into bigger files in the snapshot.\n3. Determine files from `-snapshotName`, which are missing in `-dst`. These are usually small new files and bigger merged files.\n4. Determine files from step 3, which exist in the `-origin`, and perform server-side copy of these files from `-origin` to `-dst`.\n These are usually the biggest and the oldest files, which are shared between backups.\n5. Upload the remaining files from step 3 from `-snapshotName` to `-dst`.\n\nThe algorithm splits source files into 100 MB chunks in the backup. Each chunk stored as a separate file in the backup.\nSuch splitting minimizes the amounts of data to re-transfer after temporary errors.\n\n`vmbackup` relies on [instant snapshot](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282) properties:\n\n- All the files in the snapshot are immutable.\n- Old files periodically merged into new files.\n- Smaller files have higher probability to be merged.\n- Consecutive snapshots share many identical files.\n\nThese properties allow performing fast and cheap incremental backups and server-side copying from `-origin` paths.\nSee [this article](https://medium.com/@valyala/speeding-up-backups-for-big-time-series-databases-533c1a927883) for more details.\n`vmbackup` can work improperly or slowly when these properties are violated.\n\n\n## Troubleshooting\n\n* If the backup is slow, then try setting higher value for `-concurrency` flag. This will increase the number of concurrent workers that upload data to backup storage.\n* If `vmbackup` eats all the network bandwidth, then set `-maxBytesPerSecond` to the desired value.\n* If `vmbackup` has been interrupted due to temporary error, then just restart it with the same args. It will resume the backup process.\n* Backups created from [single-node VictoriaMetrics](https://victoriametrics.github.io/Single-server-VictoriaMetrics.html) cannot be restored\n at [cluster VictoriaMetrics](https://victoriametrics.github.io/Cluster-VictoriaMetrics.html) and vice versa.\n\n\n## Advanced usage\n\n\n* Obtaining credentials from a file.\n\n Add flag `-credsFilePath=/etc/credentials` with the following content:\n\n for s3 (aws, minio or other s3 compatible storages):\n ```bash\n [default]\n aws_access_key_id=theaccesskey\n aws_secret_access_key=thesecretaccesskeyvalue\n ```\n\n for gce cloud storage:\n ```json\n {\n \"type\": \"service_account\",\n \"project_id\": \"project-id\",\n \"private_key_id\": \"key-id\",\n \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nprivate-key\\n-----END PRIVATE KEY-----\\n\",\n \"client_email\": \"service-account-email\",\n \"client_id\": \"client-id\",\n \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n \"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/service-account-email\"\n }\n ```\n\n* Usage with s3 custom url endpoint. It is possible to use `vmbackup` with s3 compatible storages like minio, cloudian, etc.\n You have to add a custom url endpoint via flag:\n```\n # for minio\n -customS3Endpoint=http://localhost:9000\n\n # for aws gov region\n -customS3Endpoint=https://s3-fips.us-gov-west-1.amazonaws.com\n```\n\n* Run `vmbackup -help` in order to see all the available options:\n\n```\n -concurrency int\n \tThe number of concurrent workers. Higher concurrency may reduce backup duration (default 10)\n -configFilePath string\n \tPath to file with S3 configs. Configs are loaded from default location if not set.\n \tSee https://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html\n -configProfile string\n \tProfile name for S3 configs. If no set, the value of the environment variable will be loaded (AWS_PROFILE or AWS_DEFAULT_PROFILE), or if both not set, DefaultSharedConfigProfile is used\n -credsFilePath string\n \tPath to file with GCS or S3 credentials. Credentials are loaded from default locations if not set.\n \tSee https://cloud.google.com/iam/docs/creating-managing-service-account-keys and https://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html\n -customS3Endpoint string\n \tCustom S3 endpoint for use with S3-compatible storages (e.g. MinIO). S3 is used if not set\n -dst string\n \tWhere to put the backup on the remote storage. Example: gcs://bucket/path/to/backup/dir, s3://bucket/path/to/backup/dir or fs:///path/to/local/backup/dir\n \t-dst can point to the previous backup. In this case incremental backup is performed, i.e. only changed data is uploaded\n -envflag.enable\n \tWhether to enable reading flags from environment variables additionally to command line. Command line flag values have priority over values from environment vars. Flags are read only from command line if this flag isn't set\n -envflag.prefix string\n \tPrefix for environment variables if -envflag.enable is set\n -fs.disableMmap\n \tWhether to use pread() instead of mmap() for reading data files. By default mmap() is used for 64-bit arches and pread() is used for 32-bit arches, since they cannot read data files bigger than 2^32 bytes in memory. mmap() is usually faster for reading small data chunks than pread()\n -loggerErrorsPerSecondLimit int\n \tPer-second limit on the number of ERROR messages. If more than the given number of errors are emitted per second, then the remaining errors are suppressed. Zero value disables the rate limit (default 10)\n -loggerFormat string\n \tFormat for logs. Possible values: default, json (default \"default\")\n -loggerLevel string\n \tMinimum level of errors to log. Possible values: INFO, WARN, ERROR, FATAL, PANIC (default \"INFO\")\n -loggerOutput string\n \tOutput for the logs. Supported values: stderr, stdout (default \"stderr\")\n -maxBytesPerSecond value\n \tThe maximum upload speed. There is no limit if it is set to 0\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 0)\n -memory.allowedBytes value\n \tAllowed size of system memory VictoriaMetrics caches may occupy. This option overrides -memory.allowedPercent if set to non-zero value. Too low value may increase cache miss rate, which usually results in higher CPU and disk IO usage. Too high value may evict too much data from OS page cache, which will result in higher disk IO usage\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 0)\n -memory.allowedPercent float\n \tAllowed percent of system memory VictoriaMetrics caches may occupy. See also -memory.allowedBytes. Too low value may increase cache miss rate, which usually results in higher CPU and disk IO usage. Too high value may evict too much data from OS page cache, which will result in higher disk IO usage (default 60)\n -origin string\n \tOptional origin directory on the remote storage with old backup for server-side copying when performing full backup. This speeds up full backups\n -snapshot.createURL string\n \tVictoriaMetrics create snapshot url. When this is given a snapshot will automatically be created during backup. Example: http://victoriametrics:8428/snaphsot/create\n -snapshot.deleteURL string\n \tVictoriaMetrics delete snapshot url. Optional. Will be generated from -snapshot.createURL if not provided. All created snaphosts will be automatically deleted. Example: http://victoriametrics:8428/snaphsot/delete\n -snapshotName string\n \tName for the snapshot to backup. See https://victoriametrics.github.io/Single-server-VictoriaMetrics.html#how-to-work-with-snapshots\n -storageDataPath string\n \tPath to VictoriaMetrics data. Must match -storageDataPath from VictoriaMetrics or vmstorage (default \"victoria-metrics-data\")\n -version\n \tShow VictoriaMetrics version\n```\n\n\n## How to build from sources\n\nIt is recommended using [binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) - see `vmutils-*` archives there.\n\n\n### Development build\n\n1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.13.\n2. Run `make vmbackup` from the root folder of the repository.\n It builds `vmbackup` binary and puts it into the `bin` folder.\n\n### Production build\n\n1. [Install docker](https://docs.docker.com/install/).\n2. Run `make vmbackup-prod` from the root folder of the repository.\n It builds `vmbackup-prod` binary and puts it into the `bin` folder.\n\n### Building docker images\n\nRun `make package-vmbackup`. It builds `victoriametrics/vmbackup:<PKG_TAG>` docker image locally.\n`<PKG_TAG>` is auto-generated image tag, which depends on source code in the repository.\nThe `<PKG_TAG>` may be manually set via `PKG_TAG=foobar make package-vmbackup`.\n\nThe base docker image is [alpine](https://hub.docker.com/_/alpine) but it is possible to use any other base image\nby setting it via `<ROOT_IMAGE>` environment variable. For example, the following command builds the image on top of [scratch](https://hub.docker.com/_/scratch) image:\n\n```bash\nROOT_IMAGE=scratch make package-vmbackup\n```\n","dir":"/","name":"vmbackup.md","path":"vmbackup.md","url":"/vmbackup.html"},{"layout":"default","title":"vmctl","content":"# vmctl\n\nVictoria metrics command-line tool\n\nFeatures:\n- [x] Prometheus: migrate data from Prometheus to VictoriaMetrics using snapshot API\n- [x] Thanos: migrate data from Thanos to VictoriaMetrics\n- [ ] ~~Prometheus: migrate data from Prometheus to VictoriaMetrics by query~~(discarded)\n- [x] InfluxDB: migrate data from InfluxDB to VictoriaMetrics\n- [ ] Storage Management: data re-balancing between nodes \n\n# Table of contents\n\n* [Articles](#articles)\n* [How to build](#how-to-build)\n* [Migrating data from InfluxDB 1.x](#migrating-data-from-influxdb-1x)\n * [Data mapping](#data-mapping)\n * [Configuration](#configuration)\n * [Filtering](#filtering)\n* [Migrating data from InfluxDB 2.x](#migrating-data-from-influxdb-2x) \n* [Migrating data from Prometheus](#migrating-data-from-prometheus)\n * [Data mapping](#data-mapping-1)\n * [Configuration](#configuration-1)\n * [Filtering](#filtering-1)\n* [Migrating data from Thanos](#migrating-data-from-thanos)\n * [Current data](#current-data)\n * [Historical data](#historical-data)\n* [Migrating data from VictoriaMetrics](#migrating-data-from-victoriametrics)\n * [Native protocol](#native-protocol)\n* [Tuning](#tuning)\n * [Influx mode](#influx-mode)\n * [Prometheus mode](#prometheus-mode)\n * [VictoriaMetrics importer](#victoriametrics-importer)\n * [Importer stats](#importer-stats)\n* [Significant figures](#significant-figures)\n* [Adding extra labels](#adding-extra-labels)\n\n\n## Articles\n\n* [How to migrate data from Prometheus](https://medium.com/@romanhavronenko/victoriametrics-how-to-migrate-data-from-prometheus-d44a6728f043)\n* [How to migrate data from Prometheus. Filtering and modifying time series](https://medium.com/@romanhavronenko/victoriametrics-how-to-migrate-data-from-prometheus-filtering-and-modifying-time-series-6d40cea4bf21)\n\n## How to build\n\nIt is recommended using [binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) - `vmctl` is located in `vmutils-*` archives there.\n\n\n### Development build\n\n1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.13.\n2. Run `make vmctl` from the root folder of the repository.\n It builds `vmctl` binary and puts it into the `bin` folder.\n\n### Production build\n\n1. [Install docker](https://docs.docker.com/install/).\n2. Run `make vmctl-prod` from the root folder of the repository.\n It builds `vmctl-prod` binary and puts it into the `bin` folder.\n\n### Building docker images\n\nRun `make package-vmctl`. It builds `victoriametrics/vmctl:<PKG_TAG>` docker image locally.\n`<PKG_TAG>` is auto-generated image tag, which depends on source code in the repository.\nThe `<PKG_TAG>` may be manually set via `PKG_TAG=foobar make package-vmctl`.\n\nThe base docker image is [alpine](https://hub.docker.com/_/alpine) but it is possible to use any other base image\nby setting it via `<ROOT_IMAGE>` environment variable. For example, the following command builds the image on top of [scratch](https://hub.docker.com/_/scratch) image:\n\n```bash\nROOT_IMAGE=scratch make package-vmctl\n```\n\n### ARM build\n\nARM build may run on Raspberry Pi or on [energy-efficient ARM servers](https://blog.cloudflare.com/arm-takes-wing/).\n\n#### Development ARM build\n\n1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.13.\n2. Run `make vmctl-arm` or `make vmctl-arm64` from the root folder of the repository.\n It builds `vmctl-arm` or `vmctl-arm64` binary respectively and puts it into the `bin` folder.\n\n#### Production ARM build\n\n1. [Install docker](https://docs.docker.com/install/).\n2. Run `make vmctl-arm-prod` or `make vmctl-arm64-prod` from the root folder of the repository.\n It builds `vmctl-arm-prod` or `vmctl-arm64-prod` binary respectively and puts it into the `bin` folder.\n\n\n## Migrating data from InfluxDB (1.x)\n\n`vmctl` supports the `influx` mode to migrate data from InfluxDB to VictoriaMetrics time-series database.\n\nSee `./vmctl influx --help` for details and full list of flags.\n\nTo use migration tool please specify the InfluxDB address `--influx-addr`, the database `--influx-database` and VictoriaMetrics address `--vm-addr`.\nFlag `--vm-addr` for single-node VM is usually equal to `--httpListenAddr`, and for cluster version\nis equal to `--httpListenAddr` flag of VMInsert component. Please note, that vmctl performs initial readiness check for the given address \nby checking `/health` endpoint. For cluster version it is additionally required to specify the `--vm-account-id` flag. \nSee more details for cluster version [here](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster).\n\nAs soon as required flags are provided and all endpoints are accessible, `vmctl` will start the InfluxDB scheme exploration.\nBasically, it just fetches all fields and timeseries from the provided database and builds up registry of all available timeseries.\nThen `vmctl` sends fetch requests for each timeseries to InfluxDB one by one and pass results to VM importer.\nVM importer then accumulates received samples in batches and sends import requests to VM.\n\nThe importing process example for local installation of InfluxDB(`http://localhost:8086`) \nand single-node VictoriaMetrics(`http://localhost:8428`):\n```\n./vmctl influx --influx-database benchmark\nInfluxDB import mode\n2020/01/18 20:47:11 Exploring scheme for database \"benchmark\"\n2020/01/18 20:47:11 fetching fields: command: \"show field keys\"; database: \"benchmark\"; retention: \"autogen\"\n2020/01/18 20:47:11 found 10 fields\n2020/01/18 20:47:11 fetching series: command: \"show series \"; database: \"benchmark\"; retention: \"autogen\"\nFound 40000 timeseries to import. Continue? [Y/n] y\n40000 / 40000 [-----------------------------------------------------------------------------------------------------------------------------------------------] 100.00% 21 p/s\n2020/01/18 21:19:00 Import finished!\n2020/01/18 21:19:00 VictoriaMetrics importer stats:\n idle duration: 13m51.461434876s;\n time spent while importing: 17m56.923899847s;\n total samples: 345600000;\n samples/s: 320914.04;\n total bytes: 5.9 GB;\n bytes/s: 5.4 MB;\n import requests: 40001;\n2020/01/18 21:19:00 Total time: 31m48.467044016s\n``` \n\n### Data mapping\n\nVmctl maps Influx data the same way as VictoriaMetrics does by using the following rules:\n\n* `influx-database` arg is mapped into `db` label value unless `db` tag exists in the Influx line.\n* Field names are mapped to time series names prefixed with {measurement}{separator} value, \nwhere {separator} equals to _ by default. \nIt can be changed with `--influx-measurement-field-separator` command-line flag.\n* Field values are mapped to time series values.\n* Tags are mapped to Prometheus labels format as-is.\n\nFor example, the following Influx line:\n```\nfoo,tag1=value1,tag2=value2 field1=12,field2=40\n```\n\nis converted into the following Prometheus format data points:\n```\nfoo_field1{tag1=\"value1\", tag2=\"value2\"} 12\nfoo_field2{tag1=\"value1\", tag2=\"value2\"} 40\n```\n\n### Configuration\n\nThe configuration flags should contain self-explanatory descriptions. \n\n### Filtering\n\nThe filtering consists of two parts: timeseries and time.\nThe first step of application is to select all available timeseries\nfor given database and retention. User may specify additional filtering\ncondition via `--influx-filter-series` flag. For example:\n```\n./vmctl influx --influx-database benchmark \\\n --influx-filter-series \"on benchmark from cpu where hostname='host_1703'\"\nInfluxDB import mode\n2020/01/26 14:23:29 Exploring scheme for database \"benchmark\"\n2020/01/26 14:23:29 fetching fields: command: \"show field keys\"; database: \"benchmark\"; retention: \"autogen\"\n2020/01/26 14:23:29 found 12 fields\n2020/01/26 14:23:29 fetching series: command: \"show series on benchmark from cpu where hostname='host_1703'\"; database: \"benchmark\"; retention: \"autogen\"\nFound 10 timeseries to import. Continue? [Y/n] \n```\nThe timeseries select query would be following:\n `fetching series: command: \"show series on benchmark from cpu where hostname='host_1703'\"; database: \"benchmark\"; retention: \"autogen\"`\n \nThe second step of filtering is a time filter and it applies when fetching the datapoints from Influx.\nTime filtering may be configured with two flags:\n* --influx-filter-time-start \n* --influx-filter-time-end \nHere's an example of importing timeseries for one day only:\n`./vmctl influx --influx-database benchmark --influx-filter-series \"where hostname='host_1703'\" --influx-filter-time-start \"2020-01-01T10:07:00Z\" --influx-filter-time-end \"2020-01-01T15:07:00Z\"`\n\nPlease see more about time filtering [here](https://docs.influxdata.com/influxdb/v1.7/query_language/schema_exploration#filter-meta-queries-by-time).\n\n## Migrating data from InfluxDB (2.x)\n\nMigrating data from InfluxDB v2.x is not supported yet ([#32](https://github.com/VictoriaMetrics/vmctl/issues/32)).\nYou may find useful a 3rd party solution for this - https://github.com/jonppe/influx_to_victoriametrics.\n\n\n## Migrating data from Prometheus\n\n`vmctl` supports the `prometheus` mode for migrating data from Prometheus to VictoriaMetrics time-series database.\nMigration is based on reading Prometheus snapshot, which is basically a hard-link to Prometheus data files.\n\nSee `./vmctl prometheus --help` for details and full list of flags.\n\nTo use migration tool please specify the path to Prometheus snapshot `--prom-snapshot` and VictoriaMetrics address `--vm-addr`.\nMore about Prometheus snapshots may be found [here](https://www.robustperception.io/taking-snapshots-of-prometheus-data).\nFlag `--vm-addr` for single-node VM is usually equal to `--httpListenAddr`, and for cluster version\nis equal to `--httpListenAddr` flag of VMInsert component. Please note, that vmctl performs initial readiness check for the given address \nby checking `/health` endpoint. For cluster version it is additionally required to specify the `--vm-account-id` flag. \nSee more details for cluster version [here](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster).\n\nAs soon as required flags are provided and all endpoints are accessible, `vmctl` will start the Prometheus snapshot exploration.\nBasically, it just fetches all available blocks in provided snapshot and read the metadata. It also does initial filtering by time\nif flags `--prom-filter-time-start` or `--prom-filter-time-end` were set. The exploration procedure prints some stats from read blocks.\nPlease note that stats are not taking into account timeseries or samples filtering. This will be done during importing process.\n \nThe importing process takes the snapshot blocks revealed from Explore procedure and processes them one by one\naccumulating timeseries and samples. Please note, that `vmctl` relies on responses from Influx on this stage,\nso ensure that Explore queries are executed without errors or limits. Please see this \n[issue](https://github.com/VictoriaMetrics/vmctl/issues/30) for details.\nThe data processed in chunks and then sent to VM.\n\nThe importing process example for local installation of Prometheus \nand single-node VictoriaMetrics(`http://localhost:8428`):\n```\n./vmctl prometheus --prom-snapshot=/path/to/snapshot \\\n --vm-concurrency=1 \\\n --vm-batch-size=200000 \\\n --prom-concurrency=3\nPrometheus import mode\nPrometheus snapshot stats:\n blocks found: 14;\n blocks skipped: 0;\n min time: 1581288163058 (2020-02-09T22:42:43Z);\n max time: 1582409128139 (2020-02-22T22:05:28Z);\n samples: 32549106;\n series: 27289.\nFound 14 blocks to import. Continue? [Y/n] y\n14 / 14 [-------------------------------------------------------------------------------------------] 100.00% 0 p/s\n2020/02/23 15:50:03 Import finished!\n2020/02/23 15:50:03 VictoriaMetrics importer stats:\n idle duration: 6.152953029s;\n time spent while importing: 44.908522491s;\n total samples: 32549106;\n samples/s: 724786.84;\n total bytes: 669.1 MB;\n bytes/s: 14.9 MB;\n import requests: 323;\n import requests retries: 0;\n2020/02/23 15:50:03 Total time: 51.077451066s\n``` \n\n### Data mapping\n\nVictoriaMetrics has very similar data model to Prometheus and supports [RemoteWrite integration](https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage).\nSo no data changes will be applied.\n\n### Configuration\n\nThe configuration flags should contain self-explanatory descriptions. \n\n### Filtering\n\nThe filtering consists of three parts: by timeseries and time.\n\nFiltering by time may be configured via flags `--prom-filter-time-start` and `--prom-filter-time-end`\nin in RFC3339 format. This filter applied twice: to drop blocks out of range and to filter timeseries in blocks with\noverlapping time range.\n\nExample of applying time filter:\n```\n./vmctl prometheus --prom-snapshot=/path/to/snapshot \\\n --prom-filter-time-start=2020-02-07T00:07:01Z \\\n --prom-filter-time-end=2020-02-11T00:07:01Z\nPrometheus import mode\nPrometheus snapshot stats:\n blocks found: 2;\n blocks skipped: 12;\n min time: 1581288163058 (2020-02-09T22:42:43Z);\n max time: 1581328800000 (2020-02-10T10:00:00Z);\n samples: 1657698;\n series: 3930.\nFound 2 blocks to import. Continue? [Y/n] y\n```\n\nPlease notice, that total amount of blocks in provided snapshot is 14, but only 2 of them were in provided\ntime range. So other 12 blocks were marked as `skipped`. The amount of samples and series is not taken into account,\nsince this is heavy operation and will be done during import process.\n\n\nFiltering by timeseries is configured with following flags: \n* `--prom-filter-label` - the label name, e.g. `__name__` or `instance`;\n* `--prom-filter-label-value` - the regular expression to filter the label value. By default matches all `.*`\n\nFor example:\n```\n./vmctl prometheus --prom-snapshot=/path/to/snapshot \\\n --prom-filter-label=\"__name__\" \\\n --prom-filter-label-value=\"promhttp.*\" \\\n --prom-filter-time-start=2020-02-07T00:07:01Z \\\n --prom-filter-time-end=2020-02-11T00:07:01Z\nPrometheus import mode\nPrometheus snapshot stats:\n blocks found: 2;\n blocks skipped: 12;\n min time: 1581288163058 (2020-02-09T22:42:43Z);\n max time: 1581328800000 (2020-02-10T10:00:00Z);\n samples: 1657698;\n series: 3930.\nFound 2 blocks to import. Continue? [Y/n] y\n14 / 14 [------------------------------------------------------------------------------------------------------------------------------------------------------] 100.00% ? p/s\n2020/02/23 15:51:07 Import finished!\n2020/02/23 15:51:07 VictoriaMetrics importer stats:\n idle duration: 0s;\n time spent while importing: 37.415461ms;\n total samples: 10128;\n samples/s: 270690.24;\n total bytes: 195.2 kB;\n bytes/s: 5.2 MB;\n import requests: 2;\n import requests retries: 0;\n2020/02/23 15:51:07 Total time: 7.153158218s\n```\n\n## Migrating data from Thanos\n\nThanos uses the same storage engine as Prometheus and the data layout on-disk should be the same. That means\n`vmctl` in mode `prometheus` may be used for Thanos historical data migration as well.\nThese instructions may vary based on the details of your Thanos configuration. \nPlease read carefully and verify as you go. We assume you're using Thanos Sidecar on your Prometheus pods, \nand that you have a separate Thanos Store installation.\n\n### Current data\n\n1. For now, keep your Thanos Sidecar and Thanos-related Prometheus configuration, but add this to also stream \n metrics to VictoriaMetrics:\n ```\n remote_write:\n - url: http://victoria-metrics:8428/api/v1/write\n ```\n2. Make sure VM is running, of course. Now check the logs to make sure that Prometheus is sending and VM is receiving. \n In Prometheus, make sure there are no errors. On the VM side, you should see messages like this:\n ```\n 2020-04-27T18:38:46.474Z\tinfo\tVictoriaMetrics/lib/storage/partition.go:207\tcreating a partition \"2020_04\" with smallPartsPath=\"/victoria-metrics-data/data/small/2020_04\", bigPartsPath=\"/victoria-metrics-data/data/big/2020_04\"\n 2020-04-27T18:38:46.506Z\tinfo\tVictoriaMetrics/lib/storage/partition.go:222\tpartition \"2020_04\" has been created\n ```\n3. Now just wait. Within two hours, Prometheus should finish its current data file and hand it off to Thanos Store for long term\n storage.\n\n### Historical data\n\nLet's assume your data is stored on S3 served by minio. You first need to copy that out to a local filesystem, \nthen import it into VM using `vmctl` in `prometheus` mode.\n1. Copy data from minio.\n 1. Run the `minio/mc` Docker container.\n 1. `mc config host add minio http://minio:9000 accessKey secretKey`, substituting appropriate values for the last 3 items.\n 1. `mc cp -r minio/prometheus thanos-data`\n1. Import using `vmctl`.\n 1. Follow the [instructions](#how-to-build) to compile `vmctl` on your machine.\n 1. Use [prometheus](#migrating-data-from-prometheus) mode to import data: \n ```\n vmctl prometheus --prom-snapshot thanos-data --vm-addr http://victoria-metrics:8428\n ```\n\n## Migrating data from VictoriaMetrics\n\n### Native protocol\n\nThe [native binary protocol](https://victoriametrics.github.io/#how-to-export-data-in-native-format)\nwas introduced in [1.42.0 release](https://github.com/VictoriaMetrics/VictoriaMetrics/releases/tag/v1.42.0)\nand provides the most efficient way to migrate data between VM instances: single to single, cluster to cluster,\nsingle to cluster and vice versa. Please note that both instances (source and destination) should be of v1.42.0\nor higher.\n\nSee `./vmctl vm-native --help` for details and full list of flags.\n\nIn this mode `vmctl` acts as a proxy between two VM instances, where time series filtering is done by \"source\" (`src`) \nand processing is done by \"destination\" (`dst`). Because of that, `vmctl` doesn't actually know how much data will be \nprocessed and can't show the progress bar. It will show the current processing speed and total number of processed bytes:\n\n```\n./vmctl vm-native --vm-native-src-addr=http://localhost:8528 \\\n --vm-native-dst-addr=http://localhost:8428 \\\n --vm-native-filter-match='{job=\"vmagent\"}' \\\n --vm-native-filter-time-start='2020-01-01T20:07:00Z'\nVictoriaMetrics Native import mode\nIniting export pipe from \"http://localhost:8528\" with filters: \n filter: match[]={job=\"vmagent\"}\nIniting import process to \"http://localhost:8428\":\nTotal: 336.75 KiB ↖ Speed: 454.46 KiB p/s \n2020/10/13 17:04:59 Total time: 952.143376ms\n``` \n\nImporting tips:\n1. Migrating all the metrics from one VM to another may collide with existing application metrics \n(prefixed with `vm_`) at destination and lead to confusion when using \n[official Grafana dashboards](https://grafana.com/orgs/victoriametrics/dashboards). \nTo avoid such situation try to filter out VM process metrics via `--vm-native-filter-match` flag.\n2. Migration is a backfilling process, so it is recommended to read \n[Backfilling tips](https://github.com/VictoriaMetrics/VictoriaMetrics#backfilling) section.\n3. `vmctl` doesn't provide relabeling or other types of labels management in this mode.\nInstead, use [relabeling in VictoriaMetrics](https://github.com/VictoriaMetrics/vmctl/issues/4#issuecomment-683424375).\n\n\n## Tuning\n\n### Influx mode\n\nThe flag `--influx-concurrency` controls how many concurrent requests may be sent to InfluxDB while fetching\ntimeseries. Please set it wisely to avoid InfluxDB overwhelming.\n\nThe flag `--influx-chunk-size` controls the max amount of datapoints to return in single chunk from fetch requests.\nPlease see more details [here](https://docs.influxdata.com/influxdb/v1.7/guides/querying_data/#chunking).\nThe chunk size is used to control InfluxDB memory usage, so it won't OOM on processing large timeseries with \nbillions of datapoints.\n\n### Prometheus mode\n\nThe flag `--prom-concurrency` controls how many concurrent readers will be reading the blocks in snapshot.\nSince snapshots are just files on disk it would be hard to overwhelm the system. Please go with value equal\nto number of free CPU cores.\n\n### VictoriaMetrics importer\n\nThe flag `--vm-concurrency` controls the number of concurrent workers that process the input from InfluxDB query results.\nPlease note that each import request can load up to a single vCPU core on VictoriaMetrics. So try to set it according\nto allocated CPU resources of your VictoriMetrics installation.\n\nThe flag `--vm-batch-size` controls max amount of samples collected before sending the import request.\nFor example, if `--influx-chunk-size=500` and `--vm-batch-size=2000` then importer will process not more \nthan 4 chunks before sending the request. \n\n### Importer stats\n\nAfter successful import `vmctl` prints some statistics for details. \nThe important numbers to watch are following:\n - `idle duration` - shows time that importer spent while waiting for data from InfluxDB/Prometheus \nto fill up `--vm-batch-size` batch size. Value shows total duration across all workers configured\nvia `--vm-concurrency`. High value may be a sign of too slow InfluxDB/Prometheus fetches or too\nhigh `--vm-concurrency` value. Try to improve it by increasing `--<mode>-concurrency` value or \ndecreasing `--vm-concurrency` value.\n- `import requests` - shows how many import requests were issued to VM server.\nThe import request is issued once the batch size(`--vm-batch-size`) is full and ready to be sent.\nPlease prefer big batch sizes (50k-500k) to improve performance.\n- `import requests retries` - shows number of unsuccessful import requests. Non-zero value may be\na sign of network issues or VM being overloaded. See the logs during import for error messages.\n\n### Silent mode\n\nBy default `vmctl` waits confirmation from user before starting the import. If this is unwanted\nbehavior and no user interaction required - pass `-s` flag to enable \"silence\" mode:\n```\n -s Whether to run in silent mode. If set to true no confirmation prompts will appear. (default: false)\n```\n\n### Significant figures\n\n`vmctl` allows to limit the number of [significant figures](https://en.wikipedia.org/wiki/Significant_figures)\nbefore importing. For example, the average value for response size is `102.342305` bytes and it has 9 significant figures.\nIf you ask a human to pronounce this value then with high probability value will be rounded to first 4 or 5 figures \nbecause the rest aren't really that important to mention. In most cases, such a high precision is too much. \nMoreover, such values may be just a result of [floating point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic), \ncreate a [false precision](https://en.wikipedia.org/wiki/False_precision) and result into bad compression ratio \naccording to [information theory](https://en.wikipedia.org/wiki/Information_theory). \n\n`vmctl` provides the following flags for improving data compression:\n\n* `--vm-round-digits` flag for rounding processed values to the given number of decimal digits after the point.\n For example, `--vm-round-digits=2` would round `1.2345` to `1.23`. By default the rounding is disabled.\n\n* `--vm-significant-figures` flag for limiting the number of significant figures in processed values. It takes no effect if set\n to 0 (by default), but set `--vm-significant-figures=5` and `102.342305` will be rounded to `102.34`.\n\nThe most common case for using these flags is to improve data compression for time series storing aggregation\nresults such as `average`, `rate`, etc.\n\n### Adding extra labels\n\n `vmctl` allows to add extra labels to all imported series. It can be achived with flag `--vm-extra-label label=value`. \n If multiple labels needs to be added, set flag for each label, for example, `--vm-extra-label label1=value1 --vm-extra-label label2=value2`.\n If timeseries already have label, that must be added with `--vm-extra-label` flag, flag has priority and will override label value from timeseries.\n \n","dir":"/","name":"vmctl.md","path":"vmctl.md","url":"/vmctl.html"},{"layout":"default","title":"vmrestore","content":"## vmrestore\n\n`vmrestore` restores data from backups created by [vmbackup](https://victoriametrics.github.io/vbackup.html).\nVictoriaMetrics `v1.29.0` and newer versions must be used for working with the restored data.\n\nRestore process can be interrupted at any time. It is automatically resumed from the interruption point\nwhen restarting `vmrestore` with the same args.\n\n\n## Usage\n\nVictoriaMetrics must be stopped during the restore process.\n\n```\nvmrestore -src=gcs://<bucket>/<path/to/backup> -storageDataPath=<local/path/to/restore>\n\n```\n\n* `<bucket>` is [GCS bucket](https://cloud.google.com/storage/docs/creating-buckets) name.\n* `<path/to/backup>` is the path to backup made with [vmbackup](https://victoriametrics.github.io/vbackup.html) on GCS bucket.\n* `<local/path/to/restore>` is the path to folder where data will be restored. This folder must be passed\n to VictoriaMetrics in `-storageDataPath` command-line flag after the restore process is complete.\n\nThe original `-storageDataPath` directory may contain old files. They will be substituted by the files from backup,\ni.e. the end result would be similar to [rsync --delete](https://askubuntu.com/questions/476041/how-do-i-make-rsync-delete-files-that-have-been-deleted-from-the-source-folder).\n\n\n## Troubleshooting\n\n* If `vmrestore` eats all the network bandwidth, then set `-maxBytesPerSecond` to the desired value.\n* If `vmrestore` has been interrupted due to temporary error, then just restart it with the same args. It will resume the restore process.\n\n\n## Advanced usage\n\n* Obtaining credentials from a file.\n\n Add flag `-credsFilePath=/etc/credentials` with following content:\n\n for s3 (aws, minio or other s3 compatible storages):\n ```bash\n [default]\n aws_access_key_id=theaccesskey\n aws_secret_access_key=thesecretaccesskeyvalue\n ```\n\n for gce cloud storage:\n ```json\n {\n \"type\": \"service_account\",\n \"project_id\": \"project-id\",\n \"private_key_id\": \"key-id\",\n \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nprivate-key\\n-----END PRIVATE KEY-----\\n\",\n \"client_email\": \"service-account-email\",\n \"client_id\": \"client-id\",\n \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n \"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/service-account-email\"\n }\n ```\n\n* Usage with s3 custom url endpoint. It is possible to use `vmrestore` with s3 api compatible storages, like minio, cloudian and other.\n You have to add custom url endpoint with a flag:\n```\n # for minio:\n -customS3Endpoint=http://localhost:9000\n\n # for aws gov region\n -customS3Endpoint=https://s3-fips.us-gov-west-1.amazonaws.com\n```\n\n* Run `vmrestore -help` in order to see all the available options:\n\n```\n -concurrency int\n \tThe number of concurrent workers. Higher concurrency may reduce restore duration (default 10)\n -configFilePath string\n \tPath to file with S3 configs. Configs are loaded from default location if not set.\n \tSee https://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html\n -configProfile string\n \tProfile name for S3 configs. If no set, the value of the environment variable will be loaded (AWS_PROFILE or AWS_DEFAULT_PROFILE), or if both not set, DefaultSharedConfigProfile is used\n -credsFilePath string\n \tPath to file with GCS or S3 credentials. Credentials are loaded from default locations if not set.\n \tSee https://cloud.google.com/iam/docs/creating-managing-service-account-keys and https://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html\n -customS3Endpoint string\n \tCustom S3 endpoint for use with S3-compatible storages (e.g. MinIO). S3 is used if not set\n -envflag.enable\n \tWhether to enable reading flags from environment variables additionally to command line. Command line flag values have priority over values from environment vars. Flags are read only from command line if this flag isn't set\n -envflag.prefix string\n \tPrefix for environment variables if -envflag.enable is set\n -fs.disableMmap\n \tWhether to use pread() instead of mmap() for reading data files. By default mmap() is used for 64-bit arches and pread() is used for 32-bit arches, since they cannot read data files bigger than 2^32 bytes in memory. mmap() is usually faster for reading small data chunks than pread()\n -loggerErrorsPerSecondLimit int\n \tPer-second limit on the number of ERROR messages. If more than the given number of errors are emitted per second, then the remaining errors are suppressed. Zero value disables the rate limit (default 10)\n -loggerFormat string\n \tFormat for logs. Possible values: default, json (default \"default\")\n -loggerLevel string\n \tMinimum level of errors to log. Possible values: INFO, WARN, ERROR, FATAL, PANIC (default \"INFO\")\n -loggerOutput string\n \tOutput for the logs. Supported values: stderr, stdout (default \"stderr\")\n -maxBytesPerSecond value\n \tThe maximum download speed. There is no limit if it is set to 0\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 0)\n -memory.allowedBytes value\n \tAllowed size of system memory VictoriaMetrics caches may occupy. This option overrides -memory.allowedPercent if set to non-zero value. Too low value may increase cache miss rate, which usually results in higher CPU and disk IO usage. Too high value may evict too much data from OS page cache, which will result in higher disk IO usage\n \tSupports the following optional suffixes for values: KB, MB, GB, KiB, MiB, GiB (default 0)\n -memory.allowedPercent float\n \tAllowed percent of system memory VictoriaMetrics caches may occupy. See also -memory.allowedBytes. Too low value may increase cache miss rate, which usually results in higher CPU and disk IO usage. Too high value may evict too much data from OS page cache, which will result in higher disk IO usage (default 60)\n -skipBackupCompleteCheck\n \tWhether to skip checking for 'backup complete' file in -src. This may be useful for restoring from old backups, which were created without 'backup complete' file\n -src string\n \tSource path with backup on the remote storage. Example: gcs://bucket/path/to/backup/dir, s3://bucket/path/to/backup/dir or fs:///path/to/local/backup/dir\n -storageDataPath string\n \tDestination path where backup must be restored. VictoriaMetrics must be stopped when restoring from backup. -storageDataPath dir can be non-empty. In this case the contents of -storageDataPath dir is synchronized with -src contents, i.e. it works like 'rsync --delete' (default \"victoria-metrics-data\")\n -version\n \tShow VictoriaMetrics version\n```\n\n\n## How to build from sources\n\nIt is recommended using [binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) - see `vmutils-*` archives there.\n\n\n### Development build\n\n1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.13.\n2. Run `make vmrestore` from the root folder of the repository.\n It builds `vmrestore` binary and puts it into the `bin` folder.\n\n### Production build\n\n1. [Install docker](https://docs.docker.com/install/).\n2. Run `make vmrestore-prod` from the root folder of the repository.\n It builds `vmrestore-prod` binary and puts it into the `bin` folder.\n\n### Building docker images\n\nRun `make package-vmrestore`. It builds `victoriametrics/vmrestore:<PKG_TAG>` docker image locally.\n`<PKG_TAG>` is auto-generated image tag, which depends on source code in the repository.\nThe `<PKG_TAG>` may be manually set via `PKG_TAG=foobar make package-vmrestore`.\n\nThe base docker image is [alpine](https://hub.docker.com/_/alpine) but it is possible to use any other base image\nby setting it via `<ROOT_IMAGE>` environment variable. For example, the following command builds the image on top of [scratch](https://hub.docker.com/_/scratch) image:\n\n```bash\nROOT_IMAGE=scratch make package-vmrestore\n```\n","dir":"/","name":"vmrestore.md","path":"vmrestore.md","url":"/vmrestore.html"}]